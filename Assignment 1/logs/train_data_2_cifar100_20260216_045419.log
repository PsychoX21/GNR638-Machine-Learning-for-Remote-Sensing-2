Logging to: logs/train_data_2_cifar100_20260216_045419.log
======================================================================
DeepNet Training
======================================================================

Building model config from: configs\cifar100.yaml
Image size: 32x32, Channels: 3, Batch size: 128, Epochs: 100

Loading dataset: datasets/data_2 (data_2)
Preloading 45000 train samples...
Preload complete. 45000 images in RAM.
Preloading 5000 val samples...
Preload complete. 5000 images in RAM.
Dataset loading time: 8.29 seconds
Number of classes: 100
Train samples: 45000, Val samples: 5000

Model Statistics:
  Parameters: 278,324
  MACs: 5,224,824,832
  FLOPs: 10,449,649,664

CUDA status: enabled
Moving model to CUDA...

Optimizer: SGD, LR: 0.1
Scheduler: CosineAnnealingLR (T_max=100, eta_min=0.0001)

======================================================================
Training Started
======================================================================
Epoch [1] Batch [0/352] Loss: 6.2229 Acc: 1.56%
Epoch [1] Batch [10/352] Loss: 4.6066 Acc: 1.42%
Epoch [1] Batch [20/352] Loss: 4.6163 Acc: 1.79%
Epoch [1] Batch [30/352] Loss: 4.5148 Acc: 2.22%
Epoch [1] Batch [40/352] Loss: 4.3432 Acc: 2.67%
Epoch [1] Batch [50/352] Loss: 4.3218 Acc: 2.85%
Epoch [1] Batch [60/352] Loss: 4.1920 Acc: 3.18%
Epoch [1] Batch [70/352] Loss: 4.2697 Acc: 3.43%
Epoch [1] Batch [80/352] Loss: 4.1058 Acc: 3.71%
Epoch [1] Batch [90/352] Loss: 4.1699 Acc: 3.78%
Epoch [1] Batch [100/352] Loss: 4.1402 Acc: 4.12%
Epoch [1] Batch [110/352] Loss: 4.1265 Acc: 4.36%
Epoch [1] Batch [120/352] Loss: 3.9863 Acc: 4.62%
Epoch [1] Batch [130/352] Loss: 4.0131 Acc: 4.81%
Epoch [1] Batch [140/352] Loss: 3.9067 Acc: 5.06%
Epoch [1] Batch [150/352] Loss: 3.9890 Acc: 5.33%
Epoch [1] Batch [160/352] Loss: 4.0909 Acc: 5.62%
Epoch [1] Batch [170/352] Loss: 3.9195 Acc: 5.79%
Epoch [1] Batch [180/352] Loss: 3.8774 Acc: 5.92%
Epoch [1] Batch [190/352] Loss: 3.8806 Acc: 6.02%
Epoch [1] Batch [200/352] Loss: 3.6504 Acc: 6.18%
Epoch [1] Batch [210/352] Loss: 3.8790 Acc: 6.33%
Epoch [1] Batch [220/352] Loss: 4.1465 Acc: 6.50%
Epoch [1] Batch [230/352] Loss: 3.8483 Acc: 6.66%
Epoch [1] Batch [240/352] Loss: 3.8780 Acc: 6.84%
Epoch [1] Batch [250/352] Loss: 3.6712 Acc: 6.98%
Epoch [1] Batch [260/352] Loss: 3.8754 Acc: 7.15%
Epoch [1] Batch [270/352] Loss: 3.7539 Acc: 7.30%
Epoch [1] Batch [280/352] Loss: 3.8749 Acc: 7.40%
Epoch [1] Batch [290/352] Loss: 3.9665 Acc: 7.51%
Epoch [1] Batch [300/352] Loss: 3.7046 Acc: 7.66%
Epoch [1] Batch [310/352] Loss: 3.7175 Acc: 7.78%
Epoch [1] Batch [320/352] Loss: 3.6415 Acc: 7.92%
Epoch [1] Batch [330/352] Loss: 3.6380 Acc: 8.06%
Epoch [1] Batch [340/352] Loss: 3.5184 Acc: 8.20%
Epoch [1] Batch [350/352] Loss: 3.6204 Acc: 8.35%

======================================================================
Epoch 1/100
Train Loss: 3.9938, Train Acc: 8.36%
Val Loss: 4.0316, Val Acc: 7.66%
Time: 94.86s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 7.66%)
Epoch [2] Batch [0/352] Loss: 3.6405 Acc: 14.06%
Epoch [2] Batch [10/352] Loss: 3.4351 Acc: 15.06%
Epoch [2] Batch [20/352] Loss: 3.6242 Acc: 14.32%
Epoch [2] Batch [30/352] Loss: 3.5503 Acc: 14.26%
Epoch [2] Batch [40/352] Loss: 3.7602 Acc: 14.48%
Epoch [2] Batch [50/352] Loss: 3.4605 Acc: 14.98%
Epoch [2] Batch [60/352] Loss: 3.8173 Acc: 14.93%
Epoch [2] Batch [70/352] Loss: 3.5252 Acc: 14.87%
Epoch [2] Batch [80/352] Loss: 3.5539 Acc: 15.00%
Epoch [2] Batch [90/352] Loss: 3.6603 Acc: 15.06%
Epoch [2] Batch [100/352] Loss: 3.6146 Acc: 15.07%
Epoch [2] Batch [110/352] Loss: 3.6322 Acc: 15.16%
Epoch [2] Batch [120/352] Loss: 3.3849 Acc: 15.30%
Epoch [2] Batch [130/352] Loss: 3.5660 Acc: 15.43%
Epoch [2] Batch [140/352] Loss: 3.5327 Acc: 15.45%
Epoch [2] Batch [150/352] Loss: 3.4057 Acc: 15.55%
Epoch [2] Batch [160/352] Loss: 3.3539 Acc: 15.70%
Epoch [2] Batch [170/352] Loss: 3.4546 Acc: 15.79%
Epoch [2] Batch [180/352] Loss: 3.3837 Acc: 15.79%
Epoch [2] Batch [190/352] Loss: 3.4751 Acc: 15.87%
Epoch [2] Batch [200/352] Loss: 3.3138 Acc: 15.97%
Epoch [2] Batch [210/352] Loss: 3.2814 Acc: 16.07%
Epoch [2] Batch [220/352] Loss: 3.3331 Acc: 16.17%
Epoch [2] Batch [230/352] Loss: 3.2402 Acc: 16.37%
Epoch [2] Batch [240/352] Loss: 3.2368 Acc: 16.44%
Epoch [2] Batch [250/352] Loss: 3.2189 Acc: 16.56%
Epoch [2] Batch [260/352] Loss: 3.3809 Acc: 16.66%
Epoch [2] Batch [270/352] Loss: 3.2395 Acc: 16.73%
Epoch [2] Batch [280/352] Loss: 2.9496 Acc: 16.95%
Epoch [2] Batch [290/352] Loss: 3.1220 Acc: 17.10%
Epoch [2] Batch [300/352] Loss: 3.2700 Acc: 17.24%
Epoch [2] Batch [310/352] Loss: 3.0908 Acc: 17.40%
Epoch [2] Batch [320/352] Loss: 3.3217 Acc: 17.57%
Epoch [2] Batch [330/352] Loss: 3.2195 Acc: 17.68%
Epoch [2] Batch [340/352] Loss: 3.1855 Acc: 17.78%
Epoch [2] Batch [350/352] Loss: 3.0554 Acc: 17.87%

======================================================================
Epoch 2/100
Train Loss: 3.3702, Train Acc: 17.87%
Val Loss: 3.2748, Val Acc: 19.80%
Time: 93.14s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 19.80%)
Epoch [3] Batch [0/352] Loss: 3.2082 Acc: 23.44%
Epoch [3] Batch [10/352] Loss: 2.8985 Acc: 24.36%
Epoch [3] Batch [20/352] Loss: 2.9413 Acc: 23.03%
Epoch [3] Batch [30/352] Loss: 3.0059 Acc: 23.01%
Epoch [3] Batch [40/352] Loss: 3.0739 Acc: 22.90%
Epoch [3] Batch [50/352] Loss: 3.1786 Acc: 22.73%
Epoch [3] Batch [60/352] Loss: 3.1144 Acc: 22.94%
Epoch [3] Batch [70/352] Loss: 2.8502 Acc: 23.10%
Epoch [3] Batch [80/352] Loss: 2.7324 Acc: 22.93%
Epoch [3] Batch [90/352] Loss: 2.7688 Acc: 23.11%
Epoch [3] Batch [100/352] Loss: 3.0946 Acc: 22.96%
Epoch [3] Batch [110/352] Loss: 2.9979 Acc: 23.18%
Epoch [3] Batch [120/352] Loss: 2.9944 Acc: 23.39%
Epoch [3] Batch [130/352] Loss: 3.0061 Acc: 23.43%
Epoch [3] Batch [140/352] Loss: 3.0726 Acc: 23.53%
Epoch [3] Batch [150/352] Loss: 2.8520 Acc: 23.54%
Epoch [3] Batch [160/352] Loss: 3.0303 Acc: 23.57%
Epoch [3] Batch [170/352] Loss: 2.8884 Acc: 23.63%
Epoch [3] Batch [180/352] Loss: 2.8304 Acc: 23.69%
Epoch [3] Batch [190/352] Loss: 2.9434 Acc: 23.73%
Epoch [3] Batch [200/352] Loss: 2.9194 Acc: 23.89%
Epoch [3] Batch [210/352] Loss: 2.7878 Acc: 24.02%
Epoch [3] Batch [220/352] Loss: 2.9110 Acc: 24.04%
Epoch [3] Batch [230/352] Loss: 2.8890 Acc: 24.16%
Epoch [3] Batch [240/352] Loss: 2.8430 Acc: 24.16%
Epoch [3] Batch [250/352] Loss: 2.8670 Acc: 24.21%
Epoch [3] Batch [260/352] Loss: 2.7305 Acc: 24.30%
Epoch [3] Batch [270/352] Loss: 2.7062 Acc: 24.50%
Epoch [3] Batch [280/352] Loss: 2.8889 Acc: 24.59%
Epoch [3] Batch [290/352] Loss: 2.7035 Acc: 24.68%
Epoch [3] Batch [300/352] Loss: 3.0526 Acc: 24.83%
Epoch [3] Batch [310/352] Loss: 2.8120 Acc: 24.89%
Epoch [3] Batch [320/352] Loss: 2.8833 Acc: 25.01%
Epoch [3] Batch [330/352] Loss: 2.7804 Acc: 25.10%
Epoch [3] Batch [340/352] Loss: 2.7256 Acc: 25.18%
Epoch [3] Batch [350/352] Loss: 2.7245 Acc: 25.21%

======================================================================
Epoch 3/100
Train Loss: 2.9294, Train Acc: 25.22%
Val Loss: 3.1339, Val Acc: 22.54%
Time: 94.12s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 22.54%)
Epoch [4] Batch [0/352] Loss: 2.7006 Acc: 28.91%
Epoch [4] Batch [10/352] Loss: 3.0245 Acc: 27.49%
Epoch [4] Batch [20/352] Loss: 2.7014 Acc: 28.39%
Epoch [4] Batch [30/352] Loss: 2.5888 Acc: 28.93%
Epoch [4] Batch [40/352] Loss: 2.6760 Acc: 28.96%
Epoch [4] Batch [50/352] Loss: 2.7028 Acc: 29.34%
Epoch [4] Batch [60/352] Loss: 2.5760 Acc: 29.28%
Epoch [4] Batch [70/352] Loss: 2.6850 Acc: 29.46%
Epoch [4] Batch [80/352] Loss: 2.7845 Acc: 29.45%
Epoch [4] Batch [90/352] Loss: 2.7875 Acc: 29.40%
Epoch [4] Batch [100/352] Loss: 2.6427 Acc: 29.70%
Epoch [4] Batch [110/352] Loss: 2.6628 Acc: 29.72%
Epoch [4] Batch [120/352] Loss: 2.7335 Acc: 30.00%
Epoch [4] Batch [130/352] Loss: 2.6821 Acc: 30.20%
Epoch [4] Batch [140/352] Loss: 2.7009 Acc: 30.19%
Epoch [4] Batch [150/352] Loss: 2.8033 Acc: 30.13%
Epoch [4] Batch [160/352] Loss: 3.0055 Acc: 30.11%
Epoch [4] Batch [170/352] Loss: 2.6080 Acc: 30.10%
Epoch [4] Batch [180/352] Loss: 2.5036 Acc: 30.20%
Epoch [4] Batch [190/352] Loss: 2.3938 Acc: 30.33%
Epoch [4] Batch [200/352] Loss: 2.5223 Acc: 30.27%
Epoch [4] Batch [210/352] Loss: 2.3586 Acc: 30.30%
Epoch [4] Batch [220/352] Loss: 2.5408 Acc: 30.39%
Epoch [4] Batch [230/352] Loss: 2.4778 Acc: 30.46%
Epoch [4] Batch [240/352] Loss: 2.6253 Acc: 30.60%
Epoch [4] Batch [250/352] Loss: 2.6188 Acc: 30.67%
Epoch [4] Batch [260/352] Loss: 2.6915 Acc: 30.70%
Epoch [4] Batch [270/352] Loss: 2.5118 Acc: 30.76%
Epoch [4] Batch [280/352] Loss: 2.4275 Acc: 30.85%
Epoch [4] Batch [290/352] Loss: 2.5532 Acc: 30.97%
Epoch [4] Batch [300/352] Loss: 2.6204 Acc: 30.95%
Epoch [4] Batch [310/352] Loss: 2.4840 Acc: 31.01%
Epoch [4] Batch [320/352] Loss: 2.4664 Acc: 31.17%
Epoch [4] Batch [330/352] Loss: 2.6428 Acc: 31.26%
Epoch [4] Batch [340/352] Loss: 2.5380 Acc: 31.32%
Epoch [4] Batch [350/352] Loss: 2.8524 Acc: 31.37%

======================================================================
Epoch 4/100
Train Loss: 2.6240, Train Acc: 31.36%
Val Loss: 2.7255, Val Acc: 31.78%
Time: 94.66s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 31.78%)
Epoch [5] Batch [0/352] Loss: 2.2909 Acc: 39.84%
Epoch [5] Batch [10/352] Loss: 2.4120 Acc: 35.58%
Epoch [5] Batch [20/352] Loss: 2.6017 Acc: 34.56%
Epoch [5] Batch [30/352] Loss: 2.3735 Acc: 34.70%
Epoch [5] Batch [40/352] Loss: 2.6049 Acc: 34.87%
Epoch [5] Batch [50/352] Loss: 2.1541 Acc: 35.08%
Epoch [5] Batch [60/352] Loss: 2.4854 Acc: 34.98%
Epoch [5] Batch [70/352] Loss: 2.3441 Acc: 34.97%
Epoch [5] Batch [80/352] Loss: 2.5656 Acc: 34.98%
Epoch [5] Batch [90/352] Loss: 2.4793 Acc: 34.94%
Epoch [5] Batch [100/352] Loss: 2.3455 Acc: 35.18%
Epoch [5] Batch [110/352] Loss: 2.4737 Acc: 35.20%
Epoch [5] Batch [120/352] Loss: 2.3362 Acc: 35.32%
Epoch [5] Batch [130/352] Loss: 2.6528 Acc: 35.34%
Epoch [5] Batch [140/352] Loss: 2.3939 Acc: 35.46%
Epoch [5] Batch [150/352] Loss: 2.5002 Acc: 35.47%
Epoch [5] Batch [160/352] Loss: 2.2242 Acc: 35.48%
Epoch [5] Batch [170/352] Loss: 2.6445 Acc: 35.35%
Epoch [5] Batch [180/352] Loss: 2.5130 Acc: 35.25%
Epoch [5] Batch [190/352] Loss: 2.5594 Acc: 35.32%
Epoch [5] Batch [200/352] Loss: 2.5669 Acc: 35.29%
Epoch [5] Batch [210/352] Loss: 2.6037 Acc: 35.32%
Epoch [5] Batch [220/352] Loss: 2.5596 Acc: 35.38%
Epoch [5] Batch [230/352] Loss: 2.4988 Acc: 35.36%
Epoch [5] Batch [240/352] Loss: 2.4225 Acc: 35.40%
Epoch [5] Batch [250/352] Loss: 2.3925 Acc: 35.39%
Epoch [5] Batch [260/352] Loss: 2.2355 Acc: 35.42%
Epoch [5] Batch [270/352] Loss: 2.3758 Acc: 35.52%
Epoch [5] Batch [280/352] Loss: 2.3717 Acc: 35.61%
Epoch [5] Batch [290/352] Loss: 2.2978 Acc: 35.70%
Epoch [5] Batch [300/352] Loss: 2.3288 Acc: 35.75%
Epoch [5] Batch [310/352] Loss: 2.2579 Acc: 35.78%
Epoch [5] Batch [320/352] Loss: 2.4654 Acc: 35.74%
Epoch [5] Batch [330/352] Loss: 2.3377 Acc: 35.80%
Epoch [5] Batch [340/352] Loss: 2.4252 Acc: 35.78%
Epoch [5] Batch [350/352] Loss: 2.2829 Acc: 35.84%

======================================================================
Epoch 5/100
Train Loss: 2.4099, Train Acc: 35.85%
Val Loss: 2.8849, Val Acc: 30.08%
Time: 93.87s
======================================================================

Epoch [6] Batch [0/352] Loss: 2.5292 Acc: 34.38%
Epoch [6] Batch [10/352] Loss: 2.4246 Acc: 39.06%
Epoch [6] Batch [20/352] Loss: 2.4164 Acc: 39.58%
Epoch [6] Batch [30/352] Loss: 2.3849 Acc: 39.62%
Epoch [6] Batch [40/352] Loss: 2.0238 Acc: 39.63%
Epoch [6] Batch [50/352] Loss: 2.1715 Acc: 39.20%
Epoch [6] Batch [60/352] Loss: 2.2112 Acc: 38.87%
Epoch [6] Batch [70/352] Loss: 2.0812 Acc: 38.53%
Epoch [6] Batch [80/352] Loss: 2.2481 Acc: 38.67%
Epoch [6] Batch [90/352] Loss: 2.1314 Acc: 38.55%
Epoch [6] Batch [100/352] Loss: 2.1035 Acc: 38.70%
Epoch [6] Batch [110/352] Loss: 2.4652 Acc: 38.67%
Epoch [6] Batch [120/352] Loss: 2.1360 Acc: 38.86%
Epoch [6] Batch [130/352] Loss: 2.0788 Acc: 38.71%
Epoch [6] Batch [140/352] Loss: 2.3862 Acc: 38.68%
Epoch [6] Batch [150/352] Loss: 2.2584 Acc: 38.67%
Epoch [6] Batch [160/352] Loss: 2.4741 Acc: 38.74%
Epoch [6] Batch [170/352] Loss: 2.3121 Acc: 38.66%
Epoch [6] Batch [180/352] Loss: 2.1630 Acc: 38.70%
Epoch [6] Batch [190/352] Loss: 2.0767 Acc: 38.69%
Epoch [6] Batch [200/352] Loss: 2.2785 Acc: 38.77%
Epoch [6] Batch [210/352] Loss: 2.2186 Acc: 38.76%
Epoch [6] Batch [220/352] Loss: 2.0988 Acc: 38.74%
Epoch [6] Batch [230/352] Loss: 2.2125 Acc: 38.77%
Epoch [6] Batch [240/352] Loss: 1.8626 Acc: 38.78%
Epoch [6] Batch [250/352] Loss: 2.3535 Acc: 38.86%
Epoch [6] Batch [260/352] Loss: 2.0231 Acc: 38.92%
Epoch [6] Batch [270/352] Loss: 2.1293 Acc: 38.94%
Epoch [6] Batch [280/352] Loss: 2.1422 Acc: 39.02%
Epoch [6] Batch [290/352] Loss: 2.0874 Acc: 39.09%
Epoch [6] Batch [300/352] Loss: 1.9038 Acc: 39.25%
Epoch [6] Batch [310/352] Loss: 2.2397 Acc: 39.23%
Epoch [6] Batch [320/352] Loss: 2.5133 Acc: 39.26%
Epoch [6] Batch [330/352] Loss: 2.3363 Acc: 39.24%
Epoch [6] Batch [340/352] Loss: 2.3330 Acc: 39.23%
Epoch [6] Batch [350/352] Loss: 2.2376 Acc: 39.21%

======================================================================
Epoch 6/100
Train Loss: 2.2538, Train Acc: 39.21%
Val Loss: 2.4173, Val Acc: 36.92%
Time: 93.02s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 36.92%)
Epoch [7] Batch [0/352] Loss: 2.3550 Acc: 33.59%
Epoch [7] Batch [10/352] Loss: 2.0585 Acc: 40.48%
Epoch [7] Batch [20/352] Loss: 2.1714 Acc: 42.00%
Epoch [7] Batch [30/352] Loss: 2.1986 Acc: 41.43%
Epoch [7] Batch [40/352] Loss: 2.0983 Acc: 41.14%
Epoch [7] Batch [50/352] Loss: 2.0564 Acc: 41.39%
Epoch [7] Batch [60/352] Loss: 2.1829 Acc: 41.55%
Epoch [7] Batch [70/352] Loss: 2.0980 Acc: 40.97%
Epoch [7] Batch [80/352] Loss: 2.2571 Acc: 41.13%
Epoch [7] Batch [90/352] Loss: 2.3868 Acc: 40.99%
Epoch [7] Batch [100/352] Loss: 2.1428 Acc: 41.32%
Epoch [7] Batch [110/352] Loss: 2.1418 Acc: 41.55%
Epoch [7] Batch [120/352] Loss: 2.1915 Acc: 41.58%
Epoch [7] Batch [130/352] Loss: 1.8353 Acc: 41.75%
Epoch [7] Batch [140/352] Loss: 2.0880 Acc: 41.83%
Epoch [7] Batch [150/352] Loss: 2.1039 Acc: 41.84%
Epoch [7] Batch [160/352] Loss: 2.2532 Acc: 41.78%
Epoch [7] Batch [170/352] Loss: 2.0096 Acc: 41.84%
Epoch [7] Batch [180/352] Loss: 2.2117 Acc: 41.89%
Epoch [7] Batch [190/352] Loss: 2.1549 Acc: 41.85%
Epoch [7] Batch [200/352] Loss: 2.2093 Acc: 41.87%
Epoch [7] Batch [210/352] Loss: 2.3540 Acc: 41.86%
Epoch [7] Batch [220/352] Loss: 2.2608 Acc: 41.88%
Epoch [7] Batch [230/352] Loss: 2.2568 Acc: 41.86%
Epoch [7] Batch [240/352] Loss: 2.0415 Acc: 41.91%
Epoch [7] Batch [250/352] Loss: 2.0332 Acc: 41.80%
Epoch [7] Batch [260/352] Loss: 2.1907 Acc: 41.75%
Epoch [7] Batch [270/352] Loss: 2.0861 Acc: 41.77%
Epoch [7] Batch [280/352] Loss: 2.1687 Acc: 41.77%
Epoch [7] Batch [290/352] Loss: 2.1569 Acc: 41.76%
Epoch [7] Batch [300/352] Loss: 2.1019 Acc: 41.75%
Epoch [7] Batch [310/352] Loss: 1.9198 Acc: 41.80%
Epoch [7] Batch [320/352] Loss: 2.3569 Acc: 41.71%
Epoch [7] Batch [330/352] Loss: 2.1924 Acc: 41.72%
Epoch [7] Batch [340/352] Loss: 2.1426 Acc: 41.74%
Epoch [7] Batch [350/352] Loss: 2.0979 Acc: 41.80%

======================================================================
Epoch 7/100
Train Loss: 2.1367, Train Acc: 41.80%
Val Loss: 3.0849, Val Acc: 29.66%
Time: 92.65s
======================================================================

Epoch [8] Batch [0/352] Loss: 2.0353 Acc: 48.44%
Epoch [8] Batch [10/352] Loss: 2.1427 Acc: 42.97%
Epoch [8] Batch [20/352] Loss: 1.7943 Acc: 42.49%
Epoch [8] Batch [30/352] Loss: 2.0321 Acc: 43.37%
Epoch [8] Batch [40/352] Loss: 2.0695 Acc: 44.17%
Epoch [8] Batch [50/352] Loss: 2.0647 Acc: 44.26%
Epoch [8] Batch [60/352] Loss: 1.9044 Acc: 44.68%
Epoch [8] Batch [70/352] Loss: 2.1441 Acc: 44.86%
Epoch [8] Batch [80/352] Loss: 2.0813 Acc: 44.64%
Epoch [8] Batch [90/352] Loss: 1.8922 Acc: 44.61%
Epoch [8] Batch [100/352] Loss: 2.2355 Acc: 44.38%
Epoch [8] Batch [110/352] Loss: 2.0767 Acc: 44.33%
Epoch [8] Batch [120/352] Loss: 2.0082 Acc: 44.14%
Epoch [8] Batch [130/352] Loss: 2.1385 Acc: 44.01%
Epoch [8] Batch [140/352] Loss: 2.0740 Acc: 44.15%
Epoch [8] Batch [150/352] Loss: 1.9857 Acc: 43.92%
Epoch [8] Batch [160/352] Loss: 2.0495 Acc: 43.98%
Epoch [8] Batch [170/352] Loss: 1.8158 Acc: 44.02%
Epoch [8] Batch [180/352] Loss: 2.0205 Acc: 43.98%
Epoch [8] Batch [190/352] Loss: 2.0722 Acc: 43.97%
Epoch [8] Batch [200/352] Loss: 2.3230 Acc: 43.83%
Epoch [8] Batch [210/352] Loss: 2.0630 Acc: 43.82%
Epoch [8] Batch [220/352] Loss: 1.9822 Acc: 43.87%
Epoch [8] Batch [230/352] Loss: 1.9935 Acc: 44.03%
Epoch [8] Batch [240/352] Loss: 2.1438 Acc: 44.14%
Epoch [8] Batch [250/352] Loss: 2.2683 Acc: 44.19%
Epoch [8] Batch [260/352] Loss: 2.2048 Acc: 44.15%
Epoch [8] Batch [270/352] Loss: 2.0497 Acc: 44.17%
Epoch [8] Batch [280/352] Loss: 2.0613 Acc: 44.19%
Epoch [8] Batch [290/352] Loss: 2.0787 Acc: 44.25%
Epoch [8] Batch [300/352] Loss: 1.9960 Acc: 44.29%
Epoch [8] Batch [310/352] Loss: 2.3584 Acc: 44.37%
Epoch [8] Batch [320/352] Loss: 1.9687 Acc: 44.44%
Epoch [8] Batch [330/352] Loss: 1.8724 Acc: 44.42%
Epoch [8] Batch [340/352] Loss: 1.8845 Acc: 44.45%
Epoch [8] Batch [350/352] Loss: 2.1020 Acc: 44.56%

======================================================================
Epoch 8/100
Train Loss: 2.0381, Train Acc: 44.56%
Val Loss: 2.2301, Val Acc: 40.96%
Time: 97.63s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 40.96%)
Epoch [9] Batch [0/352] Loss: 2.0689 Acc: 40.62%
Epoch [9] Batch [10/352] Loss: 1.9700 Acc: 47.30%
Epoch [9] Batch [20/352] Loss: 2.2174 Acc: 46.24%
Epoch [9] Batch [30/352] Loss: 1.7594 Acc: 46.60%
Epoch [9] Batch [40/352] Loss: 1.7705 Acc: 46.44%
Epoch [9] Batch [50/352] Loss: 2.2264 Acc: 46.55%
Epoch [9] Batch [60/352] Loss: 1.8122 Acc: 46.20%
Epoch [9] Batch [70/352] Loss: 2.0180 Acc: 46.15%
Epoch [9] Batch [80/352] Loss: 2.2019 Acc: 45.92%
Epoch [9] Batch [90/352] Loss: 1.8862 Acc: 45.73%
Epoch [9] Batch [100/352] Loss: 1.8670 Acc: 45.99%
Epoch [9] Batch [110/352] Loss: 1.7923 Acc: 46.20%
Epoch [9] Batch [120/352] Loss: 1.6691 Acc: 46.38%
Epoch [9] Batch [130/352] Loss: 2.2907 Acc: 46.18%
Epoch [9] Batch [140/352] Loss: 2.1105 Acc: 46.10%
Epoch [9] Batch [150/352] Loss: 1.7276 Acc: 46.12%
Epoch [9] Batch [160/352] Loss: 1.8193 Acc: 46.05%
Epoch [9] Batch [170/352] Loss: 1.7072 Acc: 45.97%
Epoch [9] Batch [180/352] Loss: 1.7594 Acc: 45.99%
Epoch [9] Batch [190/352] Loss: 2.2203 Acc: 45.95%
Epoch [9] Batch [200/352] Loss: 1.9147 Acc: 45.91%
Epoch [9] Batch [210/352] Loss: 2.0108 Acc: 45.93%
Epoch [9] Batch [220/352] Loss: 2.0526 Acc: 46.01%
Epoch [9] Batch [230/352] Loss: 1.7642 Acc: 46.08%
Epoch [9] Batch [240/352] Loss: 2.1400 Acc: 45.96%
Epoch [9] Batch [250/352] Loss: 1.8727 Acc: 45.92%
Epoch [9] Batch [260/352] Loss: 2.2574 Acc: 45.91%
Epoch [9] Batch [270/352] Loss: 2.2176 Acc: 45.79%
Epoch [9] Batch [280/352] Loss: 2.0824 Acc: 45.77%
Epoch [9] Batch [290/352] Loss: 2.2175 Acc: 45.77%
Epoch [9] Batch [300/352] Loss: 1.9221 Acc: 45.72%
Epoch [9] Batch [310/352] Loss: 1.9993 Acc: 45.62%
Epoch [9] Batch [320/352] Loss: 2.0837 Acc: 45.65%
Epoch [9] Batch [330/352] Loss: 1.9561 Acc: 45.65%
Epoch [9] Batch [340/352] Loss: 2.0985 Acc: 45.72%
Epoch [9] Batch [350/352] Loss: 1.7247 Acc: 45.72%

======================================================================
Epoch 9/100
Train Loss: 1.9746, Train Acc: 45.74%
Val Loss: 2.0414, Val Acc: 44.98%
Time: 94.92s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 44.98%)
Epoch [10] Batch [0/352] Loss: 1.6644 Acc: 53.12%
Epoch [10] Batch [10/352] Loss: 1.9363 Acc: 48.30%
Epoch [10] Batch [20/352] Loss: 1.7807 Acc: 47.77%
Epoch [10] Batch [30/352] Loss: 1.9043 Acc: 47.33%
Epoch [10] Batch [40/352] Loss: 1.6714 Acc: 47.54%
Epoch [10] Batch [50/352] Loss: 1.8342 Acc: 47.32%
Epoch [10] Batch [60/352] Loss: 1.8840 Acc: 47.09%
Epoch [10] Batch [70/352] Loss: 2.0966 Acc: 46.85%
Epoch [10] Batch [80/352] Loss: 1.9438 Acc: 47.15%
Epoch [10] Batch [90/352] Loss: 1.8719 Acc: 47.29%
Epoch [10] Batch [100/352] Loss: 1.7158 Acc: 47.48%
Epoch [10] Batch [110/352] Loss: 2.0724 Acc: 47.45%
Epoch [10] Batch [120/352] Loss: 2.0364 Acc: 47.55%
Epoch [10] Batch [130/352] Loss: 2.0336 Acc: 47.40%
Epoch [10] Batch [140/352] Loss: 1.8241 Acc: 47.37%
Epoch [10] Batch [150/352] Loss: 1.6676 Acc: 47.17%
Epoch [10] Batch [160/352] Loss: 1.9879 Acc: 47.25%
Epoch [10] Batch [170/352] Loss: 1.9661 Acc: 47.14%
Epoch [10] Batch [180/352] Loss: 1.8798 Acc: 46.98%
Epoch [10] Batch [190/352] Loss: 1.9937 Acc: 46.93%
Epoch [10] Batch [200/352] Loss: 1.8385 Acc: 46.99%
Epoch [10] Batch [210/352] Loss: 1.6883 Acc: 46.97%
Epoch [10] Batch [220/352] Loss: 2.0558 Acc: 46.90%
Epoch [10] Batch [230/352] Loss: 1.8580 Acc: 46.82%
Epoch [10] Batch [240/352] Loss: 1.9823 Acc: 46.87%
Epoch [10] Batch [250/352] Loss: 1.9694 Acc: 46.95%
Epoch [10] Batch [260/352] Loss: 1.8422 Acc: 46.93%
Epoch [10] Batch [270/352] Loss: 2.1798 Acc: 46.90%
Epoch [10] Batch [280/352] Loss: 2.0358 Acc: 46.83%
Epoch [10] Batch [290/352] Loss: 2.0283 Acc: 46.89%
Epoch [10] Batch [300/352] Loss: 1.7745 Acc: 46.84%
Epoch [10] Batch [310/352] Loss: 2.0579 Acc: 46.80%
Epoch [10] Batch [320/352] Loss: 1.7574 Acc: 46.92%
Epoch [10] Batch [330/352] Loss: 1.8102 Acc: 46.98%
Epoch [10] Batch [340/352] Loss: 1.8586 Acc: 46.95%
Epoch [10] Batch [350/352] Loss: 1.8264 Acc: 47.00%

======================================================================
Epoch 10/100
Train Loss: 1.9160, Train Acc: 47.00%
Val Loss: 2.3504, Val Acc: 40.72%
Time: 94.14s
======================================================================

Epoch [11] Batch [0/352] Loss: 1.9505 Acc: 46.09%
Epoch [11] Batch [10/352] Loss: 1.9140 Acc: 50.92%
Epoch [11] Batch [20/352] Loss: 1.7951 Acc: 50.41%
Epoch [11] Batch [30/352] Loss: 1.8945 Acc: 49.70%
Epoch [11] Batch [40/352] Loss: 1.7081 Acc: 49.14%
Epoch [11] Batch [50/352] Loss: 1.9176 Acc: 48.50%
Epoch [11] Batch [60/352] Loss: 1.9695 Acc: 48.36%
Epoch [11] Batch [70/352] Loss: 1.8562 Acc: 48.47%
Epoch [11] Batch [80/352] Loss: 1.8710 Acc: 48.62%
Epoch [11] Batch [90/352] Loss: 1.7346 Acc: 48.45%
Epoch [11] Batch [100/352] Loss: 1.7867 Acc: 48.38%
Epoch [11] Batch [110/352] Loss: 1.7590 Acc: 48.41%
Epoch [11] Batch [120/352] Loss: 1.8606 Acc: 48.33%
Epoch [11] Batch [130/352] Loss: 1.8554 Acc: 48.27%
Epoch [11] Batch [140/352] Loss: 1.7545 Acc: 48.33%
Epoch [11] Batch [150/352] Loss: 1.9617 Acc: 48.22%
Epoch [11] Batch [160/352] Loss: 2.0080 Acc: 48.22%
Epoch [11] Batch [170/352] Loss: 2.0087 Acc: 48.14%
Epoch [11] Batch [180/352] Loss: 2.0024 Acc: 47.96%
Epoch [11] Batch [190/352] Loss: 2.0572 Acc: 48.03%
Epoch [11] Batch [200/352] Loss: 1.6641 Acc: 47.97%
Epoch [11] Batch [210/352] Loss: 1.9308 Acc: 47.99%
Epoch [11] Batch [220/352] Loss: 2.1201 Acc: 47.96%
Epoch [11] Batch [230/352] Loss: 1.7731 Acc: 47.97%
Epoch [11] Batch [240/352] Loss: 2.0416 Acc: 48.03%
Epoch [11] Batch [250/352] Loss: 1.8892 Acc: 48.10%
Epoch [11] Batch [260/352] Loss: 1.9464 Acc: 48.06%
Epoch [11] Batch [270/352] Loss: 1.8371 Acc: 48.10%
Epoch [11] Batch [280/352] Loss: 1.9978 Acc: 48.10%
Epoch [11] Batch [290/352] Loss: 2.0853 Acc: 48.03%
Epoch [11] Batch [300/352] Loss: 1.9618 Acc: 48.08%
Epoch [11] Batch [310/352] Loss: 1.8433 Acc: 48.07%
Epoch [11] Batch [320/352] Loss: 1.8415 Acc: 48.09%
Epoch [11] Batch [330/352] Loss: 1.6744 Acc: 48.11%
Epoch [11] Batch [340/352] Loss: 1.7416 Acc: 48.14%
Epoch [11] Batch [350/352] Loss: 1.8331 Acc: 48.15%

======================================================================
Epoch 11/100
Train Loss: 1.8706, Train Acc: 48.15%
Val Loss: 2.1019, Val Acc: 44.50%
Time: 94.12s
======================================================================

Epoch [12] Batch [0/352] Loss: 1.7737 Acc: 53.12%
Epoch [12] Batch [10/352] Loss: 1.5662 Acc: 50.36%
Epoch [12] Batch [20/352] Loss: 1.8285 Acc: 49.81%
Epoch [12] Batch [30/352] Loss: 1.9779 Acc: 49.80%
Epoch [12] Batch [40/352] Loss: 1.6640 Acc: 49.37%
Epoch [12] Batch [50/352] Loss: 1.9468 Acc: 49.37%
Epoch [12] Batch [60/352] Loss: 1.8743 Acc: 49.35%
Epoch [12] Batch [70/352] Loss: 1.7914 Acc: 49.37%
Epoch [12] Batch [80/352] Loss: 1.8288 Acc: 49.17%
Epoch [12] Batch [90/352] Loss: 1.7683 Acc: 49.17%
Epoch [12] Batch [100/352] Loss: 2.0109 Acc: 49.22%
Epoch [12] Batch [110/352] Loss: 1.8991 Acc: 49.37%
Epoch [12] Batch [120/352] Loss: 1.5288 Acc: 49.29%
Epoch [12] Batch [130/352] Loss: 1.9246 Acc: 49.48%
Epoch [12] Batch [140/352] Loss: 1.9004 Acc: 49.44%
Epoch [12] Batch [150/352] Loss: 1.5928 Acc: 49.35%
Epoch [12] Batch [160/352] Loss: 1.7638 Acc: 49.34%
Epoch [12] Batch [170/352] Loss: 1.8851 Acc: 49.25%
Epoch [12] Batch [180/352] Loss: 1.7032 Acc: 49.40%
Epoch [12] Batch [190/352] Loss: 1.7270 Acc: 49.45%
Epoch [12] Batch [200/352] Loss: 1.7215 Acc: 49.51%
Epoch [12] Batch [210/352] Loss: 1.8348 Acc: 49.54%
Epoch [12] Batch [220/352] Loss: 1.6232 Acc: 49.46%
Epoch [12] Batch [230/352] Loss: 1.8898 Acc: 49.51%
Epoch [12] Batch [240/352] Loss: 2.0859 Acc: 49.61%
Epoch [12] Batch [250/352] Loss: 1.9730 Acc: 49.58%
Epoch [12] Batch [260/352] Loss: 1.7058 Acc: 49.57%
Epoch [12] Batch [270/352] Loss: 1.7417 Acc: 49.59%
Epoch [12] Batch [280/352] Loss: 1.8786 Acc: 49.57%
Epoch [12] Batch [290/352] Loss: 1.6026 Acc: 49.59%
Epoch [12] Batch [300/352] Loss: 1.8807 Acc: 49.52%
Epoch [12] Batch [310/352] Loss: 1.6071 Acc: 49.52%
Epoch [12] Batch [320/352] Loss: 1.7650 Acc: 49.48%
Epoch [12] Batch [330/352] Loss: 2.0399 Acc: 49.47%
Epoch [12] Batch [340/352] Loss: 1.8632 Acc: 49.41%
Epoch [12] Batch [350/352] Loss: 1.9562 Acc: 49.40%

======================================================================
Epoch 12/100
Train Loss: 1.8242, Train Acc: 49.40%
Val Loss: 2.1118, Val Acc: 45.08%
Time: 94.27s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 45.08%)
Epoch [13] Batch [0/352] Loss: 1.5560 Acc: 56.25%
Epoch [13] Batch [10/352] Loss: 1.8679 Acc: 50.28%
Epoch [13] Batch [20/352] Loss: 1.6734 Acc: 50.19%
Epoch [13] Batch [30/352] Loss: 1.6332 Acc: 50.15%
Epoch [13] Batch [40/352] Loss: 1.5412 Acc: 50.25%
Epoch [13] Batch [50/352] Loss: 1.8304 Acc: 49.82%
Epoch [13] Batch [60/352] Loss: 1.7046 Acc: 50.14%
Epoch [13] Batch [70/352] Loss: 1.6094 Acc: 50.14%
Epoch [13] Batch [80/352] Loss: 1.9050 Acc: 50.23%
Epoch [13] Batch [90/352] Loss: 1.9188 Acc: 50.27%
Epoch [13] Batch [100/352] Loss: 1.6701 Acc: 50.74%
Epoch [13] Batch [110/352] Loss: 1.8114 Acc: 50.89%
Epoch [13] Batch [120/352] Loss: 1.6961 Acc: 50.81%
Epoch [13] Batch [130/352] Loss: 2.0406 Acc: 50.65%
Epoch [13] Batch [140/352] Loss: 1.8245 Acc: 50.58%
Epoch [13] Batch [150/352] Loss: 1.7123 Acc: 50.42%
Epoch [13] Batch [160/352] Loss: 1.5793 Acc: 50.41%
Epoch [13] Batch [170/352] Loss: 1.6015 Acc: 50.29%
Epoch [13] Batch [180/352] Loss: 1.9594 Acc: 50.25%
Epoch [13] Batch [190/352] Loss: 1.6790 Acc: 50.36%
Epoch [13] Batch [200/352] Loss: 1.7408 Acc: 50.52%
Epoch [13] Batch [210/352] Loss: 2.0534 Acc: 50.40%
Epoch [13] Batch [220/352] Loss: 1.7932 Acc: 50.44%
Epoch [13] Batch [230/352] Loss: 1.7169 Acc: 50.47%
Epoch [13] Batch [240/352] Loss: 1.7512 Acc: 50.54%
Epoch [13] Batch [250/352] Loss: 1.8374 Acc: 50.49%
Epoch [13] Batch [260/352] Loss: 1.9730 Acc: 50.52%
Epoch [13] Batch [270/352] Loss: 1.5548 Acc: 50.49%
Epoch [13] Batch [280/352] Loss: 1.7906 Acc: 50.50%
Epoch [13] Batch [290/352] Loss: 1.7600 Acc: 50.40%
Epoch [13] Batch [300/352] Loss: 1.8233 Acc: 50.33%
Epoch [13] Batch [310/352] Loss: 1.7373 Acc: 50.35%
Epoch [13] Batch [320/352] Loss: 1.8156 Acc: 50.34%
Epoch [13] Batch [330/352] Loss: 1.8507 Acc: 50.34%
Epoch [13] Batch [340/352] Loss: 1.6370 Acc: 50.37%
Epoch [13] Batch [350/352] Loss: 1.8203 Acc: 50.35%

======================================================================
Epoch 13/100
Train Loss: 1.7916, Train Acc: 50.35%
Val Loss: 2.5027, Val Acc: 37.72%
Time: 93.22s
======================================================================

Epoch [14] Batch [0/352] Loss: 1.7295 Acc: 53.91%
Epoch [14] Batch [10/352] Loss: 1.6949 Acc: 53.12%
Epoch [14] Batch [20/352] Loss: 2.0482 Acc: 51.41%
Epoch [14] Batch [30/352] Loss: 1.5179 Acc: 51.92%
Epoch [14] Batch [40/352] Loss: 1.6837 Acc: 51.71%
Epoch [14] Batch [50/352] Loss: 1.7577 Acc: 52.01%
Epoch [14] Batch [60/352] Loss: 1.6552 Acc: 51.74%
Epoch [14] Batch [70/352] Loss: 1.7763 Acc: 51.91%
Epoch [14] Batch [80/352] Loss: 1.5651 Acc: 51.97%
Epoch [14] Batch [90/352] Loss: 1.6977 Acc: 51.93%
Epoch [14] Batch [100/352] Loss: 1.6389 Acc: 51.49%
Epoch [14] Batch [110/352] Loss: 1.7233 Acc: 51.41%
Epoch [14] Batch [120/352] Loss: 1.8531 Acc: 51.34%
Epoch [14] Batch [130/352] Loss: 2.0442 Acc: 51.32%
Epoch [14] Batch [140/352] Loss: 2.0295 Acc: 51.15%
Epoch [14] Batch [150/352] Loss: 1.6815 Acc: 51.29%
Epoch [14] Batch [160/352] Loss: 1.9755 Acc: 51.30%
Epoch [14] Batch [170/352] Loss: 1.5701 Acc: 51.43%
Epoch [14] Batch [180/352] Loss: 1.7439 Acc: 51.28%
Epoch [14] Batch [190/352] Loss: 1.6757 Acc: 51.31%
Epoch [14] Batch [200/352] Loss: 1.5076 Acc: 51.33%
Epoch [14] Batch [210/352] Loss: 1.9470 Acc: 51.28%
Epoch [14] Batch [220/352] Loss: 1.7947 Acc: 51.23%
Epoch [14] Batch [230/352] Loss: 1.8271 Acc: 51.24%
Epoch [14] Batch [240/352] Loss: 1.8718 Acc: 51.17%
Epoch [14] Batch [250/352] Loss: 1.8147 Acc: 51.17%
Epoch [14] Batch [260/352] Loss: 1.7040 Acc: 51.10%
Epoch [14] Batch [270/352] Loss: 1.6666 Acc: 51.04%
Epoch [14] Batch [280/352] Loss: 1.7568 Acc: 51.04%
Epoch [14] Batch [290/352] Loss: 1.8312 Acc: 51.04%
Epoch [14] Batch [300/352] Loss: 1.7593 Acc: 50.97%
Epoch [14] Batch [310/352] Loss: 1.6159 Acc: 50.91%
Epoch [14] Batch [320/352] Loss: 1.5652 Acc: 50.90%
Epoch [14] Batch [330/352] Loss: 1.7202 Acc: 50.93%
Epoch [14] Batch [340/352] Loss: 1.4934 Acc: 50.96%
Epoch [14] Batch [350/352] Loss: 1.6287 Acc: 50.97%

======================================================================
Epoch 14/100
Train Loss: 1.7562, Train Acc: 50.97%
Val Loss: 2.3673, Val Acc: 39.02%
Time: 92.71s
======================================================================

Epoch [15] Batch [0/352] Loss: 1.8091 Acc: 46.88%
Epoch [15] Batch [10/352] Loss: 1.3655 Acc: 53.41%
Epoch [15] Batch [20/352] Loss: 1.6099 Acc: 52.83%
Epoch [15] Batch [30/352] Loss: 1.7237 Acc: 52.60%
Epoch [15] Batch [40/352] Loss: 1.8894 Acc: 52.31%
Epoch [15] Batch [50/352] Loss: 1.5354 Acc: 52.31%
Epoch [15] Batch [60/352] Loss: 1.6003 Acc: 52.31%
Epoch [15] Batch [70/352] Loss: 1.6686 Acc: 52.28%
Epoch [15] Batch [80/352] Loss: 1.8620 Acc: 52.31%
Epoch [15] Batch [90/352] Loss: 1.8034 Acc: 52.00%
Epoch [15] Batch [100/352] Loss: 1.9745 Acc: 51.83%
Epoch [15] Batch [110/352] Loss: 1.7762 Acc: 52.06%
Epoch [15] Batch [120/352] Loss: 1.7090 Acc: 51.91%
Epoch [15] Batch [130/352] Loss: 1.5180 Acc: 52.10%
Epoch [15] Batch [140/352] Loss: 1.7187 Acc: 51.92%
Epoch [15] Batch [150/352] Loss: 1.7732 Acc: 51.85%
Epoch [15] Batch [160/352] Loss: 1.7012 Acc: 51.81%
Epoch [15] Batch [170/352] Loss: 1.7403 Acc: 51.76%
Epoch [15] Batch [180/352] Loss: 1.8150 Acc: 51.66%
Epoch [15] Batch [190/352] Loss: 1.8967 Acc: 51.53%
Epoch [15] Batch [200/352] Loss: 1.4618 Acc: 51.41%
Epoch [15] Batch [210/352] Loss: 1.7810 Acc: 51.42%
Epoch [15] Batch [220/352] Loss: 1.6081 Acc: 51.41%
Epoch [15] Batch [230/352] Loss: 1.6843 Acc: 51.56%
Epoch [15] Batch [240/352] Loss: 1.7001 Acc: 51.56%
Epoch [15] Batch [250/352] Loss: 1.8562 Acc: 51.58%
Epoch [15] Batch [260/352] Loss: 1.6349 Acc: 51.64%
Epoch [15] Batch [270/352] Loss: 1.8161 Acc: 51.57%
Epoch [15] Batch [280/352] Loss: 1.7491 Acc: 51.60%
Epoch [15] Batch [290/352] Loss: 1.7792 Acc: 51.63%
Epoch [15] Batch [300/352] Loss: 1.6741 Acc: 51.60%
Epoch [15] Batch [310/352] Loss: 1.7590 Acc: 51.64%
Epoch [15] Batch [320/352] Loss: 1.6434 Acc: 51.60%
Epoch [15] Batch [330/352] Loss: 1.5536 Acc: 51.58%
Epoch [15] Batch [340/352] Loss: 1.7859 Acc: 51.55%
Epoch [15] Batch [350/352] Loss: 1.7244 Acc: 51.58%

======================================================================
Epoch 15/100
Train Loss: 1.7274, Train Acc: 51.57%
Val Loss: 2.0305, Val Acc: 46.70%
Time: 92.43s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 46.70%)
Epoch [16] Batch [0/352] Loss: 1.4734 Acc: 57.03%
Epoch [16] Batch [10/352] Loss: 1.6502 Acc: 53.48%
Epoch [16] Batch [20/352] Loss: 1.7440 Acc: 53.05%
Epoch [16] Batch [30/352] Loss: 1.8708 Acc: 53.38%
Epoch [16] Batch [40/352] Loss: 1.5369 Acc: 53.20%
Epoch [16] Batch [50/352] Loss: 1.6046 Acc: 53.57%
Epoch [16] Batch [60/352] Loss: 2.0151 Acc: 52.87%
Epoch [16] Batch [70/352] Loss: 1.7331 Acc: 52.60%
Epoch [16] Batch [80/352] Loss: 1.6642 Acc: 52.40%
Epoch [16] Batch [90/352] Loss: 1.8012 Acc: 52.36%
Epoch [16] Batch [100/352] Loss: 1.8527 Acc: 52.44%
Epoch [16] Batch [110/352] Loss: 1.4886 Acc: 52.54%
Epoch [16] Batch [120/352] Loss: 1.4233 Acc: 52.85%
Epoch [16] Batch [130/352] Loss: 1.6007 Acc: 52.79%
Epoch [16] Batch [140/352] Loss: 1.7324 Acc: 52.48%
Epoch [16] Batch [150/352] Loss: 1.9056 Acc: 52.38%
Epoch [16] Batch [160/352] Loss: 1.9114 Acc: 52.47%
Epoch [16] Batch [170/352] Loss: 1.8293 Acc: 52.50%
Epoch [16] Batch [180/352] Loss: 1.7238 Acc: 52.46%
Epoch [16] Batch [190/352] Loss: 1.9069 Acc: 52.45%
Epoch [16] Batch [200/352] Loss: 1.8312 Acc: 52.50%
Epoch [16] Batch [210/352] Loss: 1.7253 Acc: 52.40%
Epoch [16] Batch [220/352] Loss: 1.7392 Acc: 52.26%
Epoch [16] Batch [230/352] Loss: 1.7716 Acc: 52.21%
Epoch [16] Batch [240/352] Loss: 1.7123 Acc: 52.15%
Epoch [16] Batch [250/352] Loss: 1.8204 Acc: 52.16%
Epoch [16] Batch [260/352] Loss: 1.8177 Acc: 52.23%
Epoch [16] Batch [270/352] Loss: 1.6451 Acc: 52.25%
Epoch [16] Batch [280/352] Loss: 2.1162 Acc: 52.23%
Epoch [16] Batch [290/352] Loss: 1.7569 Acc: 52.25%
Epoch [16] Batch [300/352] Loss: 1.8477 Acc: 52.26%
Epoch [16] Batch [310/352] Loss: 1.5044 Acc: 52.24%
Epoch [16] Batch [320/352] Loss: 1.6188 Acc: 52.29%
Epoch [16] Batch [330/352] Loss: 1.7257 Acc: 52.28%
Epoch [16] Batch [340/352] Loss: 1.5688 Acc: 52.25%
Epoch [16] Batch [350/352] Loss: 1.5043 Acc: 52.27%

======================================================================
Epoch 16/100
Train Loss: 1.7124, Train Acc: 52.26%
Val Loss: 1.9592, Val Acc: 47.42%
Time: 92.69s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 47.42%)
Epoch [17] Batch [0/352] Loss: 1.4487 Acc: 56.25%
Epoch [17] Batch [10/352] Loss: 1.8233 Acc: 52.77%
Epoch [17] Batch [20/352] Loss: 1.7974 Acc: 52.60%
Epoch [17] Batch [30/352] Loss: 1.6156 Acc: 53.45%
Epoch [17] Batch [40/352] Loss: 1.7279 Acc: 52.97%
Epoch [17] Batch [50/352] Loss: 1.7765 Acc: 52.67%
Epoch [17] Batch [60/352] Loss: 1.3898 Acc: 52.75%
Epoch [17] Batch [70/352] Loss: 1.5943 Acc: 52.44%
Epoch [17] Batch [80/352] Loss: 1.6210 Acc: 52.36%
Epoch [17] Batch [90/352] Loss: 1.8747 Acc: 52.52%
Epoch [17] Batch [100/352] Loss: 1.7756 Acc: 52.42%
Epoch [17] Batch [110/352] Loss: 1.5813 Acc: 52.36%
Epoch [17] Batch [120/352] Loss: 1.7218 Acc: 52.42%
Epoch [17] Batch [130/352] Loss: 1.5423 Acc: 52.49%
Epoch [17] Batch [140/352] Loss: 1.4638 Acc: 52.42%
Epoch [17] Batch [150/352] Loss: 1.6610 Acc: 52.43%
Epoch [17] Batch [160/352] Loss: 1.5848 Acc: 52.36%
Epoch [17] Batch [170/352] Loss: 1.7060 Acc: 52.36%
Epoch [17] Batch [180/352] Loss: 2.0588 Acc: 52.37%
Epoch [17] Batch [190/352] Loss: 1.8085 Acc: 52.33%
Epoch [17] Batch [200/352] Loss: 1.6205 Acc: 52.41%
Epoch [17] Batch [210/352] Loss: 1.5560 Acc: 52.47%
Epoch [17] Batch [220/352] Loss: 1.5045 Acc: 52.45%
Epoch [17] Batch [230/352] Loss: 1.7213 Acc: 52.51%
Epoch [17] Batch [240/352] Loss: 1.8764 Acc: 52.50%
Epoch [17] Batch [250/352] Loss: 1.8903 Acc: 52.54%
Epoch [17] Batch [260/352] Loss: 1.6215 Acc: 52.49%
Epoch [17] Batch [270/352] Loss: 1.7242 Acc: 52.46%
Epoch [17] Batch [280/352] Loss: 1.8645 Acc: 52.49%
Epoch [17] Batch [290/352] Loss: 1.5333 Acc: 52.51%
Epoch [17] Batch [300/352] Loss: 1.6808 Acc: 52.54%
Epoch [17] Batch [310/352] Loss: 1.8903 Acc: 52.49%
Epoch [17] Batch [320/352] Loss: 1.9561 Acc: 52.48%
Epoch [17] Batch [330/352] Loss: 1.7200 Acc: 52.44%
Epoch [17] Batch [340/352] Loss: 1.7930 Acc: 52.41%
Epoch [17] Batch [350/352] Loss: 1.7462 Acc: 52.40%

======================================================================
Epoch 17/100
Train Loss: 1.6881, Train Acc: 52.41%
Val Loss: 1.8912, Val Acc: 48.88%
Time: 92.38s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 48.88%)
Epoch [18] Batch [0/352] Loss: 1.6794 Acc: 56.25%
Epoch [18] Batch [10/352] Loss: 1.5875 Acc: 54.33%
Epoch [18] Batch [20/352] Loss: 1.5723 Acc: 54.58%
Epoch [18] Batch [30/352] Loss: 1.6755 Acc: 54.08%
Epoch [18] Batch [40/352] Loss: 1.7743 Acc: 54.10%
Epoch [18] Batch [50/352] Loss: 1.6571 Acc: 54.20%
Epoch [18] Batch [60/352] Loss: 1.5003 Acc: 54.84%
Epoch [18] Batch [70/352] Loss: 1.5625 Acc: 54.64%
Epoch [18] Batch [80/352] Loss: 1.7490 Acc: 54.43%
Epoch [18] Batch [90/352] Loss: 1.5555 Acc: 54.22%
Epoch [18] Batch [100/352] Loss: 1.7593 Acc: 54.11%
Epoch [18] Batch [110/352] Loss: 1.6525 Acc: 54.09%
Epoch [18] Batch [120/352] Loss: 1.4754 Acc: 54.25%
Epoch [18] Batch [130/352] Loss: 1.6052 Acc: 54.22%
Epoch [18] Batch [140/352] Loss: 1.7004 Acc: 54.16%
Epoch [18] Batch [150/352] Loss: 1.4117 Acc: 54.11%
Epoch [18] Batch [160/352] Loss: 1.4760 Acc: 54.01%
Epoch [18] Batch [170/352] Loss: 1.6373 Acc: 53.88%
Epoch [18] Batch [180/352] Loss: 1.5136 Acc: 53.91%
Epoch [18] Batch [190/352] Loss: 1.8093 Acc: 53.68%
Epoch [18] Batch [200/352] Loss: 1.8367 Acc: 53.60%
Epoch [18] Batch [210/352] Loss: 1.8525 Acc: 53.42%
Epoch [18] Batch [220/352] Loss: 1.8883 Acc: 53.40%
Epoch [18] Batch [230/352] Loss: 1.8230 Acc: 53.34%
Epoch [18] Batch [240/352] Loss: 1.8009 Acc: 53.37%
Epoch [18] Batch [250/352] Loss: 1.7315 Acc: 53.35%
Epoch [18] Batch [260/352] Loss: 1.6477 Acc: 53.35%
Epoch [18] Batch [270/352] Loss: 1.7451 Acc: 53.38%
Epoch [18] Batch [280/352] Loss: 1.6689 Acc: 53.35%
Epoch [18] Batch [290/352] Loss: 1.5520 Acc: 53.34%
Epoch [18] Batch [300/352] Loss: 1.5898 Acc: 53.38%
Epoch [18] Batch [310/352] Loss: 1.6314 Acc: 53.33%
Epoch [18] Batch [320/352] Loss: 1.6438 Acc: 53.29%
Epoch [18] Batch [330/352] Loss: 1.6889 Acc: 53.33%
Epoch [18] Batch [340/352] Loss: 1.7206 Acc: 53.26%
Epoch [18] Batch [350/352] Loss: 1.8805 Acc: 53.23%

======================================================================
Epoch 18/100
Train Loss: 1.6680, Train Acc: 53.23%
Val Loss: 2.2022, Val Acc: 43.60%
Time: 92.48s
======================================================================

Epoch [19] Batch [0/352] Loss: 1.5503 Acc: 52.34%
Epoch [19] Batch [10/352] Loss: 1.6834 Acc: 55.18%
Epoch [19] Batch [20/352] Loss: 1.5223 Acc: 54.84%
Epoch [19] Batch [30/352] Loss: 1.5065 Acc: 54.89%
Epoch [19] Batch [40/352] Loss: 1.6464 Acc: 54.88%
Epoch [19] Batch [50/352] Loss: 1.5683 Acc: 54.49%
Epoch [19] Batch [60/352] Loss: 1.5289 Acc: 54.46%
Epoch [19] Batch [70/352] Loss: 1.7071 Acc: 54.69%
Epoch [19] Batch [80/352] Loss: 1.9491 Acc: 54.50%
Epoch [19] Batch [90/352] Loss: 1.6789 Acc: 54.60%
Epoch [19] Batch [100/352] Loss: 1.3403 Acc: 54.57%
Epoch [19] Batch [110/352] Loss: 1.7520 Acc: 54.31%
Epoch [19] Batch [120/352] Loss: 1.6813 Acc: 54.42%
Epoch [19] Batch [130/352] Loss: 1.5482 Acc: 54.43%
Epoch [19] Batch [140/352] Loss: 1.7523 Acc: 54.37%
Epoch [19] Batch [150/352] Loss: 1.6386 Acc: 54.48%
Epoch [19] Batch [160/352] Loss: 1.6741 Acc: 54.51%
Epoch [19] Batch [170/352] Loss: 1.4173 Acc: 54.32%
Epoch [19] Batch [180/352] Loss: 1.6495 Acc: 54.20%
Epoch [19] Batch [190/352] Loss: 2.0944 Acc: 54.28%
Epoch [19] Batch [200/352] Loss: 1.6146 Acc: 54.21%
Epoch [19] Batch [210/352] Loss: 1.4545 Acc: 54.21%
Epoch [19] Batch [220/352] Loss: 1.6480 Acc: 54.20%
Epoch [19] Batch [230/352] Loss: 1.4513 Acc: 54.23%
Epoch [19] Batch [240/352] Loss: 1.6328 Acc: 54.16%
Epoch [19] Batch [250/352] Loss: 1.4249 Acc: 54.16%
Epoch [19] Batch [260/352] Loss: 1.5901 Acc: 54.15%
Epoch [19] Batch [270/352] Loss: 1.6131 Acc: 54.13%
Epoch [19] Batch [280/352] Loss: 1.6429 Acc: 54.13%
Epoch [19] Batch [290/352] Loss: 1.7611 Acc: 54.10%
Epoch [19] Batch [300/352] Loss: 1.7813 Acc: 54.05%
Epoch [19] Batch [310/352] Loss: 1.7218 Acc: 53.95%
Epoch [19] Batch [320/352] Loss: 1.7620 Acc: 53.92%
Epoch [19] Batch [330/352] Loss: 1.6606 Acc: 53.92%
Epoch [19] Batch [340/352] Loss: 1.5123 Acc: 53.92%
Epoch [19] Batch [350/352] Loss: 1.7336 Acc: 53.92%

======================================================================
Epoch 19/100
Train Loss: 1.6427, Train Acc: 53.90%
Val Loss: 1.9062, Val Acc: 49.26%
Time: 92.37s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 49.26%)
Epoch [20] Batch [0/352] Loss: 1.3999 Acc: 60.94%
Epoch [20] Batch [10/352] Loss: 1.4741 Acc: 56.61%
Epoch [20] Batch [20/352] Loss: 1.3792 Acc: 56.55%
Epoch [20] Batch [30/352] Loss: 1.6194 Acc: 55.17%
Epoch [20] Batch [40/352] Loss: 1.7413 Acc: 55.09%
Epoch [20] Batch [50/352] Loss: 1.7277 Acc: 54.69%
Epoch [20] Batch [60/352] Loss: 1.6801 Acc: 54.60%
Epoch [20] Batch [70/352] Loss: 1.6107 Acc: 54.68%
Epoch [20] Batch [80/352] Loss: 1.5777 Acc: 54.60%
Epoch [20] Batch [90/352] Loss: 1.5959 Acc: 54.70%
Epoch [20] Batch [100/352] Loss: 1.3223 Acc: 54.55%
Epoch [20] Batch [110/352] Loss: 1.3772 Acc: 54.60%
Epoch [20] Batch [120/352] Loss: 1.5193 Acc: 54.64%
Epoch [20] Batch [130/352] Loss: 1.5273 Acc: 54.66%
Epoch [20] Batch [140/352] Loss: 1.5815 Acc: 54.84%
Epoch [20] Batch [150/352] Loss: 1.4904 Acc: 54.92%
Epoch [20] Batch [160/352] Loss: 1.5702 Acc: 54.99%
Epoch [20] Batch [170/352] Loss: 1.9638 Acc: 54.99%
Epoch [20] Batch [180/352] Loss: 1.5247 Acc: 55.12%
Epoch [20] Batch [190/352] Loss: 1.7375 Acc: 55.06%
Epoch [20] Batch [200/352] Loss: 1.4221 Acc: 55.03%
Epoch [20] Batch [210/352] Loss: 1.8647 Acc: 54.98%
Epoch [20] Batch [220/352] Loss: 1.6285 Acc: 54.98%
Epoch [20] Batch [230/352] Loss: 1.6564 Acc: 54.91%
Epoch [20] Batch [240/352] Loss: 1.7431 Acc: 54.79%
Epoch [20] Batch [250/352] Loss: 1.5509 Acc: 54.77%
Epoch [20] Batch [260/352] Loss: 1.6388 Acc: 54.75%
Epoch [20] Batch [270/352] Loss: 1.5793 Acc: 54.65%
Epoch [20] Batch [280/352] Loss: 1.7501 Acc: 54.50%
Epoch [20] Batch [290/352] Loss: 1.6702 Acc: 54.48%
Epoch [20] Batch [300/352] Loss: 1.5885 Acc: 54.49%
Epoch [20] Batch [310/352] Loss: 1.6557 Acc: 54.46%
Epoch [20] Batch [320/352] Loss: 1.6614 Acc: 54.36%
Epoch [20] Batch [330/352] Loss: 1.6188 Acc: 54.37%
Epoch [20] Batch [340/352] Loss: 1.4910 Acc: 54.24%
Epoch [20] Batch [350/352] Loss: 1.3546 Acc: 54.22%

======================================================================
Epoch 20/100
Train Loss: 1.6314, Train Acc: 54.23%
Val Loss: 2.6699, Val Acc: 37.28%
Time: 93.46s
======================================================================

Epoch [21] Batch [0/352] Loss: 1.5589 Acc: 53.12%
Epoch [21] Batch [10/352] Loss: 1.4838 Acc: 53.20%
Epoch [21] Batch [20/352] Loss: 1.5069 Acc: 54.09%
Epoch [21] Batch [30/352] Loss: 1.5156 Acc: 55.19%
Epoch [21] Batch [40/352] Loss: 1.4867 Acc: 55.32%
Epoch [21] Batch [50/352] Loss: 1.6179 Acc: 54.98%
Epoch [21] Batch [60/352] Loss: 1.6320 Acc: 55.35%
Epoch [21] Batch [70/352] Loss: 1.6513 Acc: 55.53%
Epoch [21] Batch [80/352] Loss: 1.7792 Acc: 55.20%
Epoch [21] Batch [90/352] Loss: 1.7031 Acc: 54.94%
Epoch [21] Batch [100/352] Loss: 1.5222 Acc: 54.99%
Epoch [21] Batch [110/352] Loss: 1.5262 Acc: 55.00%
Epoch [21] Batch [120/352] Loss: 1.4750 Acc: 55.05%
Epoch [21] Batch [130/352] Loss: 1.6710 Acc: 55.00%
Epoch [21] Batch [140/352] Loss: 1.6561 Acc: 54.90%
Epoch [21] Batch [150/352] Loss: 1.6635 Acc: 54.89%
Epoch [21] Batch [160/352] Loss: 1.5028 Acc: 54.96%
Epoch [21] Batch [170/352] Loss: 1.5728 Acc: 54.92%
Epoch [21] Batch [180/352] Loss: 1.9266 Acc: 54.84%
Epoch [21] Batch [190/352] Loss: 1.5559 Acc: 54.85%
Epoch [21] Batch [200/352] Loss: 1.8900 Acc: 54.84%
Epoch [21] Batch [210/352] Loss: 1.6226 Acc: 54.82%
Epoch [21] Batch [220/352] Loss: 1.7909 Acc: 54.72%
Epoch [21] Batch [230/352] Loss: 1.5233 Acc: 54.78%
Epoch [21] Batch [240/352] Loss: 1.5762 Acc: 54.79%
Epoch [21] Batch [250/352] Loss: 1.7613 Acc: 54.69%
Epoch [21] Batch [260/352] Loss: 1.5750 Acc: 54.59%
Epoch [21] Batch [270/352] Loss: 1.5800 Acc: 54.65%
Epoch [21] Batch [280/352] Loss: 1.6532 Acc: 54.70%
Epoch [21] Batch [290/352] Loss: 1.5494 Acc: 54.63%
Epoch [21] Batch [300/352] Loss: 1.8724 Acc: 54.70%
Epoch [21] Batch [310/352] Loss: 1.5456 Acc: 54.76%
Epoch [21] Batch [320/352] Loss: 1.5876 Acc: 54.67%
Epoch [21] Batch [330/352] Loss: 1.5254 Acc: 54.67%
Epoch [21] Batch [340/352] Loss: 1.6162 Acc: 54.63%
Epoch [21] Batch [350/352] Loss: 1.8542 Acc: 54.65%

======================================================================
Epoch 21/100
Train Loss: 1.6073, Train Acc: 54.66%
Val Loss: 1.8268, Val Acc: 50.06%
Time: 94.29s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 50.06%)
Epoch [22] Batch [0/352] Loss: 1.4074 Acc: 64.06%
Epoch [22] Batch [10/352] Loss: 1.4397 Acc: 57.81%
Epoch [22] Batch [20/352] Loss: 1.5095 Acc: 56.10%
Epoch [22] Batch [30/352] Loss: 1.9455 Acc: 55.49%
Epoch [22] Batch [40/352] Loss: 1.8648 Acc: 55.05%
Epoch [22] Batch [50/352] Loss: 1.6155 Acc: 54.98%
Epoch [22] Batch [60/352] Loss: 1.4049 Acc: 54.93%
Epoch [22] Batch [70/352] Loss: 1.3767 Acc: 55.15%
Epoch [22] Batch [80/352] Loss: 1.7309 Acc: 55.12%
Epoch [22] Batch [90/352] Loss: 1.5254 Acc: 55.27%
Epoch [22] Batch [100/352] Loss: 1.8959 Acc: 55.16%
Epoch [22] Batch [110/352] Loss: 1.6866 Acc: 55.34%
Epoch [22] Batch [120/352] Loss: 1.5858 Acc: 55.46%
Epoch [22] Batch [130/352] Loss: 1.5461 Acc: 55.37%
Epoch [22] Batch [140/352] Loss: 1.8062 Acc: 55.25%
Epoch [22] Batch [150/352] Loss: 1.4729 Acc: 55.19%
Epoch [22] Batch [160/352] Loss: 1.9213 Acc: 55.19%
Epoch [22] Batch [170/352] Loss: 1.5645 Acc: 55.25%
Epoch [22] Batch [180/352] Loss: 1.7396 Acc: 55.30%
Epoch [22] Batch [190/352] Loss: 1.5817 Acc: 55.31%
Epoch [22] Batch [200/352] Loss: 1.5554 Acc: 55.36%
Epoch [22] Batch [210/352] Loss: 1.6561 Acc: 55.26%
Epoch [22] Batch [220/352] Loss: 1.7978 Acc: 55.17%
Epoch [22] Batch [230/352] Loss: 1.6624 Acc: 55.14%
Epoch [22] Batch [240/352] Loss: 1.3582 Acc: 55.15%
Epoch [22] Batch [250/352] Loss: 1.5867 Acc: 55.13%
Epoch [22] Batch [260/352] Loss: 1.5142 Acc: 55.10%
Epoch [22] Batch [270/352] Loss: 1.5878 Acc: 55.07%
Epoch [22] Batch [280/352] Loss: 1.6676 Acc: 55.10%
Epoch [22] Batch [290/352] Loss: 1.5731 Acc: 55.00%
Epoch [22] Batch [300/352] Loss: 1.9356 Acc: 54.96%
Epoch [22] Batch [310/352] Loss: 1.7744 Acc: 54.87%
Epoch [22] Batch [320/352] Loss: 1.6075 Acc: 54.86%
Epoch [22] Batch [330/352] Loss: 1.7828 Acc: 54.84%
Epoch [22] Batch [340/352] Loss: 1.2704 Acc: 54.84%
Epoch [22] Batch [350/352] Loss: 1.6545 Acc: 54.83%

======================================================================
Epoch 22/100
Train Loss: 1.6046, Train Acc: 54.83%
Val Loss: 1.8288, Val Acc: 51.00%
Time: 94.01s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 51.00%)
Epoch [23] Batch [0/352] Loss: 1.4690 Acc: 58.59%
Epoch [23] Batch [10/352] Loss: 1.6279 Acc: 57.95%
Epoch [23] Batch [20/352] Loss: 1.4844 Acc: 56.70%
Epoch [23] Batch [30/352] Loss: 1.5834 Acc: 57.56%
Epoch [23] Batch [40/352] Loss: 1.5530 Acc: 57.36%
Epoch [23] Batch [50/352] Loss: 1.9273 Acc: 57.18%
Epoch [23] Batch [60/352] Loss: 1.6781 Acc: 57.24%
Epoch [23] Batch [70/352] Loss: 1.6267 Acc: 56.71%
Epoch [23] Batch [80/352] Loss: 1.5059 Acc: 56.51%
Epoch [23] Batch [90/352] Loss: 1.7374 Acc: 56.28%
Epoch [23] Batch [100/352] Loss: 1.5405 Acc: 56.07%
Epoch [23] Batch [110/352] Loss: 1.5899 Acc: 56.06%
Epoch [23] Batch [120/352] Loss: 1.6755 Acc: 56.00%
Epoch [23] Batch [130/352] Loss: 1.5983 Acc: 55.93%
Epoch [23] Batch [140/352] Loss: 1.8422 Acc: 55.93%
Epoch [23] Batch [150/352] Loss: 1.5447 Acc: 55.90%
Epoch [23] Batch [160/352] Loss: 1.5888 Acc: 55.87%
Epoch [23] Batch [170/352] Loss: 1.5421 Acc: 55.65%
Epoch [23] Batch [180/352] Loss: 1.6766 Acc: 55.58%
Epoch [23] Batch [190/352] Loss: 1.6276 Acc: 55.60%
Epoch [23] Batch [200/352] Loss: 1.3647 Acc: 55.58%
Epoch [23] Batch [210/352] Loss: 1.4941 Acc: 55.59%
Epoch [23] Batch [220/352] Loss: 1.6036 Acc: 55.64%
Epoch [23] Batch [230/352] Loss: 1.6382 Acc: 55.62%
Epoch [23] Batch [240/352] Loss: 1.7032 Acc: 55.50%
Epoch [23] Batch [250/352] Loss: 1.7786 Acc: 55.38%
Epoch [23] Batch [260/352] Loss: 1.7099 Acc: 55.35%
Epoch [23] Batch [270/352] Loss: 1.7071 Acc: 55.33%
Epoch [23] Batch [280/352] Loss: 1.6628 Acc: 55.24%
Epoch [23] Batch [290/352] Loss: 1.6525 Acc: 55.25%
Epoch [23] Batch [300/352] Loss: 1.7021 Acc: 55.16%
Epoch [23] Batch [310/352] Loss: 1.8221 Acc: 55.16%
Epoch [23] Batch [320/352] Loss: 1.6053 Acc: 55.08%
Epoch [23] Batch [330/352] Loss: 1.4931 Acc: 55.12%
Epoch [23] Batch [340/352] Loss: 1.7025 Acc: 55.02%
Epoch [23] Batch [350/352] Loss: 1.5888 Acc: 54.99%

======================================================================
Epoch 23/100
Train Loss: 1.5909, Train Acc: 55.00%
Val Loss: 2.1017, Val Acc: 46.84%
Time: 94.33s
======================================================================

Epoch [24] Batch [0/352] Loss: 1.7756 Acc: 47.66%
Epoch [24] Batch [10/352] Loss: 1.4258 Acc: 56.39%
Epoch [24] Batch [20/352] Loss: 1.6593 Acc: 56.21%
Epoch [24] Batch [30/352] Loss: 1.4439 Acc: 57.08%
Epoch [24] Batch [40/352] Loss: 1.3617 Acc: 57.01%
Epoch [24] Batch [50/352] Loss: 1.4681 Acc: 56.99%
Epoch [24] Batch [60/352] Loss: 1.7212 Acc: 56.78%
Epoch [24] Batch [70/352] Loss: 1.4110 Acc: 57.02%
Epoch [24] Batch [80/352] Loss: 1.4405 Acc: 56.75%
Epoch [24] Batch [90/352] Loss: 1.4314 Acc: 56.85%
Epoch [24] Batch [100/352] Loss: 1.6612 Acc: 56.81%
Epoch [24] Batch [110/352] Loss: 1.5985 Acc: 56.84%
Epoch [24] Batch [120/352] Loss: 1.5128 Acc: 56.75%
Epoch [24] Batch [130/352] Loss: 1.4804 Acc: 56.83%
Epoch [24] Batch [140/352] Loss: 1.6229 Acc: 56.73%
Epoch [24] Batch [150/352] Loss: 1.6385 Acc: 56.52%
Epoch [24] Batch [160/352] Loss: 1.4499 Acc: 56.41%
Epoch [24] Batch [170/352] Loss: 1.6032 Acc: 56.35%
Epoch [24] Batch [180/352] Loss: 1.3196 Acc: 56.46%
Epoch [24] Batch [190/352] Loss: 1.5678 Acc: 56.49%
Epoch [24] Batch [200/352] Loss: 1.6658 Acc: 56.45%
Epoch [24] Batch [210/352] Loss: 1.6219 Acc: 56.45%
Epoch [24] Batch [220/352] Loss: 1.6385 Acc: 56.47%
Epoch [24] Batch [230/352] Loss: 1.6709 Acc: 56.42%
Epoch [24] Batch [240/352] Loss: 1.7123 Acc: 56.31%
Epoch [24] Batch [250/352] Loss: 1.4563 Acc: 56.35%
Epoch [24] Batch [260/352] Loss: 1.4991 Acc: 56.32%
Epoch [24] Batch [270/352] Loss: 1.7362 Acc: 56.18%
Epoch [24] Batch [280/352] Loss: 1.8535 Acc: 56.12%
Epoch [24] Batch [290/352] Loss: 1.6177 Acc: 56.08%
Epoch [24] Batch [300/352] Loss: 1.5960 Acc: 56.04%
Epoch [24] Batch [310/352] Loss: 1.7066 Acc: 56.00%
Epoch [24] Batch [320/352] Loss: 1.6331 Acc: 55.97%
Epoch [24] Batch [330/352] Loss: 1.3845 Acc: 55.90%
Epoch [24] Batch [340/352] Loss: 1.7938 Acc: 55.84%
Epoch [24] Batch [350/352] Loss: 1.5173 Acc: 55.89%

======================================================================
Epoch 24/100
Train Loss: 1.5695, Train Acc: 55.90%
Val Loss: 1.8152, Val Acc: 50.70%
Time: 94.16s
======================================================================

Epoch [25] Batch [0/352] Loss: 1.3303 Acc: 57.03%
Epoch [25] Batch [10/352] Loss: 1.6673 Acc: 56.32%
Epoch [25] Batch [20/352] Loss: 1.7507 Acc: 57.44%
Epoch [25] Batch [30/352] Loss: 1.6733 Acc: 56.73%
Epoch [25] Batch [40/352] Loss: 2.0102 Acc: 56.36%
Epoch [25] Batch [50/352] Loss: 1.4071 Acc: 56.60%
Epoch [25] Batch [60/352] Loss: 1.6324 Acc: 56.75%
Epoch [25] Batch [70/352] Loss: 1.3581 Acc: 56.86%
Epoch [25] Batch [80/352] Loss: 1.4778 Acc: 57.17%
Epoch [25] Batch [90/352] Loss: 1.4732 Acc: 57.07%
Epoch [25] Batch [100/352] Loss: 1.5917 Acc: 57.20%
Epoch [25] Batch [110/352] Loss: 1.7023 Acc: 57.07%
Epoch [25] Batch [120/352] Loss: 1.5633 Acc: 57.04%
Epoch [25] Batch [130/352] Loss: 1.7554 Acc: 57.02%
Epoch [25] Batch [140/352] Loss: 1.2963 Acc: 56.76%
Epoch [25] Batch [150/352] Loss: 1.7414 Acc: 56.68%
Epoch [25] Batch [160/352] Loss: 1.5793 Acc: 56.73%
Epoch [25] Batch [170/352] Loss: 1.4807 Acc: 56.60%
Epoch [25] Batch [180/352] Loss: 1.3221 Acc: 56.59%
Epoch [25] Batch [190/352] Loss: 1.7247 Acc: 56.48%
Epoch [25] Batch [200/352] Loss: 1.4827 Acc: 56.41%
Epoch [25] Batch [210/352] Loss: 1.4943 Acc: 56.29%
Epoch [25] Batch [220/352] Loss: 1.4463 Acc: 56.19%
Epoch [25] Batch [230/352] Loss: 1.5223 Acc: 56.16%
Epoch [25] Batch [240/352] Loss: 1.8664 Acc: 56.14%
Epoch [25] Batch [250/352] Loss: 1.4531 Acc: 55.99%
Epoch [25] Batch [260/352] Loss: 1.5608 Acc: 55.94%
Epoch [25] Batch [270/352] Loss: 1.5436 Acc: 55.89%
Epoch [25] Batch [280/352] Loss: 1.3566 Acc: 55.87%
Epoch [25] Batch [290/352] Loss: 1.3634 Acc: 55.88%
Epoch [25] Batch [300/352] Loss: 1.6400 Acc: 55.95%
Epoch [25] Batch [310/352] Loss: 1.6332 Acc: 55.90%
Epoch [25] Batch [320/352] Loss: 1.5834 Acc: 55.86%
Epoch [25] Batch [330/352] Loss: 1.5062 Acc: 55.83%
Epoch [25] Batch [340/352] Loss: 1.5027 Acc: 55.87%
Epoch [25] Batch [350/352] Loss: 1.5705 Acc: 55.85%

======================================================================
Epoch 25/100
Train Loss: 1.5597, Train Acc: 55.85%
Val Loss: 1.8763, Val Acc: 50.08%
Time: 93.99s
======================================================================

Epoch [26] Batch [0/352] Loss: 1.4655 Acc: 59.38%
Epoch [26] Batch [10/352] Loss: 1.5166 Acc: 58.95%
Epoch [26] Batch [20/352] Loss: 1.4308 Acc: 58.30%
Epoch [26] Batch [30/352] Loss: 1.1871 Acc: 57.94%
Epoch [26] Batch [40/352] Loss: 1.8122 Acc: 57.26%
Epoch [26] Batch [50/352] Loss: 1.4440 Acc: 57.67%
Epoch [26] Batch [60/352] Loss: 1.5710 Acc: 57.61%
Epoch [26] Batch [70/352] Loss: 1.3827 Acc: 57.20%
Epoch [26] Batch [80/352] Loss: 1.7097 Acc: 57.20%
Epoch [26] Batch [90/352] Loss: 1.3405 Acc: 57.21%
Epoch [26] Batch [100/352] Loss: 1.6837 Acc: 57.19%
Epoch [26] Batch [110/352] Loss: 1.4902 Acc: 56.86%
Epoch [26] Batch [120/352] Loss: 1.4702 Acc: 56.64%
Epoch [26] Batch [130/352] Loss: 1.7820 Acc: 56.36%
Epoch [26] Batch [140/352] Loss: 1.6288 Acc: 56.26%
Epoch [26] Batch [150/352] Loss: 1.4331 Acc: 56.37%
Epoch [26] Batch [160/352] Loss: 1.3909 Acc: 56.27%
Epoch [26] Batch [170/352] Loss: 1.6646 Acc: 56.28%
Epoch [26] Batch [180/352] Loss: 1.7089 Acc: 56.26%
Epoch [26] Batch [190/352] Loss: 1.6590 Acc: 56.25%
Epoch [26] Batch [200/352] Loss: 1.4746 Acc: 56.17%
Epoch [26] Batch [210/352] Loss: 1.4900 Acc: 56.18%
Epoch [26] Batch [220/352] Loss: 1.5012 Acc: 56.23%
Epoch [26] Batch [230/352] Loss: 1.8231 Acc: 56.22%
Epoch [26] Batch [240/352] Loss: 1.7024 Acc: 56.22%
Epoch [26] Batch [250/352] Loss: 1.5921 Acc: 56.14%
Epoch [26] Batch [260/352] Loss: 1.6991 Acc: 56.18%
Epoch [26] Batch [270/352] Loss: 1.6487 Acc: 56.21%
Epoch [26] Batch [280/352] Loss: 1.6458 Acc: 56.20%
Epoch [26] Batch [290/352] Loss: 1.6214 Acc: 56.23%
Epoch [26] Batch [300/352] Loss: 1.7754 Acc: 56.23%
Epoch [26] Batch [310/352] Loss: 1.6039 Acc: 56.26%
Epoch [26] Batch [320/352] Loss: 1.6567 Acc: 56.27%
Epoch [26] Batch [330/352] Loss: 1.5031 Acc: 56.27%
Epoch [26] Batch [340/352] Loss: 1.4359 Acc: 56.30%
Epoch [26] Batch [350/352] Loss: 1.4269 Acc: 56.24%

======================================================================
Epoch 26/100
Train Loss: 1.5452, Train Acc: 56.24%
Val Loss: 1.8474, Val Acc: 50.32%
Time: 92.35s
======================================================================

Epoch [27] Batch [0/352] Loss: 1.7192 Acc: 55.47%
Epoch [27] Batch [10/352] Loss: 1.2579 Acc: 57.10%
Epoch [27] Batch [20/352] Loss: 1.4468 Acc: 56.73%
Epoch [27] Batch [30/352] Loss: 1.3244 Acc: 58.19%
Epoch [27] Batch [40/352] Loss: 1.4558 Acc: 58.06%
Epoch [27] Batch [50/352] Loss: 1.8642 Acc: 57.83%
Epoch [27] Batch [60/352] Loss: 1.3411 Acc: 58.04%
Epoch [27] Batch [70/352] Loss: 1.6316 Acc: 57.92%
Epoch [27] Batch [80/352] Loss: 1.7793 Acc: 57.79%
Epoch [27] Batch [90/352] Loss: 1.5972 Acc: 58.02%
Epoch [27] Batch [100/352] Loss: 1.5587 Acc: 58.15%
Epoch [27] Batch [110/352] Loss: 1.6516 Acc: 57.76%
Epoch [27] Batch [120/352] Loss: 1.4685 Acc: 57.62%
Epoch [27] Batch [130/352] Loss: 1.6553 Acc: 57.51%
Epoch [27] Batch [140/352] Loss: 1.6113 Acc: 57.48%
Epoch [27] Batch [150/352] Loss: 1.3588 Acc: 57.42%
Epoch [27] Batch [160/352] Loss: 1.5906 Acc: 57.19%
Epoch [27] Batch [170/352] Loss: 1.6521 Acc: 57.25%
Epoch [27] Batch [180/352] Loss: 1.4983 Acc: 57.24%
Epoch [27] Batch [190/352] Loss: 1.5358 Acc: 57.23%
Epoch [27] Batch [200/352] Loss: 1.5108 Acc: 57.19%
Epoch [27] Batch [210/352] Loss: 1.4226 Acc: 57.13%
Epoch [27] Batch [220/352] Loss: 1.4370 Acc: 57.11%
Epoch [27] Batch [230/352] Loss: 1.6076 Acc: 57.09%
Epoch [27] Batch [240/352] Loss: 1.6994 Acc: 57.12%
Epoch [27] Batch [250/352] Loss: 1.4450 Acc: 57.04%
Epoch [27] Batch [260/352] Loss: 1.5066 Acc: 57.01%
Epoch [27] Batch [270/352] Loss: 1.7736 Acc: 56.90%
Epoch [27] Batch [280/352] Loss: 1.5758 Acc: 56.75%
Epoch [27] Batch [290/352] Loss: 1.5242 Acc: 56.71%
Epoch [27] Batch [300/352] Loss: 1.7562 Acc: 56.65%
Epoch [27] Batch [310/352] Loss: 1.7029 Acc: 56.62%
Epoch [27] Batch [320/352] Loss: 1.6267 Acc: 56.59%
Epoch [27] Batch [330/352] Loss: 1.6396 Acc: 56.61%
Epoch [27] Batch [340/352] Loss: 1.7195 Acc: 56.58%
Epoch [27] Batch [350/352] Loss: 1.6015 Acc: 56.57%

======================================================================
Epoch 27/100
Train Loss: 1.5373, Train Acc: 56.57%
Val Loss: 2.0016, Val Acc: 47.30%
Time: 92.13s
======================================================================

Epoch [28] Batch [0/352] Loss: 1.3726 Acc: 60.16%
Epoch [28] Batch [10/352] Loss: 1.4048 Acc: 59.73%
Epoch [28] Batch [20/352] Loss: 1.5982 Acc: 59.56%
Epoch [28] Batch [30/352] Loss: 1.2841 Acc: 59.15%
Epoch [28] Batch [40/352] Loss: 1.2093 Acc: 59.17%
Epoch [28] Batch [50/352] Loss: 1.4129 Acc: 58.67%
Epoch [28] Batch [60/352] Loss: 1.4619 Acc: 58.58%
Epoch [28] Batch [70/352] Loss: 1.5473 Acc: 58.62%
Epoch [28] Batch [80/352] Loss: 1.2367 Acc: 58.39%
Epoch [28] Batch [90/352] Loss: 1.3212 Acc: 58.34%
Epoch [28] Batch [100/352] Loss: 1.4966 Acc: 58.25%
Epoch [28] Batch [110/352] Loss: 1.6786 Acc: 57.94%
Epoch [28] Batch [120/352] Loss: 1.3997 Acc: 57.80%
Epoch [28] Batch [130/352] Loss: 1.4476 Acc: 57.71%
Epoch [28] Batch [140/352] Loss: 1.4486 Acc: 57.68%
Epoch [28] Batch [150/352] Loss: 1.8221 Acc: 57.57%
Epoch [28] Batch [160/352] Loss: 1.3446 Acc: 57.44%
Epoch [28] Batch [170/352] Loss: 1.5165 Acc: 57.39%
Epoch [28] Batch [180/352] Loss: 1.7280 Acc: 57.32%
Epoch [28] Batch [190/352] Loss: 1.4902 Acc: 57.24%
Epoch [28] Batch [200/352] Loss: 1.9012 Acc: 57.18%
Epoch [28] Batch [210/352] Loss: 1.5977 Acc: 57.08%
Epoch [28] Batch [220/352] Loss: 1.5201 Acc: 57.00%
Epoch [28] Batch [230/352] Loss: 1.6851 Acc: 56.90%
Epoch [28] Batch [240/352] Loss: 1.2524 Acc: 56.90%
Epoch [28] Batch [250/352] Loss: 1.5325 Acc: 56.86%
Epoch [28] Batch [260/352] Loss: 1.4390 Acc: 56.88%
Epoch [28] Batch [270/352] Loss: 1.6533 Acc: 56.91%
Epoch [28] Batch [280/352] Loss: 1.6441 Acc: 56.82%
Epoch [28] Batch [290/352] Loss: 1.6346 Acc: 56.81%
Epoch [28] Batch [300/352] Loss: 1.7471 Acc: 56.82%
Epoch [28] Batch [310/352] Loss: 1.5354 Acc: 56.80%
Epoch [28] Batch [320/352] Loss: 1.6270 Acc: 56.72%
Epoch [28] Batch [330/352] Loss: 1.7724 Acc: 56.65%
Epoch [28] Batch [340/352] Loss: 1.6097 Acc: 56.73%
Epoch [28] Batch [350/352] Loss: 1.5946 Acc: 56.69%

======================================================================
Epoch 28/100
Train Loss: 1.5288, Train Acc: 56.68%
Val Loss: 1.9354, Val Acc: 47.86%
Time: 92.91s
======================================================================

Epoch [29] Batch [0/352] Loss: 1.4886 Acc: 59.38%
Epoch [29] Batch [10/352] Loss: 1.4853 Acc: 57.74%
Epoch [29] Batch [20/352] Loss: 1.1996 Acc: 57.40%
Epoch [29] Batch [30/352] Loss: 1.3930 Acc: 57.89%
Epoch [29] Batch [40/352] Loss: 1.5662 Acc: 57.39%
Epoch [29] Batch [50/352] Loss: 1.4132 Acc: 57.26%
Epoch [29] Batch [60/352] Loss: 1.4000 Acc: 57.54%
Epoch [29] Batch [70/352] Loss: 1.5684 Acc: 57.86%
Epoch [29] Batch [80/352] Loss: 1.6054 Acc: 57.91%
Epoch [29] Batch [90/352] Loss: 1.5114 Acc: 57.92%
Epoch [29] Batch [100/352] Loss: 1.5051 Acc: 57.96%
Epoch [29] Batch [110/352] Loss: 1.4366 Acc: 58.22%
Epoch [29] Batch [120/352] Loss: 1.3852 Acc: 58.14%
Epoch [29] Batch [130/352] Loss: 1.4571 Acc: 58.21%
Epoch [29] Batch [140/352] Loss: 1.3314 Acc: 58.17%
Epoch [29] Batch [150/352] Loss: 1.6177 Acc: 58.13%
Epoch [29] Batch [160/352] Loss: 1.5519 Acc: 58.16%
Epoch [29] Batch [170/352] Loss: 1.5425 Acc: 58.06%
Epoch [29] Batch [180/352] Loss: 1.6088 Acc: 57.91%
Epoch [29] Batch [190/352] Loss: 1.5186 Acc: 57.92%
Epoch [29] Batch [200/352] Loss: 1.4854 Acc: 57.80%
Epoch [29] Batch [210/352] Loss: 1.4311 Acc: 57.79%
Epoch [29] Batch [220/352] Loss: 1.6207 Acc: 57.71%
Epoch [29] Batch [230/352] Loss: 1.6895 Acc: 57.59%
Epoch [29] Batch [240/352] Loss: 1.5275 Acc: 57.48%
Epoch [29] Batch [250/352] Loss: 1.8543 Acc: 57.46%
Epoch [29] Batch [260/352] Loss: 1.5399 Acc: 57.39%
Epoch [29] Batch [270/352] Loss: 1.4992 Acc: 57.35%
Epoch [29] Batch [280/352] Loss: 1.7139 Acc: 57.29%
Epoch [29] Batch [290/352] Loss: 1.5319 Acc: 57.28%
Epoch [29] Batch [300/352] Loss: 1.6701 Acc: 57.17%
Epoch [29] Batch [310/352] Loss: 1.3257 Acc: 57.20%
Epoch [29] Batch [320/352] Loss: 1.4066 Acc: 57.19%
Epoch [29] Batch [330/352] Loss: 1.3250 Acc: 57.21%
Epoch [29] Batch [340/352] Loss: 1.6294 Acc: 57.20%
Epoch [29] Batch [350/352] Loss: 1.6976 Acc: 57.15%

======================================================================
Epoch 29/100
Train Loss: 1.5090, Train Acc: 57.13%
Val Loss: 1.7719, Val Acc: 51.38%
Time: 92.76s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 51.38%)
Epoch [30] Batch [0/352] Loss: 1.4064 Acc: 59.38%
Epoch [30] Batch [10/352] Loss: 1.4222 Acc: 57.95%
Epoch [30] Batch [20/352] Loss: 1.4360 Acc: 58.04%
Epoch [30] Batch [30/352] Loss: 1.4412 Acc: 58.24%
Epoch [30] Batch [40/352] Loss: 1.6970 Acc: 58.54%
Epoch [30] Batch [50/352] Loss: 1.8326 Acc: 58.29%
Epoch [30] Batch [60/352] Loss: 1.6186 Acc: 57.94%
Epoch [30] Batch [70/352] Loss: 1.4154 Acc: 57.98%
Epoch [30] Batch [80/352] Loss: 1.7340 Acc: 57.99%
Epoch [30] Batch [90/352] Loss: 1.6469 Acc: 58.14%
Epoch [30] Batch [100/352] Loss: 1.3267 Acc: 57.93%
Epoch [30] Batch [110/352] Loss: 1.6439 Acc: 57.87%
Epoch [30] Batch [120/352] Loss: 1.5586 Acc: 57.90%
Epoch [30] Batch [130/352] Loss: 1.5098 Acc: 57.79%
Epoch [30] Batch [140/352] Loss: 1.2996 Acc: 57.81%
Epoch [30] Batch [150/352] Loss: 1.3732 Acc: 57.82%
Epoch [30] Batch [160/352] Loss: 1.6013 Acc: 57.74%
Epoch [30] Batch [170/352] Loss: 1.4535 Acc: 57.58%
Epoch [30] Batch [180/352] Loss: 1.5612 Acc: 57.71%
Epoch [30] Batch [190/352] Loss: 1.4405 Acc: 57.71%
Epoch [30] Batch [200/352] Loss: 1.3544 Acc: 57.69%
Epoch [30] Batch [210/352] Loss: 1.5285 Acc: 57.69%
Epoch [30] Batch [220/352] Loss: 1.4272 Acc: 57.73%
Epoch [30] Batch [230/352] Loss: 1.3672 Acc: 57.77%
Epoch [30] Batch [240/352] Loss: 1.7249 Acc: 57.74%
Epoch [30] Batch [250/352] Loss: 1.5113 Acc: 57.62%
Epoch [30] Batch [260/352] Loss: 1.5480 Acc: 57.52%
Epoch [30] Batch [270/352] Loss: 1.4347 Acc: 57.50%
Epoch [30] Batch [280/352] Loss: 1.7422 Acc: 57.61%
Epoch [30] Batch [290/352] Loss: 1.4405 Acc: 57.61%
Epoch [30] Batch [300/352] Loss: 1.5661 Acc: 57.56%
Epoch [30] Batch [310/352] Loss: 1.6279 Acc: 57.52%
Epoch [30] Batch [320/352] Loss: 1.5154 Acc: 57.46%
Epoch [30] Batch [330/352] Loss: 1.5569 Acc: 57.41%
Epoch [30] Batch [340/352] Loss: 1.5002 Acc: 57.45%
Epoch [30] Batch [350/352] Loss: 1.5032 Acc: 57.42%

======================================================================
Epoch 30/100
Train Loss: 1.5003, Train Acc: 57.43%
Val Loss: 2.1196, Val Acc: 46.02%
Time: 92.24s
======================================================================

Epoch [31] Batch [0/352] Loss: 1.4841 Acc: 63.28%
Epoch [31] Batch [10/352] Loss: 1.2652 Acc: 60.58%
Epoch [31] Batch [20/352] Loss: 1.6577 Acc: 59.60%
Epoch [31] Batch [30/352] Loss: 1.3711 Acc: 59.15%
Epoch [31] Batch [40/352] Loss: 1.5317 Acc: 59.18%
Epoch [31] Batch [50/352] Loss: 1.4827 Acc: 58.96%
Epoch [31] Batch [60/352] Loss: 1.3787 Acc: 59.04%
Epoch [31] Batch [70/352] Loss: 1.2902 Acc: 59.10%
Epoch [31] Batch [80/352] Loss: 1.4927 Acc: 59.22%
Epoch [31] Batch [90/352] Loss: 1.3113 Acc: 59.19%
Epoch [31] Batch [100/352] Loss: 1.5722 Acc: 59.05%
Epoch [31] Batch [110/352] Loss: 1.6759 Acc: 58.85%
Epoch [31] Batch [120/352] Loss: 1.6398 Acc: 58.66%
Epoch [31] Batch [130/352] Loss: 1.3804 Acc: 58.56%
Epoch [31] Batch [140/352] Loss: 1.4280 Acc: 58.53%
Epoch [31] Batch [150/352] Loss: 1.4477 Acc: 58.43%
Epoch [31] Batch [160/352] Loss: 1.4136 Acc: 58.22%
Epoch [31] Batch [170/352] Loss: 1.7020 Acc: 58.17%
Epoch [31] Batch [180/352] Loss: 1.4171 Acc: 58.06%
Epoch [31] Batch [190/352] Loss: 1.4376 Acc: 58.09%
Epoch [31] Batch [200/352] Loss: 1.6070 Acc: 58.00%
Epoch [31] Batch [210/352] Loss: 1.5337 Acc: 57.91%
Epoch [31] Batch [220/352] Loss: 1.5266 Acc: 57.94%
Epoch [31] Batch [230/352] Loss: 1.7335 Acc: 57.93%
Epoch [31] Batch [240/352] Loss: 1.4692 Acc: 57.94%
Epoch [31] Batch [250/352] Loss: 1.5654 Acc: 58.01%
Epoch [31] Batch [260/352] Loss: 1.5302 Acc: 58.08%
Epoch [31] Batch [270/352] Loss: 1.6093 Acc: 58.05%
Epoch [31] Batch [280/352] Loss: 1.5514 Acc: 57.98%
Epoch [31] Batch [290/352] Loss: 1.4579 Acc: 57.98%
Epoch [31] Batch [300/352] Loss: 1.5400 Acc: 58.04%
Epoch [31] Batch [310/352] Loss: 1.5980 Acc: 57.97%
Epoch [31] Batch [320/352] Loss: 1.5525 Acc: 57.90%
Epoch [31] Batch [330/352] Loss: 1.4376 Acc: 57.88%
Epoch [31] Batch [340/352] Loss: 1.7278 Acc: 57.89%
Epoch [31] Batch [350/352] Loss: 1.3660 Acc: 57.84%

======================================================================
Epoch 31/100
Train Loss: 1.4878, Train Acc: 57.84%
Val Loss: 1.7903, Val Acc: 51.34%
Time: 92.12s
======================================================================

Epoch [32] Batch [0/352] Loss: 1.4998 Acc: 53.91%
Epoch [32] Batch [10/352] Loss: 1.3292 Acc: 59.38%
Epoch [32] Batch [20/352] Loss: 1.4122 Acc: 60.42%
Epoch [32] Batch [30/352] Loss: 1.4910 Acc: 60.13%
Epoch [32] Batch [40/352] Loss: 1.3600 Acc: 60.21%
Epoch [32] Batch [50/352] Loss: 1.3112 Acc: 60.34%
Epoch [32] Batch [60/352] Loss: 1.3748 Acc: 60.32%
Epoch [32] Batch [70/352] Loss: 1.4176 Acc: 60.23%
Epoch [32] Batch [80/352] Loss: 1.5541 Acc: 59.86%
Epoch [32] Batch [90/352] Loss: 1.3766 Acc: 59.65%
Epoch [32] Batch [100/352] Loss: 1.2793 Acc: 59.69%
Epoch [32] Batch [110/352] Loss: 1.7578 Acc: 59.50%
Epoch [32] Batch [120/352] Loss: 1.4054 Acc: 59.32%
Epoch [32] Batch [130/352] Loss: 1.3960 Acc: 59.33%
Epoch [32] Batch [140/352] Loss: 1.5050 Acc: 59.34%
Epoch [32] Batch [150/352] Loss: 1.5014 Acc: 59.26%
Epoch [32] Batch [160/352] Loss: 1.4512 Acc: 59.22%
Epoch [32] Batch [170/352] Loss: 1.4049 Acc: 59.12%
Epoch [32] Batch [180/352] Loss: 1.5724 Acc: 59.12%
Epoch [32] Batch [190/352] Loss: 1.4882 Acc: 59.06%
Epoch [32] Batch [200/352] Loss: 1.5759 Acc: 59.06%
Epoch [32] Batch [210/352] Loss: 1.4960 Acc: 59.10%
Epoch [32] Batch [220/352] Loss: 1.6364 Acc: 58.96%
Epoch [32] Batch [230/352] Loss: 1.4745 Acc: 58.90%
Epoch [32] Batch [240/352] Loss: 1.5342 Acc: 58.77%
Epoch [32] Batch [250/352] Loss: 1.3961 Acc: 58.72%
Epoch [32] Batch [260/352] Loss: 1.6274 Acc: 58.66%
Epoch [32] Batch [270/352] Loss: 1.4893 Acc: 58.59%
Epoch [32] Batch [280/352] Loss: 1.3375 Acc: 58.54%
Epoch [32] Batch [290/352] Loss: 1.6276 Acc: 58.49%
Epoch [32] Batch [300/352] Loss: 1.6341 Acc: 58.48%
Epoch [32] Batch [310/352] Loss: 1.1880 Acc: 58.46%
Epoch [32] Batch [320/352] Loss: 1.5040 Acc: 58.40%
Epoch [32] Batch [330/352] Loss: 1.5360 Acc: 58.42%
Epoch [32] Batch [340/352] Loss: 1.7273 Acc: 58.32%
Epoch [32] Batch [350/352] Loss: 1.5564 Acc: 58.27%

======================================================================
Epoch 32/100
Train Loss: 1.4772, Train Acc: 58.26%
Val Loss: 1.7875, Val Acc: 51.46%
Time: 93.00s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 51.46%)
Epoch [33] Batch [0/352] Loss: 1.2022 Acc: 64.84%
Epoch [33] Batch [10/352] Loss: 1.3077 Acc: 58.74%
Epoch [33] Batch [20/352] Loss: 1.2764 Acc: 60.42%
Epoch [33] Batch [30/352] Loss: 1.4088 Acc: 60.08%
Epoch [33] Batch [40/352] Loss: 1.4066 Acc: 60.21%
Epoch [33] Batch [50/352] Loss: 1.3008 Acc: 60.03%
Epoch [33] Batch [60/352] Loss: 1.6415 Acc: 59.91%
Epoch [33] Batch [70/352] Loss: 1.4455 Acc: 59.55%
Epoch [33] Batch [80/352] Loss: 1.3663 Acc: 59.61%
Epoch [33] Batch [90/352] Loss: 1.4196 Acc: 59.25%
Epoch [33] Batch [100/352] Loss: 1.4447 Acc: 59.33%
Epoch [33] Batch [110/352] Loss: 1.4175 Acc: 59.11%
Epoch [33] Batch [120/352] Loss: 1.5248 Acc: 58.86%
Epoch [33] Batch [130/352] Loss: 1.5882 Acc: 58.89%
Epoch [33] Batch [140/352] Loss: 1.3744 Acc: 58.84%
Epoch [33] Batch [150/352] Loss: 1.7014 Acc: 58.68%
Epoch [33] Batch [160/352] Loss: 1.4628 Acc: 58.69%
Epoch [33] Batch [170/352] Loss: 1.3432 Acc: 58.65%
Epoch [33] Batch [180/352] Loss: 1.5439 Acc: 58.60%
Epoch [33] Batch [190/352] Loss: 1.3208 Acc: 58.56%
Epoch [33] Batch [200/352] Loss: 1.4757 Acc: 58.53%
Epoch [33] Batch [210/352] Loss: 1.4226 Acc: 58.45%
Epoch [33] Batch [220/352] Loss: 1.2638 Acc: 58.53%
Epoch [33] Batch [230/352] Loss: 1.3692 Acc: 58.52%
Epoch [33] Batch [240/352] Loss: 1.5194 Acc: 58.45%
Epoch [33] Batch [250/352] Loss: 1.6024 Acc: 58.38%
Epoch [33] Batch [260/352] Loss: 1.4529 Acc: 58.35%
Epoch [33] Batch [270/352] Loss: 1.2326 Acc: 58.48%
Epoch [33] Batch [280/352] Loss: 1.4843 Acc: 58.57%
Epoch [33] Batch [290/352] Loss: 1.6855 Acc: 58.54%
Epoch [33] Batch [300/352] Loss: 1.4421 Acc: 58.51%
Epoch [33] Batch [310/352] Loss: 1.5375 Acc: 58.50%
Epoch [33] Batch [320/352] Loss: 1.5731 Acc: 58.46%
Epoch [33] Batch [330/352] Loss: 1.4662 Acc: 58.47%
Epoch [33] Batch [340/352] Loss: 1.5369 Acc: 58.43%
Epoch [33] Batch [350/352] Loss: 1.7314 Acc: 58.39%

======================================================================
Epoch 33/100
Train Loss: 1.4567, Train Acc: 58.39%
Val Loss: 2.0802, Val Acc: 47.14%
Time: 94.07s
======================================================================

Epoch [34] Batch [0/352] Loss: 1.1459 Acc: 62.50%
Epoch [34] Batch [10/352] Loss: 1.4241 Acc: 59.30%
Epoch [34] Batch [20/352] Loss: 1.3091 Acc: 60.79%
Epoch [34] Batch [30/352] Loss: 1.4245 Acc: 60.61%
Epoch [34] Batch [40/352] Loss: 1.1670 Acc: 60.46%
Epoch [34] Batch [50/352] Loss: 1.6048 Acc: 59.96%
Epoch [34] Batch [60/352] Loss: 1.3734 Acc: 59.61%
Epoch [34] Batch [70/352] Loss: 1.4748 Acc: 59.47%
Epoch [34] Batch [80/352] Loss: 1.0646 Acc: 59.62%
Epoch [34] Batch [90/352] Loss: 1.4942 Acc: 59.76%
Epoch [34] Batch [100/352] Loss: 1.3637 Acc: 59.79%
Epoch [34] Batch [110/352] Loss: 1.4243 Acc: 59.79%
Epoch [34] Batch [120/352] Loss: 1.4920 Acc: 59.68%
Epoch [34] Batch [130/352] Loss: 1.5604 Acc: 59.70%
Epoch [34] Batch [140/352] Loss: 1.2485 Acc: 59.61%
Epoch [34] Batch [150/352] Loss: 1.3196 Acc: 59.60%
Epoch [34] Batch [160/352] Loss: 1.4031 Acc: 59.55%
Epoch [34] Batch [170/352] Loss: 1.3917 Acc: 59.58%
Epoch [34] Batch [180/352] Loss: 1.3283 Acc: 59.46%
Epoch [34] Batch [190/352] Loss: 1.6668 Acc: 59.30%
Epoch [34] Batch [200/352] Loss: 1.3889 Acc: 59.38%
Epoch [34] Batch [210/352] Loss: 1.6191 Acc: 59.29%
Epoch [34] Batch [220/352] Loss: 1.5448 Acc: 59.23%
Epoch [34] Batch [230/352] Loss: 1.7376 Acc: 59.10%
Epoch [34] Batch [240/352] Loss: 1.5351 Acc: 59.00%
Epoch [34] Batch [250/352] Loss: 1.4190 Acc: 59.00%
Epoch [34] Batch [260/352] Loss: 1.4528 Acc: 58.94%
Epoch [34] Batch [270/352] Loss: 1.4528 Acc: 58.91%
Epoch [34] Batch [280/352] Loss: 1.5628 Acc: 58.79%
Epoch [34] Batch [290/352] Loss: 1.4871 Acc: 58.80%
Epoch [34] Batch [300/352] Loss: 1.4223 Acc: 58.76%
Epoch [34] Batch [310/352] Loss: 1.4693 Acc: 58.74%
Epoch [34] Batch [320/352] Loss: 1.2891 Acc: 58.77%
Epoch [34] Batch [330/352] Loss: 1.4838 Acc: 58.77%
Epoch [34] Batch [340/352] Loss: 1.5178 Acc: 58.79%
Epoch [34] Batch [350/352] Loss: 1.2086 Acc: 58.83%

======================================================================
Epoch 34/100
Train Loss: 1.4477, Train Acc: 58.83%
Val Loss: 1.9352, Val Acc: 50.28%
Time: 94.12s
======================================================================

Epoch [35] Batch [0/352] Loss: 1.5015 Acc: 57.03%
Epoch [35] Batch [10/352] Loss: 1.4321 Acc: 59.09%
Epoch [35] Batch [20/352] Loss: 1.4361 Acc: 59.64%
Epoch [35] Batch [30/352] Loss: 1.5256 Acc: 59.60%
Epoch [35] Batch [40/352] Loss: 1.2319 Acc: 60.25%
Epoch [35] Batch [50/352] Loss: 1.4278 Acc: 60.54%
Epoch [35] Batch [60/352] Loss: 1.2921 Acc: 60.76%
Epoch [35] Batch [70/352] Loss: 1.4378 Acc: 60.48%
Epoch [35] Batch [80/352] Loss: 1.3508 Acc: 60.27%
Epoch [35] Batch [90/352] Loss: 1.4909 Acc: 60.38%
Epoch [35] Batch [100/352] Loss: 1.3608 Acc: 60.04%
Epoch [35] Batch [110/352] Loss: 1.2860 Acc: 59.95%
Epoch [35] Batch [120/352] Loss: 1.3435 Acc: 59.84%
Epoch [35] Batch [130/352] Loss: 1.4778 Acc: 59.55%
Epoch [35] Batch [140/352] Loss: 1.5650 Acc: 59.33%
Epoch [35] Batch [150/352] Loss: 1.5630 Acc: 59.19%
Epoch [35] Batch [160/352] Loss: 1.5783 Acc: 59.10%
Epoch [35] Batch [170/352] Loss: 1.6227 Acc: 58.93%
Epoch [35] Batch [180/352] Loss: 1.2769 Acc: 58.88%
Epoch [35] Batch [190/352] Loss: 1.4532 Acc: 58.85%
Epoch [35] Batch [200/352] Loss: 1.4171 Acc: 58.96%
Epoch [35] Batch [210/352] Loss: 1.2753 Acc: 58.97%
Epoch [35] Batch [220/352] Loss: 1.4989 Acc: 58.95%
Epoch [35] Batch [230/352] Loss: 1.3403 Acc: 58.96%
Epoch [35] Batch [240/352] Loss: 1.6787 Acc: 58.89%
Epoch [35] Batch [250/352] Loss: 1.8574 Acc: 58.88%
Epoch [35] Batch [260/352] Loss: 1.2614 Acc: 58.87%
Epoch [35] Batch [270/352] Loss: 1.5716 Acc: 58.88%
Epoch [35] Batch [280/352] Loss: 1.2621 Acc: 58.92%
Epoch [35] Batch [290/352] Loss: 1.4369 Acc: 58.86%
Epoch [35] Batch [300/352] Loss: 1.6050 Acc: 58.82%
Epoch [35] Batch [310/352] Loss: 1.3862 Acc: 58.75%
Epoch [35] Batch [320/352] Loss: 1.4463 Acc: 58.76%
Epoch [35] Batch [330/352] Loss: 1.5478 Acc: 58.75%
Epoch [35] Batch [340/352] Loss: 1.7227 Acc: 58.76%
Epoch [35] Batch [350/352] Loss: 1.5098 Acc: 58.74%

======================================================================
Epoch 35/100
Train Loss: 1.4467, Train Acc: 58.74%
Val Loss: 1.8328, Val Acc: 52.04%
Time: 94.15s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 52.04%)
Epoch [36] Batch [0/352] Loss: 1.3552 Acc: 61.72%
Epoch [36] Batch [10/352] Loss: 1.2970 Acc: 60.80%
Epoch [36] Batch [20/352] Loss: 1.2332 Acc: 61.42%
Epoch [36] Batch [30/352] Loss: 1.3037 Acc: 61.57%
Epoch [36] Batch [40/352] Loss: 1.3575 Acc: 61.55%
Epoch [36] Batch [50/352] Loss: 1.4515 Acc: 61.32%
Epoch [36] Batch [60/352] Loss: 1.4397 Acc: 60.60%
Epoch [36] Batch [70/352] Loss: 1.3084 Acc: 60.81%
Epoch [36] Batch [80/352] Loss: 1.3561 Acc: 60.78%
Epoch [36] Batch [90/352] Loss: 1.5562 Acc: 60.50%
Epoch [36] Batch [100/352] Loss: 1.4947 Acc: 60.19%
Epoch [36] Batch [110/352] Loss: 1.5732 Acc: 60.18%
Epoch [36] Batch [120/352] Loss: 1.4171 Acc: 60.23%
Epoch [36] Batch [130/352] Loss: 1.4236 Acc: 60.25%
Epoch [36] Batch [140/352] Loss: 1.3257 Acc: 60.15%
Epoch [36] Batch [150/352] Loss: 1.5103 Acc: 60.15%
Epoch [36] Batch [160/352] Loss: 1.3413 Acc: 60.01%
Epoch [36] Batch [170/352] Loss: 1.3576 Acc: 59.98%
Epoch [36] Batch [180/352] Loss: 1.3689 Acc: 60.01%
Epoch [36] Batch [190/352] Loss: 1.4777 Acc: 59.95%
Epoch [36] Batch [200/352] Loss: 1.1870 Acc: 59.79%
Epoch [36] Batch [210/352] Loss: 1.4588 Acc: 59.77%
Epoch [36] Batch [220/352] Loss: 1.3115 Acc: 59.76%
Epoch [36] Batch [230/352] Loss: 1.3872 Acc: 59.62%
Epoch [36] Batch [240/352] Loss: 1.5943 Acc: 59.59%
Epoch [36] Batch [250/352] Loss: 1.1745 Acc: 59.56%
Epoch [36] Batch [260/352] Loss: 1.3917 Acc: 59.41%
Epoch [36] Batch [270/352] Loss: 1.2785 Acc: 59.41%
Epoch [36] Batch [280/352] Loss: 1.4145 Acc: 59.43%
Epoch [36] Batch [290/352] Loss: 1.2784 Acc: 59.42%
Epoch [36] Batch [300/352] Loss: 1.3004 Acc: 59.40%
Epoch [36] Batch [310/352] Loss: 1.5927 Acc: 59.36%
Epoch [36] Batch [320/352] Loss: 1.3370 Acc: 59.39%
Epoch [36] Batch [330/352] Loss: 1.2081 Acc: 59.43%
Epoch [36] Batch [340/352] Loss: 1.3422 Acc: 59.47%
Epoch [36] Batch [350/352] Loss: 1.6184 Acc: 59.44%

======================================================================
Epoch 36/100
Train Loss: 1.4274, Train Acc: 59.45%
Val Loss: 1.7148, Val Acc: 54.22%
Time: 94.33s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 54.22%)
Epoch [37] Batch [0/352] Loss: 1.2527 Acc: 60.94%
Epoch [37] Batch [10/352] Loss: 1.2952 Acc: 62.36%
Epoch [37] Batch [20/352] Loss: 1.3207 Acc: 61.46%
Epoch [37] Batch [30/352] Loss: 1.3104 Acc: 61.79%
Epoch [37] Batch [40/352] Loss: 1.4557 Acc: 61.20%
Epoch [37] Batch [50/352] Loss: 1.2452 Acc: 61.58%
Epoch [37] Batch [60/352] Loss: 1.4906 Acc: 61.41%
Epoch [37] Batch [70/352] Loss: 1.5908 Acc: 61.22%
Epoch [37] Batch [80/352] Loss: 1.2998 Acc: 61.18%
Epoch [37] Batch [90/352] Loss: 1.3616 Acc: 61.15%
Epoch [37] Batch [100/352] Loss: 1.2449 Acc: 61.05%
Epoch [37] Batch [110/352] Loss: 1.3164 Acc: 60.81%
Epoch [37] Batch [120/352] Loss: 1.4323 Acc: 60.75%
Epoch [37] Batch [130/352] Loss: 1.5782 Acc: 60.63%
Epoch [37] Batch [140/352] Loss: 1.4038 Acc: 60.68%
Epoch [37] Batch [150/352] Loss: 1.4566 Acc: 60.70%
Epoch [37] Batch [160/352] Loss: 1.5451 Acc: 60.57%
Epoch [37] Batch [170/352] Loss: 1.6366 Acc: 60.45%
Epoch [37] Batch [180/352] Loss: 1.5125 Acc: 60.25%
Epoch [37] Batch [190/352] Loss: 1.3689 Acc: 60.17%
Epoch [37] Batch [200/352] Loss: 1.4563 Acc: 60.12%
Epoch [37] Batch [210/352] Loss: 1.1489 Acc: 60.17%
Epoch [37] Batch [220/352] Loss: 1.4520 Acc: 60.20%
Epoch [37] Batch [230/352] Loss: 1.5494 Acc: 60.13%
Epoch [37] Batch [240/352] Loss: 1.6672 Acc: 60.06%
Epoch [37] Batch [250/352] Loss: 1.5061 Acc: 60.04%
Epoch [37] Batch [260/352] Loss: 1.3873 Acc: 60.03%
Epoch [37] Batch [270/352] Loss: 1.1896 Acc: 60.05%
Epoch [37] Batch [280/352] Loss: 1.3098 Acc: 59.98%
Epoch [37] Batch [290/352] Loss: 1.4505 Acc: 59.95%
Epoch [37] Batch [300/352] Loss: 1.4355 Acc: 59.97%
Epoch [37] Batch [310/352] Loss: 1.2951 Acc: 59.94%
Epoch [37] Batch [320/352] Loss: 1.4370 Acc: 59.93%
Epoch [37] Batch [330/352] Loss: 1.4421 Acc: 59.87%
Epoch [37] Batch [340/352] Loss: 1.4311 Acc: 59.90%
Epoch [37] Batch [350/352] Loss: 1.3749 Acc: 59.80%

======================================================================
Epoch 37/100
Train Loss: 1.4126, Train Acc: 59.79%
Val Loss: 1.8723, Val Acc: 50.84%
Time: 93.93s
======================================================================

Epoch [38] Batch [0/352] Loss: 1.1804 Acc: 65.62%
Epoch [38] Batch [10/352] Loss: 1.3899 Acc: 61.01%
Epoch [38] Batch [20/352] Loss: 1.1380 Acc: 61.38%
Epoch [38] Batch [30/352] Loss: 1.3845 Acc: 61.19%
Epoch [38] Batch [40/352] Loss: 1.6258 Acc: 61.36%
Epoch [38] Batch [50/352] Loss: 1.4697 Acc: 61.52%
Epoch [38] Batch [60/352] Loss: 1.0635 Acc: 61.91%
Epoch [38] Batch [70/352] Loss: 1.3186 Acc: 61.81%
Epoch [38] Batch [80/352] Loss: 1.2516 Acc: 61.81%
Epoch [38] Batch [90/352] Loss: 1.5657 Acc: 61.35%
Epoch [38] Batch [100/352] Loss: 1.3757 Acc: 61.32%
Epoch [38] Batch [110/352] Loss: 1.3422 Acc: 60.97%
Epoch [38] Batch [120/352] Loss: 1.6125 Acc: 60.81%
Epoch [38] Batch [130/352] Loss: 1.5622 Acc: 60.84%
Epoch [38] Batch [140/352] Loss: 1.2563 Acc: 60.79%
Epoch [38] Batch [150/352] Loss: 1.6482 Acc: 60.72%
Epoch [38] Batch [160/352] Loss: 1.4440 Acc: 60.65%
Epoch [38] Batch [170/352] Loss: 1.3925 Acc: 60.70%
Epoch [38] Batch [180/352] Loss: 1.4971 Acc: 60.46%
Epoch [38] Batch [190/352] Loss: 1.4006 Acc: 60.42%
Epoch [38] Batch [200/352] Loss: 1.5624 Acc: 60.39%
Epoch [38] Batch [210/352] Loss: 1.5030 Acc: 60.36%
Epoch [38] Batch [220/352] Loss: 1.3418 Acc: 60.20%
Epoch [38] Batch [230/352] Loss: 1.3835 Acc: 60.11%
Epoch [38] Batch [240/352] Loss: 1.3499 Acc: 60.01%
Epoch [38] Batch [250/352] Loss: 1.3835 Acc: 59.96%
Epoch [38] Batch [260/352] Loss: 1.3934 Acc: 59.90%
Epoch [38] Batch [270/352] Loss: 1.4496 Acc: 59.86%
Epoch [38] Batch [280/352] Loss: 1.6471 Acc: 59.73%
Epoch [38] Batch [290/352] Loss: 1.4402 Acc: 59.70%
Epoch [38] Batch [300/352] Loss: 1.5799 Acc: 59.70%
Epoch [38] Batch [310/352] Loss: 1.5421 Acc: 59.70%
Epoch [38] Batch [320/352] Loss: 1.3827 Acc: 59.74%
Epoch [38] Batch [330/352] Loss: 1.3451 Acc: 59.78%
Epoch [38] Batch [340/352] Loss: 1.5866 Acc: 59.75%
Epoch [38] Batch [350/352] Loss: 1.4895 Acc: 59.75%

======================================================================
Epoch 38/100
Train Loss: 1.4062, Train Acc: 59.74%
Val Loss: 1.6651, Val Acc: 54.64%
Time: 93.04s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 54.64%)
Epoch [39] Batch [0/352] Loss: 1.5646 Acc: 60.16%
Epoch [39] Batch [10/352] Loss: 1.1332 Acc: 61.15%
Epoch [39] Batch [20/352] Loss: 1.2597 Acc: 61.87%
Epoch [39] Batch [30/352] Loss: 1.1685 Acc: 61.52%
Epoch [39] Batch [40/352] Loss: 1.2617 Acc: 61.53%
Epoch [39] Batch [50/352] Loss: 1.3645 Acc: 61.27%
Epoch [39] Batch [60/352] Loss: 1.2502 Acc: 61.49%
Epoch [39] Batch [70/352] Loss: 1.4522 Acc: 61.38%
Epoch [39] Batch [80/352] Loss: 1.1953 Acc: 61.65%
Epoch [39] Batch [90/352] Loss: 1.4641 Acc: 61.68%
Epoch [39] Batch [100/352] Loss: 1.2333 Acc: 61.44%
Epoch [39] Batch [110/352] Loss: 1.6064 Acc: 61.29%
Epoch [39] Batch [120/352] Loss: 1.4821 Acc: 61.06%
Epoch [39] Batch [130/352] Loss: 1.0921 Acc: 61.10%
Epoch [39] Batch [140/352] Loss: 1.1792 Acc: 60.99%
Epoch [39] Batch [150/352] Loss: 1.6230 Acc: 60.94%
Epoch [39] Batch [160/352] Loss: 1.4575 Acc: 60.84%
Epoch [39] Batch [170/352] Loss: 1.3172 Acc: 60.84%
Epoch [39] Batch [180/352] Loss: 1.2606 Acc: 60.79%
Epoch [39] Batch [190/352] Loss: 1.3054 Acc: 60.72%
Epoch [39] Batch [200/352] Loss: 1.5019 Acc: 60.68%
Epoch [39] Batch [210/352] Loss: 1.6996 Acc: 60.63%
Epoch [39] Batch [220/352] Loss: 1.6319 Acc: 60.57%
Epoch [39] Batch [230/352] Loss: 1.6108 Acc: 60.55%
Epoch [39] Batch [240/352] Loss: 1.3844 Acc: 60.46%
Epoch [39] Batch [250/352] Loss: 1.3197 Acc: 60.51%
Epoch [39] Batch [260/352] Loss: 1.5678 Acc: 60.61%
Epoch [39] Batch [270/352] Loss: 1.4562 Acc: 60.47%
Epoch [39] Batch [280/352] Loss: 1.5853 Acc: 60.36%
Epoch [39] Batch [290/352] Loss: 1.4575 Acc: 60.39%
Epoch [39] Batch [300/352] Loss: 1.4078 Acc: 60.38%
Epoch [39] Batch [310/352] Loss: 1.3580 Acc: 60.40%
Epoch [39] Batch [320/352] Loss: 1.3939 Acc: 60.37%
Epoch [39] Batch [330/352] Loss: 1.4025 Acc: 60.39%
Epoch [39] Batch [340/352] Loss: 1.5183 Acc: 60.37%
Epoch [39] Batch [350/352] Loss: 1.4538 Acc: 60.31%

======================================================================
Epoch 39/100
Train Loss: 1.3929, Train Acc: 60.33%
Val Loss: 2.2079, Val Acc: 45.10%
Time: 92.37s
======================================================================

Epoch [40] Batch [0/352] Loss: 1.4260 Acc: 57.03%
Epoch [40] Batch [10/352] Loss: 1.3819 Acc: 61.58%
Epoch [40] Batch [20/352] Loss: 1.3948 Acc: 61.20%
Epoch [40] Batch [30/352] Loss: 1.3174 Acc: 61.21%
Epoch [40] Batch [40/352] Loss: 1.3727 Acc: 61.26%
Epoch [40] Batch [50/352] Loss: 1.3710 Acc: 61.04%
Epoch [40] Batch [60/352] Loss: 1.0475 Acc: 60.86%
Epoch [40] Batch [70/352] Loss: 1.3623 Acc: 60.78%
Epoch [40] Batch [80/352] Loss: 1.3076 Acc: 60.57%
Epoch [40] Batch [90/352] Loss: 1.4323 Acc: 60.47%
Epoch [40] Batch [100/352] Loss: 1.3803 Acc: 60.40%
Epoch [40] Batch [110/352] Loss: 1.3028 Acc: 60.19%
Epoch [40] Batch [120/352] Loss: 1.3419 Acc: 60.24%
Epoch [40] Batch [130/352] Loss: 1.2458 Acc: 60.11%
Epoch [40] Batch [140/352] Loss: 1.2920 Acc: 60.08%
Epoch [40] Batch [150/352] Loss: 1.4032 Acc: 60.09%
Epoch [40] Batch [160/352] Loss: 1.4429 Acc: 60.13%
Epoch [40] Batch [170/352] Loss: 1.5234 Acc: 60.23%
Epoch [40] Batch [180/352] Loss: 1.3139 Acc: 60.28%
Epoch [40] Batch [190/352] Loss: 0.9879 Acc: 60.41%
Epoch [40] Batch [200/352] Loss: 1.4918 Acc: 60.35%
Epoch [40] Batch [210/352] Loss: 1.3886 Acc: 60.39%
Epoch [40] Batch [220/352] Loss: 1.2820 Acc: 60.27%
Epoch [40] Batch [230/352] Loss: 1.3649 Acc: 60.25%
Epoch [40] Batch [240/352] Loss: 1.4899 Acc: 60.25%
Epoch [40] Batch [250/352] Loss: 1.2483 Acc: 60.27%
Epoch [40] Batch [260/352] Loss: 1.5987 Acc: 60.28%
Epoch [40] Batch [270/352] Loss: 1.5562 Acc: 60.27%
Epoch [40] Batch [280/352] Loss: 1.3471 Acc: 60.23%
Epoch [40] Batch [290/352] Loss: 1.4302 Acc: 60.21%
Epoch [40] Batch [300/352] Loss: 1.4013 Acc: 60.25%
Epoch [40] Batch [310/352] Loss: 1.4347 Acc: 60.24%
Epoch [40] Batch [320/352] Loss: 1.4304 Acc: 60.19%
Epoch [40] Batch [330/352] Loss: 1.4501 Acc: 60.18%
Epoch [40] Batch [340/352] Loss: 1.4440 Acc: 60.17%
Epoch [40] Batch [350/352] Loss: 1.2961 Acc: 60.14%

======================================================================
Epoch 40/100
Train Loss: 1.3856, Train Acc: 60.15%
Val Loss: 1.6623, Val Acc: 55.06%
Time: 92.42s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 55.06%)
Epoch [41] Batch [0/352] Loss: 1.4457 Acc: 55.47%
Epoch [41] Batch [10/352] Loss: 1.2179 Acc: 61.22%
Epoch [41] Batch [20/352] Loss: 1.3702 Acc: 62.61%
Epoch [41] Batch [30/352] Loss: 1.1630 Acc: 62.02%
Epoch [41] Batch [40/352] Loss: 1.3430 Acc: 62.08%
Epoch [41] Batch [50/352] Loss: 1.1713 Acc: 62.07%
Epoch [41] Batch [60/352] Loss: 1.3192 Acc: 61.72%
Epoch [41] Batch [70/352] Loss: 1.2373 Acc: 61.41%
Epoch [41] Batch [80/352] Loss: 1.1605 Acc: 61.42%
Epoch [41] Batch [90/352] Loss: 1.3761 Acc: 61.37%
Epoch [41] Batch [100/352] Loss: 1.4337 Acc: 61.57%
Epoch [41] Batch [110/352] Loss: 1.1378 Acc: 61.68%
Epoch [41] Batch [120/352] Loss: 1.4335 Acc: 61.62%
Epoch [41] Batch [130/352] Loss: 1.2660 Acc: 61.55%
Epoch [41] Batch [140/352] Loss: 1.4290 Acc: 61.75%
Epoch [41] Batch [150/352] Loss: 1.2395 Acc: 61.67%
Epoch [41] Batch [160/352] Loss: 1.5102 Acc: 61.58%
Epoch [41] Batch [170/352] Loss: 1.2032 Acc: 61.41%
Epoch [41] Batch [180/352] Loss: 1.5342 Acc: 61.32%
Epoch [41] Batch [190/352] Loss: 1.3881 Acc: 61.14%
Epoch [41] Batch [200/352] Loss: 1.4096 Acc: 61.00%
Epoch [41] Batch [210/352] Loss: 1.4039 Acc: 61.02%
Epoch [41] Batch [220/352] Loss: 1.2956 Acc: 61.02%
Epoch [41] Batch [230/352] Loss: 1.5004 Acc: 60.99%
Epoch [41] Batch [240/352] Loss: 1.5360 Acc: 60.89%
Epoch [41] Batch [250/352] Loss: 1.2841 Acc: 60.88%
Epoch [41] Batch [260/352] Loss: 1.4502 Acc: 60.83%
Epoch [41] Batch [270/352] Loss: 1.4368 Acc: 60.77%
Epoch [41] Batch [280/352] Loss: 1.2708 Acc: 60.79%
Epoch [41] Batch [290/352] Loss: 1.4690 Acc: 60.73%
Epoch [41] Batch [300/352] Loss: 1.2715 Acc: 60.72%
Epoch [41] Batch [310/352] Loss: 1.6939 Acc: 60.73%
Epoch [41] Batch [320/352] Loss: 1.4631 Acc: 60.73%
Epoch [41] Batch [330/352] Loss: 1.2436 Acc: 60.69%
Epoch [41] Batch [340/352] Loss: 1.6032 Acc: 60.62%
Epoch [41] Batch [350/352] Loss: 1.3637 Acc: 60.61%

======================================================================
Epoch 41/100
Train Loss: 1.3711, Train Acc: 60.61%
Val Loss: 1.8356, Val Acc: 51.84%
Time: 92.65s
======================================================================

Epoch [42] Batch [0/352] Loss: 1.2945 Acc: 55.47%
Epoch [42] Batch [10/352] Loss: 1.1747 Acc: 60.94%
Epoch [42] Batch [20/352] Loss: 1.4735 Acc: 61.42%
Epoch [42] Batch [30/352] Loss: 1.2446 Acc: 61.62%
Epoch [42] Batch [40/352] Loss: 1.2362 Acc: 61.55%
Epoch [42] Batch [50/352] Loss: 1.1926 Acc: 62.04%
Epoch [42] Batch [60/352] Loss: 1.3411 Acc: 62.05%
Epoch [42] Batch [70/352] Loss: 1.5565 Acc: 61.92%
Epoch [42] Batch [80/352] Loss: 1.1954 Acc: 61.89%
Epoch [42] Batch [90/352] Loss: 1.3083 Acc: 61.94%
Epoch [42] Batch [100/352] Loss: 1.1542 Acc: 62.00%
Epoch [42] Batch [110/352] Loss: 1.1360 Acc: 62.00%
Epoch [42] Batch [120/352] Loss: 1.3589 Acc: 61.78%
Epoch [42] Batch [130/352] Loss: 1.0196 Acc: 61.68%
Epoch [42] Batch [140/352] Loss: 1.4684 Acc: 61.54%
Epoch [42] Batch [150/352] Loss: 1.2883 Acc: 61.38%
Epoch [42] Batch [160/352] Loss: 1.3504 Acc: 61.40%
Epoch [42] Batch [170/352] Loss: 1.4499 Acc: 61.27%
Epoch [42] Batch [180/352] Loss: 1.2655 Acc: 61.24%
Epoch [42] Batch [190/352] Loss: 1.4545 Acc: 61.20%
Epoch [42] Batch [200/352] Loss: 1.2673 Acc: 61.24%
Epoch [42] Batch [210/352] Loss: 1.2038 Acc: 61.28%
Epoch [42] Batch [220/352] Loss: 1.1449 Acc: 61.32%
Epoch [42] Batch [230/352] Loss: 1.5022 Acc: 61.16%
Epoch [42] Batch [240/352] Loss: 1.2427 Acc: 61.15%
Epoch [42] Batch [250/352] Loss: 1.5760 Acc: 61.03%
Epoch [42] Batch [260/352] Loss: 1.5520 Acc: 61.01%
Epoch [42] Batch [270/352] Loss: 1.4490 Acc: 61.02%
Epoch [42] Batch [280/352] Loss: 1.3618 Acc: 61.02%
Epoch [42] Batch [290/352] Loss: 1.3657 Acc: 61.04%
Epoch [42] Batch [300/352] Loss: 1.4378 Acc: 60.99%
Epoch [42] Batch [310/352] Loss: 1.3910 Acc: 60.93%
Epoch [42] Batch [320/352] Loss: 1.3449 Acc: 60.92%
Epoch [42] Batch [330/352] Loss: 1.5260 Acc: 60.88%
Epoch [42] Batch [340/352] Loss: 1.5098 Acc: 60.85%
Epoch [42] Batch [350/352] Loss: 1.3994 Acc: 60.84%

======================================================================
Epoch 42/100
Train Loss: 1.3588, Train Acc: 60.83%
Val Loss: 1.8053, Val Acc: 52.28%
Time: 92.24s
======================================================================

Epoch [43] Batch [0/352] Loss: 1.3615 Acc: 62.50%
Epoch [43] Batch [10/352] Loss: 1.0697 Acc: 62.78%
Epoch [43] Batch [20/352] Loss: 1.3439 Acc: 63.43%
Epoch [43] Batch [30/352] Loss: 1.3017 Acc: 62.83%
Epoch [43] Batch [40/352] Loss: 1.2547 Acc: 62.69%
Epoch [43] Batch [50/352] Loss: 1.2045 Acc: 62.85%
Epoch [43] Batch [60/352] Loss: 1.4631 Acc: 62.81%
Epoch [43] Batch [70/352] Loss: 1.1949 Acc: 62.89%
Epoch [43] Batch [80/352] Loss: 1.2852 Acc: 62.85%
Epoch [43] Batch [90/352] Loss: 1.1598 Acc: 62.68%
Epoch [43] Batch [100/352] Loss: 1.4063 Acc: 62.29%
Epoch [43] Batch [110/352] Loss: 1.4317 Acc: 62.08%
Epoch [43] Batch [120/352] Loss: 1.6232 Acc: 61.92%
Epoch [43] Batch [130/352] Loss: 1.4731 Acc: 61.87%
Epoch [43] Batch [140/352] Loss: 1.2009 Acc: 61.77%
Epoch [43] Batch [150/352] Loss: 1.4093 Acc: 61.77%
Epoch [43] Batch [160/352] Loss: 1.2449 Acc: 61.90%
Epoch [43] Batch [170/352] Loss: 1.4434 Acc: 61.86%
Epoch [43] Batch [180/352] Loss: 1.2722 Acc: 61.87%
Epoch [43] Batch [190/352] Loss: 1.3447 Acc: 61.80%
Epoch [43] Batch [200/352] Loss: 1.3849 Acc: 61.76%
Epoch [43] Batch [210/352] Loss: 1.4524 Acc: 61.67%
Epoch [43] Batch [220/352] Loss: 1.4213 Acc: 61.67%
Epoch [43] Batch [230/352] Loss: 1.4841 Acc: 61.61%
Epoch [43] Batch [240/352] Loss: 1.6742 Acc: 61.57%
Epoch [43] Batch [250/352] Loss: 1.5139 Acc: 61.48%
Epoch [43] Batch [260/352] Loss: 1.5613 Acc: 61.43%
Epoch [43] Batch [270/352] Loss: 1.4635 Acc: 61.39%
Epoch [43] Batch [280/352] Loss: 1.3197 Acc: 61.32%
Epoch [43] Batch [290/352] Loss: 1.6797 Acc: 61.29%
Epoch [43] Batch [300/352] Loss: 1.4098 Acc: 61.26%
Epoch [43] Batch [310/352] Loss: 1.1991 Acc: 61.25%
Epoch [43] Batch [320/352] Loss: 1.3265 Acc: 61.21%
Epoch [43] Batch [330/352] Loss: 1.4246 Acc: 61.18%
Epoch [43] Batch [340/352] Loss: 1.0909 Acc: 61.16%
Epoch [43] Batch [350/352] Loss: 1.2566 Acc: 61.18%

======================================================================
Epoch 43/100
Train Loss: 1.3548, Train Acc: 61.17%
Val Loss: 1.8150, Val Acc: 52.68%
Time: 92.91s
======================================================================

Epoch [44] Batch [0/352] Loss: 1.2865 Acc: 60.94%
Epoch [44] Batch [10/352] Loss: 1.2673 Acc: 61.58%
Epoch [44] Batch [20/352] Loss: 1.2572 Acc: 61.24%
Epoch [44] Batch [30/352] Loss: 1.1734 Acc: 62.78%
Epoch [44] Batch [40/352] Loss: 1.4134 Acc: 63.05%
Epoch [44] Batch [50/352] Loss: 1.2434 Acc: 63.54%
Epoch [44] Batch [60/352] Loss: 1.2941 Acc: 62.92%
Epoch [44] Batch [70/352] Loss: 1.2062 Acc: 63.06%
Epoch [44] Batch [80/352] Loss: 1.2830 Acc: 62.98%
Epoch [44] Batch [90/352] Loss: 1.3329 Acc: 62.96%
Epoch [44] Batch [100/352] Loss: 1.3138 Acc: 62.95%
Epoch [44] Batch [110/352] Loss: 1.1402 Acc: 62.92%
Epoch [44] Batch [120/352] Loss: 1.4684 Acc: 62.78%
Epoch [44] Batch [130/352] Loss: 1.1593 Acc: 62.74%
Epoch [44] Batch [140/352] Loss: 1.1384 Acc: 62.61%
Epoch [44] Batch [150/352] Loss: 1.1695 Acc: 62.56%
Epoch [44] Batch [160/352] Loss: 1.3049 Acc: 62.43%
Epoch [44] Batch [170/352] Loss: 1.5764 Acc: 62.19%
Epoch [44] Batch [180/352] Loss: 1.3722 Acc: 62.16%
Epoch [44] Batch [190/352] Loss: 1.3027 Acc: 62.10%
Epoch [44] Batch [200/352] Loss: 1.4080 Acc: 62.08%
Epoch [44] Batch [210/352] Loss: 1.0466 Acc: 62.10%
Epoch [44] Batch [220/352] Loss: 1.0654 Acc: 62.05%
Epoch [44] Batch [230/352] Loss: 1.2243 Acc: 62.13%
Epoch [44] Batch [240/352] Loss: 1.6247 Acc: 62.12%
Epoch [44] Batch [250/352] Loss: 1.3230 Acc: 62.04%
Epoch [44] Batch [260/352] Loss: 1.3929 Acc: 62.07%
Epoch [44] Batch [270/352] Loss: 1.2823 Acc: 61.98%
Epoch [44] Batch [280/352] Loss: 1.3384 Acc: 61.97%
Epoch [44] Batch [290/352] Loss: 1.6346 Acc: 61.93%
Epoch [44] Batch [300/352] Loss: 1.4207 Acc: 61.85%
Epoch [44] Batch [310/352] Loss: 1.1260 Acc: 61.84%
Epoch [44] Batch [320/352] Loss: 1.4867 Acc: 61.76%
Epoch [44] Batch [330/352] Loss: 1.4740 Acc: 61.73%
Epoch [44] Batch [340/352] Loss: 1.3888 Acc: 61.76%
Epoch [44] Batch [350/352] Loss: 1.4362 Acc: 61.69%

======================================================================
Epoch 44/100
Train Loss: 1.3354, Train Acc: 61.69%
Val Loss: 1.7872, Val Acc: 52.64%
Time: 92.29s
======================================================================

Epoch [45] Batch [0/352] Loss: 1.5132 Acc: 53.12%
Epoch [45] Batch [10/352] Loss: 1.3809 Acc: 60.65%
Epoch [45] Batch [20/352] Loss: 1.2168 Acc: 62.13%
Epoch [45] Batch [30/352] Loss: 1.4804 Acc: 62.85%
Epoch [45] Batch [40/352] Loss: 1.2034 Acc: 62.96%
Epoch [45] Batch [50/352] Loss: 1.2575 Acc: 63.50%
Epoch [45] Batch [60/352] Loss: 1.3559 Acc: 63.50%
Epoch [45] Batch [70/352] Loss: 1.1681 Acc: 63.64%
Epoch [45] Batch [80/352] Loss: 1.2896 Acc: 63.55%
Epoch [45] Batch [90/352] Loss: 1.5398 Acc: 63.55%
Epoch [45] Batch [100/352] Loss: 1.1367 Acc: 63.23%
Epoch [45] Batch [110/352] Loss: 0.9820 Acc: 63.22%
Epoch [45] Batch [120/352] Loss: 1.3178 Acc: 63.20%
Epoch [45] Batch [130/352] Loss: 1.2240 Acc: 62.94%
Epoch [45] Batch [140/352] Loss: 1.4755 Acc: 62.88%
Epoch [45] Batch [150/352] Loss: 1.4360 Acc: 62.58%
Epoch [45] Batch [160/352] Loss: 1.6394 Acc: 62.43%
Epoch [45] Batch [170/352] Loss: 1.1131 Acc: 62.46%
Epoch [45] Batch [180/352] Loss: 1.3003 Acc: 62.35%
Epoch [45] Batch [190/352] Loss: 1.3825 Acc: 62.35%
Epoch [45] Batch [200/352] Loss: 1.1345 Acc: 62.34%
Epoch [45] Batch [210/352] Loss: 1.6075 Acc: 62.30%
Epoch [45] Batch [220/352] Loss: 1.3602 Acc: 62.20%
Epoch [45] Batch [230/352] Loss: 1.1009 Acc: 62.23%
Epoch [45] Batch [240/352] Loss: 1.4960 Acc: 62.20%
Epoch [45] Batch [250/352] Loss: 1.5209 Acc: 62.21%
Epoch [45] Batch [260/352] Loss: 1.6685 Acc: 62.11%
Epoch [45] Batch [270/352] Loss: 1.3024 Acc: 62.10%
Epoch [45] Batch [280/352] Loss: 1.2195 Acc: 62.02%
Epoch [45] Batch [290/352] Loss: 1.3531 Acc: 62.04%
Epoch [45] Batch [300/352] Loss: 1.3865 Acc: 62.04%
Epoch [45] Batch [310/352] Loss: 1.3799 Acc: 61.96%
Epoch [45] Batch [320/352] Loss: 1.2783 Acc: 61.97%
Epoch [45] Batch [330/352] Loss: 1.3526 Acc: 61.98%
Epoch [45] Batch [340/352] Loss: 1.1728 Acc: 62.01%
Epoch [45] Batch [350/352] Loss: 1.2640 Acc: 61.95%

======================================================================
Epoch 45/100
Train Loss: 1.3219, Train Acc: 61.96%
Val Loss: 1.7112, Val Acc: 54.48%
Time: 93.66s
======================================================================

Epoch [46] Batch [0/352] Loss: 1.0725 Acc: 69.53%
Epoch [46] Batch [10/352] Loss: 1.1465 Acc: 65.13%
Epoch [46] Batch [20/352] Loss: 1.1488 Acc: 64.84%
Epoch [46] Batch [30/352] Loss: 1.2674 Acc: 64.54%
Epoch [46] Batch [40/352] Loss: 1.6050 Acc: 64.18%
Epoch [46] Batch [50/352] Loss: 1.1588 Acc: 64.26%
Epoch [46] Batch [60/352] Loss: 1.3478 Acc: 64.23%
Epoch [46] Batch [70/352] Loss: 1.3448 Acc: 64.26%
Epoch [46] Batch [80/352] Loss: 1.6052 Acc: 64.07%
Epoch [46] Batch [90/352] Loss: 1.1608 Acc: 63.77%
Epoch [46] Batch [100/352] Loss: 1.1516 Acc: 63.74%
Epoch [46] Batch [110/352] Loss: 1.3698 Acc: 63.67%
Epoch [46] Batch [120/352] Loss: 1.3148 Acc: 63.66%
Epoch [46] Batch [130/352] Loss: 1.4531 Acc: 63.62%
Epoch [46] Batch [140/352] Loss: 1.2743 Acc: 63.67%
Epoch [46] Batch [150/352] Loss: 1.2130 Acc: 63.57%
Epoch [46] Batch [160/352] Loss: 1.1320 Acc: 63.38%
Epoch [46] Batch [170/352] Loss: 1.3651 Acc: 63.20%
Epoch [46] Batch [180/352] Loss: 1.3610 Acc: 63.10%
Epoch [46] Batch [190/352] Loss: 1.3376 Acc: 63.14%
Epoch [46] Batch [200/352] Loss: 1.2085 Acc: 63.23%
Epoch [46] Batch [210/352] Loss: 1.2580 Acc: 63.17%
Epoch [46] Batch [220/352] Loss: 1.2137 Acc: 63.08%
Epoch [46] Batch [230/352] Loss: 1.4988 Acc: 63.00%
Epoch [46] Batch [240/352] Loss: 1.6261 Acc: 62.91%
Epoch [46] Batch [250/352] Loss: 1.2835 Acc: 62.86%
Epoch [46] Batch [260/352] Loss: 1.4370 Acc: 62.87%
Epoch [46] Batch [270/352] Loss: 1.4040 Acc: 62.82%
Epoch [46] Batch [280/352] Loss: 1.3087 Acc: 62.69%
Epoch [46] Batch [290/352] Loss: 1.2714 Acc: 62.57%
Epoch [46] Batch [300/352] Loss: 1.1436 Acc: 62.53%
Epoch [46] Batch [310/352] Loss: 1.4122 Acc: 62.48%
Epoch [46] Batch [320/352] Loss: 1.3515 Acc: 62.43%
Epoch [46] Batch [330/352] Loss: 1.2357 Acc: 62.41%
Epoch [46] Batch [340/352] Loss: 1.3909 Acc: 62.35%
Epoch [46] Batch [350/352] Loss: 1.2344 Acc: 62.26%

======================================================================
Epoch 46/100
Train Loss: 1.3110, Train Acc: 62.26%
Val Loss: 1.6680, Val Acc: 54.62%
Time: 94.21s
======================================================================

Epoch [47] Batch [0/352] Loss: 1.3202 Acc: 59.38%
Epoch [47] Batch [10/352] Loss: 1.0289 Acc: 61.79%
Epoch [47] Batch [20/352] Loss: 1.3008 Acc: 62.83%
Epoch [47] Batch [30/352] Loss: 1.2029 Acc: 63.68%
Epoch [47] Batch [40/352] Loss: 1.3005 Acc: 63.93%
Epoch [47] Batch [50/352] Loss: 1.0266 Acc: 64.23%
Epoch [47] Batch [60/352] Loss: 1.0635 Acc: 64.08%
Epoch [47] Batch [70/352] Loss: 1.2131 Acc: 63.99%
Epoch [47] Batch [80/352] Loss: 1.4502 Acc: 64.12%
Epoch [47] Batch [90/352] Loss: 1.4437 Acc: 64.00%
Epoch [47] Batch [100/352] Loss: 1.2816 Acc: 63.84%
Epoch [47] Batch [110/352] Loss: 1.1966 Acc: 63.97%
Epoch [47] Batch [120/352] Loss: 1.3467 Acc: 63.84%
Epoch [47] Batch [130/352] Loss: 1.4024 Acc: 63.65%
Epoch [47] Batch [140/352] Loss: 1.2537 Acc: 63.44%
Epoch [47] Batch [150/352] Loss: 1.2310 Acc: 63.45%
Epoch [47] Batch [160/352] Loss: 1.3781 Acc: 63.41%
Epoch [47] Batch [170/352] Loss: 1.3269 Acc: 63.32%
Epoch [47] Batch [180/352] Loss: 1.2475 Acc: 63.23%
Epoch [47] Batch [190/352] Loss: 1.2673 Acc: 63.23%
Epoch [47] Batch [200/352] Loss: 1.3052 Acc: 63.23%
Epoch [47] Batch [210/352] Loss: 0.9535 Acc: 63.26%
Epoch [47] Batch [220/352] Loss: 1.3457 Acc: 63.17%
Epoch [47] Batch [230/352] Loss: 1.2133 Acc: 63.03%
Epoch [47] Batch [240/352] Loss: 1.4267 Acc: 63.00%
Epoch [47] Batch [250/352] Loss: 1.4170 Acc: 62.97%
Epoch [47] Batch [260/352] Loss: 1.3597 Acc: 62.89%
Epoch [47] Batch [270/352] Loss: 1.4240 Acc: 62.87%
Epoch [47] Batch [280/352] Loss: 1.5887 Acc: 62.78%
Epoch [47] Batch [290/352] Loss: 1.6189 Acc: 62.70%
Epoch [47] Batch [300/352] Loss: 1.4972 Acc: 62.73%
Epoch [47] Batch [310/352] Loss: 1.2385 Acc: 62.69%
Epoch [47] Batch [320/352] Loss: 1.3744 Acc: 62.66%
Epoch [47] Batch [330/352] Loss: 1.4179 Acc: 62.60%
Epoch [47] Batch [340/352] Loss: 1.4591 Acc: 62.56%
Epoch [47] Batch [350/352] Loss: 1.0901 Acc: 62.55%

======================================================================
Epoch 47/100
Train Loss: 1.2983, Train Acc: 62.53%
Val Loss: 1.7143, Val Acc: 53.46%
Time: 94.00s
======================================================================

Epoch [48] Batch [0/352] Loss: 1.1427 Acc: 67.19%
Epoch [48] Batch [10/352] Loss: 1.2119 Acc: 62.78%
Epoch [48] Batch [20/352] Loss: 1.1719 Acc: 62.43%
Epoch [48] Batch [30/352] Loss: 1.1930 Acc: 63.23%
Epoch [48] Batch [40/352] Loss: 1.1947 Acc: 63.53%
Epoch [48] Batch [50/352] Loss: 1.2460 Acc: 64.14%
Epoch [48] Batch [60/352] Loss: 1.2242 Acc: 64.05%
Epoch [48] Batch [70/352] Loss: 1.4542 Acc: 63.85%
Epoch [48] Batch [80/352] Loss: 1.1954 Acc: 63.95%
Epoch [48] Batch [90/352] Loss: 1.2710 Acc: 63.88%
Epoch [48] Batch [100/352] Loss: 1.1989 Acc: 63.71%
Epoch [48] Batch [110/352] Loss: 1.3817 Acc: 63.59%
Epoch [48] Batch [120/352] Loss: 1.1717 Acc: 63.38%
Epoch [48] Batch [130/352] Loss: 1.3638 Acc: 63.41%
Epoch [48] Batch [140/352] Loss: 1.2781 Acc: 63.43%
Epoch [48] Batch [150/352] Loss: 1.2452 Acc: 63.56%
Epoch [48] Batch [160/352] Loss: 1.2971 Acc: 63.66%
Epoch [48] Batch [170/352] Loss: 1.0940 Acc: 63.71%
Epoch [48] Batch [180/352] Loss: 1.4143 Acc: 63.67%
Epoch [48] Batch [190/352] Loss: 1.3214 Acc: 63.56%
Epoch [48] Batch [200/352] Loss: 0.9895 Acc: 63.63%
Epoch [48] Batch [210/352] Loss: 0.9955 Acc: 63.59%
Epoch [48] Batch [220/352] Loss: 1.2279 Acc: 63.58%
Epoch [48] Batch [230/352] Loss: 1.4643 Acc: 63.53%
Epoch [48] Batch [240/352] Loss: 1.2746 Acc: 63.46%
Epoch [48] Batch [250/352] Loss: 1.5041 Acc: 63.39%
Epoch [48] Batch [260/352] Loss: 1.3112 Acc: 63.32%
Epoch [48] Batch [270/352] Loss: 1.4283 Acc: 63.24%
Epoch [48] Batch [280/352] Loss: 1.2588 Acc: 63.18%
Epoch [48] Batch [290/352] Loss: 1.4880 Acc: 63.10%
Epoch [48] Batch [300/352] Loss: 1.1386 Acc: 63.07%
Epoch [48] Batch [310/352] Loss: 1.3515 Acc: 63.08%
Epoch [48] Batch [320/352] Loss: 1.1391 Acc: 63.02%
Epoch [48] Batch [330/352] Loss: 1.2617 Acc: 63.00%
Epoch [48] Batch [340/352] Loss: 1.2622 Acc: 62.97%
Epoch [48] Batch [350/352] Loss: 1.3405 Acc: 62.93%

======================================================================
Epoch 48/100
Train Loss: 1.2889, Train Acc: 62.91%
Val Loss: 1.5803, Val Acc: 57.28%
Time: 94.21s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 57.28%)
Epoch [49] Batch [0/352] Loss: 1.4099 Acc: 63.28%
Epoch [49] Batch [10/352] Loss: 1.3500 Acc: 65.84%
Epoch [49] Batch [20/352] Loss: 1.3437 Acc: 63.80%
Epoch [49] Batch [30/352] Loss: 1.1869 Acc: 64.24%
Epoch [49] Batch [40/352] Loss: 1.0208 Acc: 64.08%
Epoch [49] Batch [50/352] Loss: 1.2934 Acc: 63.80%
Epoch [49] Batch [60/352] Loss: 1.2676 Acc: 63.99%
Epoch [49] Batch [70/352] Loss: 1.3604 Acc: 63.90%
Epoch [49] Batch [80/352] Loss: 1.3955 Acc: 63.72%
Epoch [49] Batch [90/352] Loss: 1.2468 Acc: 63.75%
Epoch [49] Batch [100/352] Loss: 1.0577 Acc: 64.09%
Epoch [49] Batch [110/352] Loss: 1.2553 Acc: 63.99%
Epoch [49] Batch [120/352] Loss: 1.0389 Acc: 64.04%
Epoch [49] Batch [130/352] Loss: 1.3433 Acc: 63.90%
Epoch [49] Batch [140/352] Loss: 1.4054 Acc: 63.74%
Epoch [49] Batch [150/352] Loss: 1.2391 Acc: 63.90%
Epoch [49] Batch [160/352] Loss: 1.3974 Acc: 63.83%
Epoch [49] Batch [170/352] Loss: 1.3282 Acc: 63.81%
Epoch [49] Batch [180/352] Loss: 1.3292 Acc: 63.72%
Epoch [49] Batch [190/352] Loss: 1.5788 Acc: 63.70%
Epoch [49] Batch [200/352] Loss: 1.6493 Acc: 63.53%
Epoch [49] Batch [210/352] Loss: 1.3427 Acc: 63.51%
Epoch [49] Batch [220/352] Loss: 1.2980 Acc: 63.43%
Epoch [49] Batch [230/352] Loss: 1.4684 Acc: 63.38%
Epoch [49] Batch [240/352] Loss: 1.6521 Acc: 63.26%
Epoch [49] Batch [250/352] Loss: 1.1714 Acc: 63.16%
Epoch [49] Batch [260/352] Loss: 1.3181 Acc: 63.21%
Epoch [49] Batch [270/352] Loss: 1.1717 Acc: 63.24%
Epoch [49] Batch [280/352] Loss: 1.2572 Acc: 63.22%
Epoch [49] Batch [290/352] Loss: 1.2209 Acc: 63.30%
Epoch [49] Batch [300/352] Loss: 1.2437 Acc: 63.20%
Epoch [49] Batch [310/352] Loss: 1.0928 Acc: 63.21%
Epoch [49] Batch [320/352] Loss: 1.2670 Acc: 63.19%
Epoch [49] Batch [330/352] Loss: 1.1593 Acc: 63.18%
Epoch [49] Batch [340/352] Loss: 1.1807 Acc: 63.15%
Epoch [49] Batch [350/352] Loss: 1.0294 Acc: 63.13%

======================================================================
Epoch 49/100
Train Loss: 1.2738, Train Acc: 63.13%
Val Loss: 1.6361, Val Acc: 55.06%
Time: 94.33s
======================================================================

Epoch [50] Batch [0/352] Loss: 1.3157 Acc: 60.94%
Epoch [50] Batch [10/352] Loss: 1.2720 Acc: 63.28%
Epoch [50] Batch [20/352] Loss: 1.0755 Acc: 63.47%
Epoch [50] Batch [30/352] Loss: 1.2538 Acc: 63.91%
Epoch [50] Batch [40/352] Loss: 1.2172 Acc: 63.72%
Epoch [50] Batch [50/352] Loss: 1.0277 Acc: 63.65%
Epoch [50] Batch [60/352] Loss: 1.0715 Acc: 64.06%
Epoch [50] Batch [70/352] Loss: 1.1365 Acc: 64.10%
Epoch [50] Batch [80/352] Loss: 1.0935 Acc: 64.33%
Epoch [50] Batch [90/352] Loss: 1.4076 Acc: 64.05%
Epoch [50] Batch [100/352] Loss: 1.3664 Acc: 64.05%
Epoch [50] Batch [110/352] Loss: 1.2848 Acc: 63.88%
Epoch [50] Batch [120/352] Loss: 1.0491 Acc: 64.08%
Epoch [50] Batch [130/352] Loss: 1.4482 Acc: 63.96%
Epoch [50] Batch [140/352] Loss: 1.5059 Acc: 63.85%
Epoch [50] Batch [150/352] Loss: 1.0848 Acc: 63.92%
Epoch [50] Batch [160/352] Loss: 1.0742 Acc: 63.80%
Epoch [50] Batch [170/352] Loss: 1.3160 Acc: 63.76%
Epoch [50] Batch [180/352] Loss: 1.1997 Acc: 63.83%
Epoch [50] Batch [190/352] Loss: 1.4373 Acc: 63.74%
Epoch [50] Batch [200/352] Loss: 0.9489 Acc: 63.77%
Epoch [50] Batch [210/352] Loss: 1.5504 Acc: 63.66%
Epoch [50] Batch [220/352] Loss: 1.2261 Acc: 63.57%
Epoch [50] Batch [230/352] Loss: 1.1664 Acc: 63.58%
Epoch [50] Batch [240/352] Loss: 1.4440 Acc: 63.62%
Epoch [50] Batch [250/352] Loss: 1.2243 Acc: 63.59%
Epoch [50] Batch [260/352] Loss: 1.4192 Acc: 63.57%
Epoch [50] Batch [270/352] Loss: 1.4212 Acc: 63.53%
Epoch [50] Batch [280/352] Loss: 1.1200 Acc: 63.44%
Epoch [50] Batch [290/352] Loss: 1.2283 Acc: 63.45%
Epoch [50] Batch [300/352] Loss: 1.1399 Acc: 63.47%
Epoch [50] Batch [310/352] Loss: 1.2762 Acc: 63.42%
Epoch [50] Batch [320/352] Loss: 1.4614 Acc: 63.38%
Epoch [50] Batch [330/352] Loss: 1.2844 Acc: 63.39%
Epoch [50] Batch [340/352] Loss: 1.3577 Acc: 63.37%
Epoch [50] Batch [350/352] Loss: 1.3058 Acc: 63.36%

======================================================================
Epoch 50/100
Train Loss: 1.2617, Train Acc: 63.35%
Val Loss: 1.5517, Val Acc: 57.32%
Time: 93.92s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 57.32%)
Epoch [51] Batch [0/352] Loss: 1.4439 Acc: 64.84%
Epoch [51] Batch [10/352] Loss: 1.3569 Acc: 64.06%
Epoch [51] Batch [20/352] Loss: 1.1773 Acc: 63.39%
Epoch [51] Batch [30/352] Loss: 1.1234 Acc: 63.53%
Epoch [51] Batch [40/352] Loss: 1.2831 Acc: 63.45%
Epoch [51] Batch [50/352] Loss: 1.2803 Acc: 63.83%
Epoch [51] Batch [60/352] Loss: 1.2957 Acc: 64.11%
Epoch [51] Batch [70/352] Loss: 1.3286 Acc: 64.25%
Epoch [51] Batch [80/352] Loss: 1.4003 Acc: 64.36%
Epoch [51] Batch [90/352] Loss: 1.1283 Acc: 64.37%
Epoch [51] Batch [100/352] Loss: 1.1702 Acc: 64.35%
Epoch [51] Batch [110/352] Loss: 1.1461 Acc: 64.58%
Epoch [51] Batch [120/352] Loss: 1.0987 Acc: 64.64%
Epoch [51] Batch [130/352] Loss: 1.1407 Acc: 64.77%
Epoch [51] Batch [140/352] Loss: 1.2153 Acc: 64.77%
Epoch [51] Batch [150/352] Loss: 1.4382 Acc: 64.87%
Epoch [51] Batch [160/352] Loss: 1.0787 Acc: 64.81%
Epoch [51] Batch [170/352] Loss: 1.3861 Acc: 64.70%
Epoch [51] Batch [180/352] Loss: 1.1397 Acc: 64.78%
Epoch [51] Batch [190/352] Loss: 1.4421 Acc: 64.65%
Epoch [51] Batch [200/352] Loss: 1.1556 Acc: 64.58%
Epoch [51] Batch [210/352] Loss: 1.2985 Acc: 64.48%
Epoch [51] Batch [220/352] Loss: 1.4044 Acc: 64.37%
Epoch [51] Batch [230/352] Loss: 1.1440 Acc: 64.30%
Epoch [51] Batch [240/352] Loss: 1.1847 Acc: 64.31%
Epoch [51] Batch [250/352] Loss: 1.4221 Acc: 64.29%
Epoch [51] Batch [260/352] Loss: 1.2188 Acc: 64.18%
Epoch [51] Batch [270/352] Loss: 1.4224 Acc: 64.11%
Epoch [51] Batch [280/352] Loss: 1.1454 Acc: 64.03%
Epoch [51] Batch [290/352] Loss: 1.2718 Acc: 63.99%
Epoch [51] Batch [300/352] Loss: 1.3658 Acc: 63.96%
Epoch [51] Batch [310/352] Loss: 1.2406 Acc: 63.93%
Epoch [51] Batch [320/352] Loss: 1.3336 Acc: 63.85%
Epoch [51] Batch [330/352] Loss: 1.3465 Acc: 63.78%
Epoch [51] Batch [340/352] Loss: 1.1350 Acc: 63.80%
Epoch [51] Batch [350/352] Loss: 0.9998 Acc: 63.76%

======================================================================
Epoch 51/100
Train Loss: 1.2473, Train Acc: 63.77%
Val Loss: 1.9208, Val Acc: 50.28%
Time: 92.43s
======================================================================

Epoch [52] Batch [0/352] Loss: 1.1826 Acc: 64.84%
Epoch [52] Batch [10/352] Loss: 1.3048 Acc: 66.19%
Epoch [52] Batch [20/352] Loss: 1.3688 Acc: 65.14%
Epoch [52] Batch [30/352] Loss: 1.2687 Acc: 65.57%
Epoch [52] Batch [40/352] Loss: 1.0303 Acc: 65.76%
Epoch [52] Batch [50/352] Loss: 1.2561 Acc: 65.72%
Epoch [52] Batch [60/352] Loss: 1.3407 Acc: 65.61%
Epoch [52] Batch [70/352] Loss: 1.1923 Acc: 65.65%
Epoch [52] Batch [80/352] Loss: 1.1334 Acc: 65.47%
Epoch [52] Batch [90/352] Loss: 1.3487 Acc: 65.18%
Epoch [52] Batch [100/352] Loss: 1.2732 Acc: 64.95%
Epoch [52] Batch [110/352] Loss: 1.4016 Acc: 64.65%
Epoch [52] Batch [120/352] Loss: 1.4070 Acc: 64.87%
Epoch [52] Batch [130/352] Loss: 1.1283 Acc: 64.93%
Epoch [52] Batch [140/352] Loss: 1.1758 Acc: 64.90%
Epoch [52] Batch [150/352] Loss: 1.2645 Acc: 64.79%
Epoch [52] Batch [160/352] Loss: 1.2589 Acc: 64.82%
Epoch [52] Batch [170/352] Loss: 1.2466 Acc: 64.70%
Epoch [52] Batch [180/352] Loss: 1.4108 Acc: 64.58%
Epoch [52] Batch [190/352] Loss: 1.0538 Acc: 64.67%
Epoch [52] Batch [200/352] Loss: 1.1421 Acc: 64.68%
Epoch [52] Batch [210/352] Loss: 1.4966 Acc: 64.62%
Epoch [52] Batch [220/352] Loss: 1.2467 Acc: 64.57%
Epoch [52] Batch [230/352] Loss: 1.2726 Acc: 64.61%
Epoch [52] Batch [240/352] Loss: 1.2345 Acc: 64.63%
Epoch [52] Batch [250/352] Loss: 1.1638 Acc: 64.57%
Epoch [52] Batch [260/352] Loss: 1.1915 Acc: 64.54%
Epoch [52] Batch [270/352] Loss: 1.2720 Acc: 64.46%
Epoch [52] Batch [280/352] Loss: 1.1295 Acc: 64.50%
Epoch [52] Batch [290/352] Loss: 1.4542 Acc: 64.41%
Epoch [52] Batch [300/352] Loss: 1.1594 Acc: 64.36%
Epoch [52] Batch [310/352] Loss: 1.3283 Acc: 64.38%
Epoch [52] Batch [320/352] Loss: 1.3758 Acc: 64.31%
Epoch [52] Batch [330/352] Loss: 1.2190 Acc: 64.29%
Epoch [52] Batch [340/352] Loss: 1.3320 Acc: 64.25%
Epoch [52] Batch [350/352] Loss: 1.4017 Acc: 64.19%

======================================================================
Epoch 52/100
Train Loss: 1.2326, Train Acc: 64.18%
Val Loss: 1.7924, Val Acc: 53.66%
Time: 93.01s
======================================================================

Epoch [53] Batch [0/352] Loss: 1.2912 Acc: 60.16%
Epoch [53] Batch [10/352] Loss: 0.9391 Acc: 67.90%
Epoch [53] Batch [20/352] Loss: 1.2531 Acc: 65.81%
Epoch [53] Batch [30/352] Loss: 1.3654 Acc: 65.50%
Epoch [53] Batch [40/352] Loss: 1.4116 Acc: 65.40%
Epoch [53] Batch [50/352] Loss: 1.1529 Acc: 65.69%
Epoch [53] Batch [60/352] Loss: 1.2254 Acc: 65.75%
Epoch [53] Batch [70/352] Loss: 1.6350 Acc: 65.69%
Epoch [53] Batch [80/352] Loss: 1.1132 Acc: 65.91%
Epoch [53] Batch [90/352] Loss: 1.1815 Acc: 65.77%
Epoch [53] Batch [100/352] Loss: 1.1262 Acc: 65.58%
Epoch [53] Batch [110/352] Loss: 1.2964 Acc: 65.65%
Epoch [53] Batch [120/352] Loss: 1.2008 Acc: 65.65%
Epoch [53] Batch [130/352] Loss: 1.1203 Acc: 65.40%
Epoch [53] Batch [140/352] Loss: 1.4715 Acc: 65.26%
Epoch [53] Batch [150/352] Loss: 1.2212 Acc: 65.40%
Epoch [53] Batch [160/352] Loss: 1.1606 Acc: 65.39%
Epoch [53] Batch [170/352] Loss: 1.0415 Acc: 65.37%
Epoch [53] Batch [180/352] Loss: 1.1799 Acc: 65.24%
Epoch [53] Batch [190/352] Loss: 1.3251 Acc: 65.18%
Epoch [53] Batch [200/352] Loss: 1.3541 Acc: 65.10%
Epoch [53] Batch [210/352] Loss: 1.2919 Acc: 65.01%
Epoch [53] Batch [220/352] Loss: 1.1185 Acc: 64.97%
Epoch [53] Batch [230/352] Loss: 1.1214 Acc: 64.95%
Epoch [53] Batch [240/352] Loss: 1.2554 Acc: 64.96%
Epoch [53] Batch [250/352] Loss: 0.9983 Acc: 64.94%
Epoch [53] Batch [260/352] Loss: 1.5299 Acc: 64.89%
Epoch [53] Batch [270/352] Loss: 1.3457 Acc: 64.86%
Epoch [53] Batch [280/352] Loss: 1.3078 Acc: 64.85%
Epoch [53] Batch [290/352] Loss: 1.3612 Acc: 64.77%
Epoch [53] Batch [300/352] Loss: 1.1652 Acc: 64.82%
Epoch [53] Batch [310/352] Loss: 1.1243 Acc: 64.81%
Epoch [53] Batch [320/352] Loss: 1.3313 Acc: 64.70%
Epoch [53] Batch [330/352] Loss: 1.2058 Acc: 64.72%
Epoch [53] Batch [340/352] Loss: 1.2126 Acc: 64.65%
Epoch [53] Batch [350/352] Loss: 1.2432 Acc: 64.60%

======================================================================
Epoch 53/100
Train Loss: 1.2241, Train Acc: 64.59%
Val Loss: 1.6954, Val Acc: 55.22%
Time: 92.22s
======================================================================

Epoch [54] Batch [0/352] Loss: 1.2544 Acc: 67.97%
Epoch [54] Batch [10/352] Loss: 1.0700 Acc: 66.48%
Epoch [54] Batch [20/352] Loss: 1.1714 Acc: 65.74%
Epoch [54] Batch [30/352] Loss: 1.1712 Acc: 65.80%
Epoch [54] Batch [40/352] Loss: 1.1500 Acc: 65.89%
Epoch [54] Batch [50/352] Loss: 1.1019 Acc: 66.01%
Epoch [54] Batch [60/352] Loss: 1.1431 Acc: 66.12%
Epoch [54] Batch [70/352] Loss: 1.2287 Acc: 66.02%
Epoch [54] Batch [80/352] Loss: 1.2253 Acc: 65.89%
Epoch [54] Batch [90/352] Loss: 1.3247 Acc: 65.63%
Epoch [54] Batch [100/352] Loss: 1.1902 Acc: 65.39%
Epoch [54] Batch [110/352] Loss: 1.2721 Acc: 65.41%
Epoch [54] Batch [120/352] Loss: 1.1626 Acc: 65.55%
Epoch [54] Batch [130/352] Loss: 1.1306 Acc: 65.72%
Epoch [54] Batch [140/352] Loss: 1.1206 Acc: 65.64%
Epoch [54] Batch [150/352] Loss: 1.0919 Acc: 65.68%
Epoch [54] Batch [160/352] Loss: 1.2058 Acc: 65.46%
Epoch [54] Batch [170/352] Loss: 1.0756 Acc: 65.41%
Epoch [54] Batch [180/352] Loss: 1.0905 Acc: 65.44%
Epoch [54] Batch [190/352] Loss: 1.1106 Acc: 65.37%
Epoch [54] Batch [200/352] Loss: 1.2068 Acc: 65.35%
Epoch [54] Batch [210/352] Loss: 1.2366 Acc: 65.27%
Epoch [54] Batch [220/352] Loss: 1.0572 Acc: 65.25%
Epoch [54] Batch [230/352] Loss: 1.3570 Acc: 65.27%
Epoch [54] Batch [240/352] Loss: 1.1515 Acc: 65.25%
Epoch [54] Batch [250/352] Loss: 1.3957 Acc: 65.26%
Epoch [54] Batch [260/352] Loss: 1.3560 Acc: 65.18%
Epoch [54] Batch [270/352] Loss: 1.5237 Acc: 65.10%
Epoch [54] Batch [280/352] Loss: 1.4398 Acc: 64.99%
Epoch [54] Batch [290/352] Loss: 1.2044 Acc: 64.91%
Epoch [54] Batch [300/352] Loss: 1.1941 Acc: 64.85%
Epoch [54] Batch [310/352] Loss: 1.1222 Acc: 64.84%
Epoch [54] Batch [320/352] Loss: 0.9722 Acc: 64.88%
Epoch [54] Batch [330/352] Loss: 1.3367 Acc: 64.90%
Epoch [54] Batch [340/352] Loss: 1.5559 Acc: 64.83%
Epoch [54] Batch [350/352] Loss: 1.4681 Acc: 64.84%

======================================================================
Epoch 54/100
Train Loss: 1.2101, Train Acc: 64.83%
Val Loss: 1.6289, Val Acc: 56.96%
Time: 92.90s
======================================================================

Epoch [55] Batch [0/352] Loss: 1.2062 Acc: 57.81%
Epoch [55] Batch [10/352] Loss: 1.0747 Acc: 66.12%
Epoch [55] Batch [20/352] Loss: 1.1420 Acc: 65.48%
Epoch [55] Batch [30/352] Loss: 1.1885 Acc: 66.23%
Epoch [55] Batch [40/352] Loss: 1.2557 Acc: 66.31%
Epoch [55] Batch [50/352] Loss: 1.0303 Acc: 66.22%
Epoch [55] Batch [60/352] Loss: 1.1839 Acc: 66.39%
Epoch [55] Batch [70/352] Loss: 1.1495 Acc: 66.74%
Epoch [55] Batch [80/352] Loss: 1.2423 Acc: 66.76%
Epoch [55] Batch [90/352] Loss: 1.0047 Acc: 66.66%
Epoch [55] Batch [100/352] Loss: 1.2825 Acc: 66.34%
Epoch [55] Batch [110/352] Loss: 1.1742 Acc: 66.42%
Epoch [55] Batch [120/352] Loss: 1.2571 Acc: 66.20%
Epoch [55] Batch [130/352] Loss: 0.9910 Acc: 66.20%
Epoch [55] Batch [140/352] Loss: 1.3149 Acc: 66.05%
Epoch [55] Batch [150/352] Loss: 1.2707 Acc: 66.15%
Epoch [55] Batch [160/352] Loss: 1.2685 Acc: 66.29%
Epoch [55] Batch [170/352] Loss: 1.3011 Acc: 66.21%
Epoch [55] Batch [180/352] Loss: 1.3134 Acc: 66.26%
Epoch [55] Batch [190/352] Loss: 1.2837 Acc: 66.17%
Epoch [55] Batch [200/352] Loss: 1.1957 Acc: 66.17%
Epoch [55] Batch [210/352] Loss: 1.2091 Acc: 66.10%
Epoch [55] Batch [220/352] Loss: 1.5144 Acc: 65.99%
Epoch [55] Batch [230/352] Loss: 1.1056 Acc: 65.90%
Epoch [55] Batch [240/352] Loss: 1.3450 Acc: 65.91%
Epoch [55] Batch [250/352] Loss: 1.0703 Acc: 65.80%
Epoch [55] Batch [260/352] Loss: 1.2110 Acc: 65.80%
Epoch [55] Batch [270/352] Loss: 1.3059 Acc: 65.76%
Epoch [55] Batch [280/352] Loss: 1.3086 Acc: 65.75%
Epoch [55] Batch [290/352] Loss: 1.2227 Acc: 65.61%
Epoch [55] Batch [300/352] Loss: 1.3157 Acc: 65.49%
Epoch [55] Batch [310/352] Loss: 1.3475 Acc: 65.45%
Epoch [55] Batch [320/352] Loss: 1.1418 Acc: 65.41%
Epoch [55] Batch [330/352] Loss: 1.1959 Acc: 65.41%
Epoch [55] Batch [340/352] Loss: 1.1656 Acc: 65.46%
Epoch [55] Batch [350/352] Loss: 1.2577 Acc: 65.43%

======================================================================
Epoch 55/100
Train Loss: 1.1909, Train Acc: 65.43%
Val Loss: 1.5866, Val Acc: 57.80%
Time: 92.32s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 57.80%)
Epoch [56] Batch [0/352] Loss: 1.0083 Acc: 68.75%
Epoch [56] Batch [10/352] Loss: 1.1200 Acc: 68.18%
Epoch [56] Batch [20/352] Loss: 1.0964 Acc: 67.30%
Epoch [56] Batch [30/352] Loss: 1.4334 Acc: 67.16%
Epoch [56] Batch [40/352] Loss: 1.0882 Acc: 67.11%
Epoch [56] Batch [50/352] Loss: 1.2195 Acc: 66.82%
Epoch [56] Batch [60/352] Loss: 1.1832 Acc: 66.61%
Epoch [56] Batch [70/352] Loss: 1.1423 Acc: 66.57%
Epoch [56] Batch [80/352] Loss: 1.2483 Acc: 66.55%
Epoch [56] Batch [90/352] Loss: 0.9173 Acc: 66.79%
Epoch [56] Batch [100/352] Loss: 1.3638 Acc: 66.62%
Epoch [56] Batch [110/352] Loss: 1.0262 Acc: 66.55%
Epoch [56] Batch [120/352] Loss: 1.1962 Acc: 66.63%
Epoch [56] Batch [130/352] Loss: 1.2205 Acc: 66.53%
Epoch [56] Batch [140/352] Loss: 1.0963 Acc: 66.54%
Epoch [56] Batch [150/352] Loss: 0.9677 Acc: 66.39%
Epoch [56] Batch [160/352] Loss: 1.0807 Acc: 66.54%
Epoch [56] Batch [170/352] Loss: 1.3258 Acc: 66.37%
Epoch [56] Batch [180/352] Loss: 1.3732 Acc: 66.30%
Epoch [56] Batch [190/352] Loss: 1.1602 Acc: 66.25%
Epoch [56] Batch [200/352] Loss: 1.0924 Acc: 66.20%
Epoch [56] Batch [210/352] Loss: 1.1679 Acc: 66.20%
Epoch [56] Batch [220/352] Loss: 1.0702 Acc: 66.09%
Epoch [56] Batch [230/352] Loss: 1.2120 Acc: 66.04%
Epoch [56] Batch [240/352] Loss: 0.9672 Acc: 66.04%
Epoch [56] Batch [250/352] Loss: 1.1293 Acc: 65.97%
Epoch [56] Batch [260/352] Loss: 1.1783 Acc: 65.93%
Epoch [56] Batch [270/352] Loss: 1.0861 Acc: 65.82%
Epoch [56] Batch [280/352] Loss: 1.4738 Acc: 65.86%
Epoch [56] Batch [290/352] Loss: 1.4670 Acc: 65.78%
Epoch [56] Batch [300/352] Loss: 1.2892 Acc: 65.87%
Epoch [56] Batch [310/352] Loss: 1.1251 Acc: 65.91%
Epoch [56] Batch [320/352] Loss: 0.9636 Acc: 65.91%
Epoch [56] Batch [330/352] Loss: 1.0161 Acc: 65.90%
Epoch [56] Batch [340/352] Loss: 1.2476 Acc: 65.92%
Epoch [56] Batch [350/352] Loss: 1.1422 Acc: 65.95%

======================================================================
Epoch 56/100
Train Loss: 1.1706, Train Acc: 65.96%
Val Loss: 1.6947, Val Acc: 54.98%
Time: 92.34s
======================================================================

Epoch [57] Batch [0/352] Loss: 0.8662 Acc: 78.91%
Epoch [57] Batch [10/352] Loss: 1.2014 Acc: 67.61%
Epoch [57] Batch [20/352] Loss: 1.1845 Acc: 67.86%
Epoch [57] Batch [30/352] Loss: 1.2527 Acc: 66.91%
Epoch [57] Batch [40/352] Loss: 1.2108 Acc: 66.96%
Epoch [57] Batch [50/352] Loss: 1.2702 Acc: 66.74%
Epoch [57] Batch [60/352] Loss: 1.0976 Acc: 66.51%
Epoch [57] Batch [70/352] Loss: 1.3505 Acc: 66.32%
Epoch [57] Batch [80/352] Loss: 1.2742 Acc: 66.20%
Epoch [57] Batch [90/352] Loss: 1.1096 Acc: 66.02%
Epoch [57] Batch [100/352] Loss: 0.9697 Acc: 65.99%
Epoch [57] Batch [110/352] Loss: 1.1204 Acc: 66.11%
Epoch [57] Batch [120/352] Loss: 1.1348 Acc: 66.19%
Epoch [57] Batch [130/352] Loss: 1.0404 Acc: 66.23%
Epoch [57] Batch [140/352] Loss: 1.1795 Acc: 66.06%
Epoch [57] Batch [150/352] Loss: 1.3939 Acc: 66.23%
Epoch [57] Batch [160/352] Loss: 1.2069 Acc: 66.20%
Epoch [57] Batch [170/352] Loss: 1.2648 Acc: 66.17%
Epoch [57] Batch [180/352] Loss: 1.3653 Acc: 66.20%
Epoch [57] Batch [190/352] Loss: 1.2582 Acc: 66.34%
Epoch [57] Batch [200/352] Loss: 1.1625 Acc: 66.36%
Epoch [57] Batch [210/352] Loss: 1.1409 Acc: 66.32%
Epoch [57] Batch [220/352] Loss: 1.2607 Acc: 66.29%
Epoch [57] Batch [230/352] Loss: 1.2224 Acc: 66.33%
Epoch [57] Batch [240/352] Loss: 1.1484 Acc: 66.30%
Epoch [57] Batch [250/352] Loss: 1.2384 Acc: 66.26%
Epoch [57] Batch [260/352] Loss: 1.2171 Acc: 66.24%
Epoch [57] Batch [270/352] Loss: 1.3205 Acc: 66.16%
Epoch [57] Batch [280/352] Loss: 1.2412 Acc: 66.17%
Epoch [57] Batch [290/352] Loss: 1.0831 Acc: 66.08%
Epoch [57] Batch [300/352] Loss: 1.1965 Acc: 66.02%
Epoch [57] Batch [310/352] Loss: 1.2726 Acc: 66.02%
Epoch [57] Batch [320/352] Loss: 1.1318 Acc: 66.05%
Epoch [57] Batch [330/352] Loss: 1.2222 Acc: 66.04%
Epoch [57] Batch [340/352] Loss: 1.0976 Acc: 66.04%
Epoch [57] Batch [350/352] Loss: 1.1052 Acc: 65.93%

======================================================================
Epoch 57/100
Train Loss: 1.1709, Train Acc: 65.92%
Val Loss: 1.6638, Val Acc: 56.82%
Time: 92.66s
======================================================================

Epoch [58] Batch [0/352] Loss: 1.0054 Acc: 71.09%
Epoch [58] Batch [10/352] Loss: 1.0020 Acc: 68.82%
Epoch [58] Batch [20/352] Loss: 0.9821 Acc: 68.08%
Epoch [58] Batch [30/352] Loss: 1.1461 Acc: 68.93%
Epoch [58] Batch [40/352] Loss: 1.0962 Acc: 68.71%
Epoch [58] Batch [50/352] Loss: 1.1306 Acc: 68.50%
Epoch [58] Batch [60/352] Loss: 1.1799 Acc: 68.25%
Epoch [58] Batch [70/352] Loss: 1.1783 Acc: 68.00%
Epoch [58] Batch [80/352] Loss: 0.9285 Acc: 67.91%
Epoch [58] Batch [90/352] Loss: 1.3672 Acc: 67.64%
Epoch [58] Batch [100/352] Loss: 1.1284 Acc: 67.50%
Epoch [58] Batch [110/352] Loss: 1.3007 Acc: 67.52%
Epoch [58] Batch [120/352] Loss: 1.0199 Acc: 67.50%
Epoch [58] Batch [130/352] Loss: 1.0523 Acc: 67.51%
Epoch [58] Batch [140/352] Loss: 1.2069 Acc: 67.50%
Epoch [58] Batch [150/352] Loss: 1.1071 Acc: 67.49%
Epoch [58] Batch [160/352] Loss: 1.3158 Acc: 67.36%
Epoch [58] Batch [170/352] Loss: 1.3269 Acc: 67.30%
Epoch [58] Batch [180/352] Loss: 1.0455 Acc: 67.26%
Epoch [58] Batch [190/352] Loss: 1.0927 Acc: 67.21%
Epoch [58] Batch [200/352] Loss: 1.1789 Acc: 67.12%
Epoch [58] Batch [210/352] Loss: 1.1504 Acc: 67.12%
Epoch [58] Batch [220/352] Loss: 1.3093 Acc: 67.05%
Epoch [58] Batch [230/352] Loss: 1.0089 Acc: 66.95%
Epoch [58] Batch [240/352] Loss: 1.0128 Acc: 66.92%
Epoch [58] Batch [250/352] Loss: 1.2497 Acc: 66.85%
Epoch [58] Batch [260/352] Loss: 1.2214 Acc: 66.80%
Epoch [58] Batch [270/352] Loss: 1.1844 Acc: 66.71%
Epoch [58] Batch [280/352] Loss: 1.0963 Acc: 66.80%
Epoch [58] Batch [290/352] Loss: 1.3093 Acc: 66.72%
Epoch [58] Batch [300/352] Loss: 1.3049 Acc: 66.67%
Epoch [58] Batch [310/352] Loss: 1.1152 Acc: 66.67%
Epoch [58] Batch [320/352] Loss: 1.0103 Acc: 66.69%
Epoch [58] Batch [330/352] Loss: 1.1758 Acc: 66.65%
Epoch [58] Batch [340/352] Loss: 1.0267 Acc: 66.66%
Epoch [58] Batch [350/352] Loss: 0.9972 Acc: 66.67%

======================================================================
Epoch 58/100
Train Loss: 1.1404, Train Acc: 66.66%
Val Loss: 1.6607, Val Acc: 56.20%
Time: 93.91s
======================================================================

Epoch [59] Batch [0/352] Loss: 1.1335 Acc: 71.88%
Epoch [59] Batch [10/352] Loss: 0.9204 Acc: 70.67%
Epoch [59] Batch [20/352] Loss: 1.0964 Acc: 68.97%
Epoch [59] Batch [30/352] Loss: 1.1303 Acc: 69.00%
Epoch [59] Batch [40/352] Loss: 0.9782 Acc: 68.77%
Epoch [59] Batch [50/352] Loss: 1.0653 Acc: 68.50%
Epoch [59] Batch [60/352] Loss: 1.4315 Acc: 68.16%
Epoch [59] Batch [70/352] Loss: 1.1923 Acc: 68.10%
Epoch [59] Batch [80/352] Loss: 1.1112 Acc: 68.02%
Epoch [59] Batch [90/352] Loss: 1.1986 Acc: 67.99%
Epoch [59] Batch [100/352] Loss: 1.0369 Acc: 67.91%
Epoch [59] Batch [110/352] Loss: 1.2466 Acc: 67.80%
Epoch [59] Batch [120/352] Loss: 1.2149 Acc: 67.62%
Epoch [59] Batch [130/352] Loss: 1.2078 Acc: 67.60%
Epoch [59] Batch [140/352] Loss: 1.1183 Acc: 67.56%
Epoch [59] Batch [150/352] Loss: 1.3538 Acc: 67.42%
Epoch [59] Batch [160/352] Loss: 0.9568 Acc: 67.44%
Epoch [59] Batch [170/352] Loss: 1.1354 Acc: 67.41%
Epoch [59] Batch [180/352] Loss: 1.1643 Acc: 67.35%
Epoch [59] Batch [190/352] Loss: 1.4954 Acc: 67.27%
Epoch [59] Batch [200/352] Loss: 1.0128 Acc: 67.20%
Epoch [59] Batch [210/352] Loss: 1.0369 Acc: 67.19%
Epoch [59] Batch [220/352] Loss: 1.1719 Acc: 67.19%
Epoch [59] Batch [230/352] Loss: 1.1265 Acc: 67.20%
Epoch [59] Batch [240/352] Loss: 1.0642 Acc: 67.26%
Epoch [59] Batch [250/352] Loss: 1.0127 Acc: 67.21%
Epoch [59] Batch [260/352] Loss: 1.0440 Acc: 67.15%
Epoch [59] Batch [270/352] Loss: 1.0880 Acc: 67.12%
Epoch [59] Batch [280/352] Loss: 1.1442 Acc: 67.07%
Epoch [59] Batch [290/352] Loss: 1.2147 Acc: 67.04%
Epoch [59] Batch [300/352] Loss: 1.2246 Acc: 67.02%
Epoch [59] Batch [310/352] Loss: 1.1766 Acc: 66.94%
Epoch [59] Batch [320/352] Loss: 1.1200 Acc: 66.99%
Epoch [59] Batch [330/352] Loss: 1.1854 Acc: 66.98%
Epoch [59] Batch [340/352] Loss: 0.8706 Acc: 67.02%
Epoch [59] Batch [350/352] Loss: 1.0831 Acc: 67.03%

======================================================================
Epoch 59/100
Train Loss: 1.1347, Train Acc: 67.03%
Val Loss: 1.5828, Val Acc: 57.44%
Time: 94.09s
======================================================================

Epoch [60] Batch [0/352] Loss: 1.1331 Acc: 64.06%
Epoch [60] Batch [10/352] Loss: 1.1805 Acc: 66.90%
Epoch [60] Batch [20/352] Loss: 0.9106 Acc: 67.82%
Epoch [60] Batch [30/352] Loss: 1.0450 Acc: 68.37%
Epoch [60] Batch [40/352] Loss: 1.0938 Acc: 68.35%
Epoch [60] Batch [50/352] Loss: 1.3258 Acc: 68.17%
Epoch [60] Batch [60/352] Loss: 0.8496 Acc: 68.03%
Epoch [60] Batch [70/352] Loss: 0.9757 Acc: 68.19%
Epoch [60] Batch [80/352] Loss: 1.0751 Acc: 68.22%
Epoch [60] Batch [90/352] Loss: 1.0251 Acc: 68.29%
Epoch [60] Batch [100/352] Loss: 1.1656 Acc: 68.05%
Epoch [60] Batch [110/352] Loss: 0.8818 Acc: 67.99%
Epoch [60] Batch [120/352] Loss: 1.0400 Acc: 68.12%
Epoch [60] Batch [130/352] Loss: 1.2368 Acc: 67.89%
Epoch [60] Batch [140/352] Loss: 1.2086 Acc: 67.79%
Epoch [60] Batch [150/352] Loss: 1.2021 Acc: 67.72%
Epoch [60] Batch [160/352] Loss: 0.9508 Acc: 67.76%
Epoch [60] Batch [170/352] Loss: 1.1097 Acc: 67.71%
Epoch [60] Batch [180/352] Loss: 1.1377 Acc: 67.72%
Epoch [60] Batch [190/352] Loss: 0.9219 Acc: 67.77%
Epoch [60] Batch [200/352] Loss: 1.1630 Acc: 67.64%
Epoch [60] Batch [210/352] Loss: 0.9794 Acc: 67.68%
Epoch [60] Batch [220/352] Loss: 1.0934 Acc: 67.60%
Epoch [60] Batch [230/352] Loss: 1.0884 Acc: 67.59%
Epoch [60] Batch [240/352] Loss: 0.9916 Acc: 67.60%
Epoch [60] Batch [250/352] Loss: 0.9237 Acc: 67.59%
Epoch [60] Batch [260/352] Loss: 1.2551 Acc: 67.61%
Epoch [60] Batch [270/352] Loss: 1.1124 Acc: 67.53%
Epoch [60] Batch [280/352] Loss: 1.2574 Acc: 67.51%
Epoch [60] Batch [290/352] Loss: 1.2855 Acc: 67.54%
Epoch [60] Batch [300/352] Loss: 0.9456 Acc: 67.49%
Epoch [60] Batch [310/352] Loss: 1.0292 Acc: 67.53%
Epoch [60] Batch [320/352] Loss: 1.2396 Acc: 67.56%
Epoch [60] Batch [330/352] Loss: 1.0383 Acc: 67.57%
Epoch [60] Batch [340/352] Loss: 1.2052 Acc: 67.56%
Epoch [60] Batch [350/352] Loss: 1.0231 Acc: 67.61%

======================================================================
Epoch 60/100
Train Loss: 1.1133, Train Acc: 67.61%
Val Loss: 1.5387, Val Acc: 59.38%
Time: 93.90s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 59.38%)
Epoch [61] Batch [0/352] Loss: 0.8236 Acc: 73.44%
Epoch [61] Batch [10/352] Loss: 0.9612 Acc: 72.73%
Epoch [61] Batch [20/352] Loss: 1.0936 Acc: 71.76%
Epoch [61] Batch [30/352] Loss: 0.7979 Acc: 71.35%
Epoch [61] Batch [40/352] Loss: 1.0141 Acc: 71.28%
Epoch [61] Batch [50/352] Loss: 1.1831 Acc: 70.68%
Epoch [61] Batch [60/352] Loss: 0.9840 Acc: 70.70%
Epoch [61] Batch [70/352] Loss: 1.0208 Acc: 70.25%
Epoch [61] Batch [80/352] Loss: 1.2595 Acc: 69.85%
Epoch [61] Batch [90/352] Loss: 1.0347 Acc: 69.83%
Epoch [61] Batch [100/352] Loss: 1.0265 Acc: 69.64%
Epoch [61] Batch [110/352] Loss: 0.9620 Acc: 69.57%
Epoch [61] Batch [120/352] Loss: 1.1447 Acc: 69.54%
Epoch [61] Batch [130/352] Loss: 1.1937 Acc: 69.47%
Epoch [61] Batch [140/352] Loss: 1.1014 Acc: 69.45%
Epoch [61] Batch [150/352] Loss: 1.1468 Acc: 69.27%
Epoch [61] Batch [160/352] Loss: 1.1713 Acc: 69.19%
Epoch [61] Batch [170/352] Loss: 1.0561 Acc: 69.13%
Epoch [61] Batch [180/352] Loss: 1.0259 Acc: 69.07%
Epoch [61] Batch [190/352] Loss: 1.2561 Acc: 69.04%
Epoch [61] Batch [200/352] Loss: 1.3693 Acc: 68.98%
Epoch [61] Batch [210/352] Loss: 1.0461 Acc: 68.89%
Epoch [61] Batch [220/352] Loss: 0.9811 Acc: 68.81%
Epoch [61] Batch [230/352] Loss: 0.9795 Acc: 68.75%
Epoch [61] Batch [240/352] Loss: 1.2095 Acc: 68.68%
Epoch [61] Batch [250/352] Loss: 1.0904 Acc: 68.55%
Epoch [61] Batch [260/352] Loss: 1.2025 Acc: 68.58%
Epoch [61] Batch [270/352] Loss: 1.1680 Acc: 68.53%
Epoch [61] Batch [280/352] Loss: 1.0843 Acc: 68.46%
Epoch [61] Batch [290/352] Loss: 1.2350 Acc: 68.36%
Epoch [61] Batch [300/352] Loss: 1.0735 Acc: 68.35%
Epoch [61] Batch [310/352] Loss: 1.3974 Acc: 68.31%
Epoch [61] Batch [320/352] Loss: 1.2559 Acc: 68.29%
Epoch [61] Batch [330/352] Loss: 1.2209 Acc: 68.20%
Epoch [61] Batch [340/352] Loss: 1.1954 Acc: 68.14%
Epoch [61] Batch [350/352] Loss: 1.3845 Acc: 68.07%

======================================================================
Epoch 61/100
Train Loss: 1.0990, Train Acc: 68.06%
Val Loss: 1.5120, Val Acc: 60.04%
Time: 93.83s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 60.04%)
Epoch [62] Batch [0/352] Loss: 0.8463 Acc: 76.56%
Epoch [62] Batch [10/352] Loss: 0.9709 Acc: 70.88%
Epoch [62] Batch [20/352] Loss: 1.1580 Acc: 69.16%
Epoch [62] Batch [30/352] Loss: 0.8509 Acc: 69.83%
Epoch [62] Batch [40/352] Loss: 0.7415 Acc: 70.16%
Epoch [62] Batch [50/352] Loss: 0.7312 Acc: 70.54%
Epoch [62] Batch [60/352] Loss: 0.8538 Acc: 70.36%
Epoch [62] Batch [70/352] Loss: 0.8185 Acc: 70.40%
Epoch [62] Batch [80/352] Loss: 1.0858 Acc: 70.47%
Epoch [62] Batch [90/352] Loss: 1.0768 Acc: 70.40%
Epoch [62] Batch [100/352] Loss: 0.9778 Acc: 70.33%
Epoch [62] Batch [110/352] Loss: 1.1736 Acc: 70.26%
Epoch [62] Batch [120/352] Loss: 0.9955 Acc: 70.22%
Epoch [62] Batch [130/352] Loss: 1.2151 Acc: 70.01%
Epoch [62] Batch [140/352] Loss: 0.8452 Acc: 69.92%
Epoch [62] Batch [150/352] Loss: 0.8471 Acc: 69.92%
Epoch [62] Batch [160/352] Loss: 1.1332 Acc: 69.76%
Epoch [62] Batch [170/352] Loss: 1.0313 Acc: 69.60%
Epoch [62] Batch [180/352] Loss: 0.9249 Acc: 69.54%
Epoch [62] Batch [190/352] Loss: 1.1312 Acc: 69.37%
Epoch [62] Batch [200/352] Loss: 0.9461 Acc: 69.26%
Epoch [62] Batch [210/352] Loss: 1.0204 Acc: 69.21%
Epoch [62] Batch [220/352] Loss: 0.9897 Acc: 69.11%
Epoch [62] Batch [230/352] Loss: 1.1649 Acc: 69.07%
Epoch [62] Batch [240/352] Loss: 1.0875 Acc: 68.94%
Epoch [62] Batch [250/352] Loss: 1.1963 Acc: 68.74%
Epoch [62] Batch [260/352] Loss: 1.1942 Acc: 68.68%
Epoch [62] Batch [270/352] Loss: 0.9290 Acc: 68.63%
Epoch [62] Batch [280/352] Loss: 0.9967 Acc: 68.66%
Epoch [62] Batch [290/352] Loss: 1.1260 Acc: 68.63%
Epoch [62] Batch [300/352] Loss: 1.2585 Acc: 68.59%
Epoch [62] Batch [310/352] Loss: 1.2719 Acc: 68.58%
Epoch [62] Batch [320/352] Loss: 1.0726 Acc: 68.57%
Epoch [62] Batch [330/352] Loss: 1.3979 Acc: 68.50%
Epoch [62] Batch [340/352] Loss: 0.9970 Acc: 68.46%
Epoch [62] Batch [350/352] Loss: 1.1504 Acc: 68.45%

======================================================================
Epoch 62/100
Train Loss: 1.0795, Train Acc: 68.41%
Val Loss: 1.4622, Val Acc: 60.90%
Time: 94.08s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 60.90%)
Epoch [63] Batch [0/352] Loss: 1.0751 Acc: 66.41%
Epoch [63] Batch [10/352] Loss: 0.8912 Acc: 70.53%
Epoch [63] Batch [20/352] Loss: 1.0399 Acc: 69.05%
Epoch [63] Batch [30/352] Loss: 0.9369 Acc: 69.51%
Epoch [63] Batch [40/352] Loss: 1.1487 Acc: 69.38%
Epoch [63] Batch [50/352] Loss: 0.8847 Acc: 69.45%
Epoch [63] Batch [60/352] Loss: 1.0932 Acc: 69.68%
Epoch [63] Batch [70/352] Loss: 1.1170 Acc: 69.76%
Epoch [63] Batch [80/352] Loss: 1.3418 Acc: 69.49%
Epoch [63] Batch [90/352] Loss: 0.9278 Acc: 69.39%
Epoch [63] Batch [100/352] Loss: 1.0354 Acc: 69.32%
Epoch [63] Batch [110/352] Loss: 1.0039 Acc: 69.22%
Epoch [63] Batch [120/352] Loss: 1.0000 Acc: 69.41%
Epoch [63] Batch [130/352] Loss: 0.8850 Acc: 69.42%
Epoch [63] Batch [140/352] Loss: 1.1181 Acc: 69.18%
Epoch [63] Batch [150/352] Loss: 1.0541 Acc: 69.16%
Epoch [63] Batch [160/352] Loss: 0.9125 Acc: 69.20%
Epoch [63] Batch [170/352] Loss: 1.2260 Acc: 69.02%
Epoch [63] Batch [180/352] Loss: 1.0704 Acc: 68.98%
Epoch [63] Batch [190/352] Loss: 1.1053 Acc: 68.95%
Epoch [63] Batch [200/352] Loss: 1.2338 Acc: 68.81%
Epoch [63] Batch [210/352] Loss: 1.0763 Acc: 68.74%
Epoch [63] Batch [220/352] Loss: 0.9936 Acc: 68.75%
Epoch [63] Batch [230/352] Loss: 1.2499 Acc: 68.71%
Epoch [63] Batch [240/352] Loss: 0.9570 Acc: 68.83%
Epoch [63] Batch [250/352] Loss: 1.2922 Acc: 68.82%
Epoch [63] Batch [260/352] Loss: 1.1575 Acc: 68.85%
Epoch [63] Batch [270/352] Loss: 1.1299 Acc: 68.75%
Epoch [63] Batch [280/352] Loss: 1.2620 Acc: 68.71%
Epoch [63] Batch [290/352] Loss: 1.1032 Acc: 68.61%
Epoch [63] Batch [300/352] Loss: 1.1621 Acc: 68.60%
Epoch [63] Batch [310/352] Loss: 1.2963 Acc: 68.62%
Epoch [63] Batch [320/352] Loss: 0.9611 Acc: 68.64%
Epoch [63] Batch [330/352] Loss: 1.1787 Acc: 68.59%
Epoch [63] Batch [340/352] Loss: 1.1030 Acc: 68.58%
Epoch [63] Batch [350/352] Loss: 1.0650 Acc: 68.55%

======================================================================
Epoch 63/100
Train Loss: 1.0749, Train Acc: 68.53%
Val Loss: 1.5436, Val Acc: 58.20%
Time: 93.24s
======================================================================

Epoch [64] Batch [0/352] Loss: 1.0620 Acc: 66.41%
Epoch [64] Batch [10/352] Loss: 0.8220 Acc: 72.02%
Epoch [64] Batch [20/352] Loss: 1.0370 Acc: 71.95%
Epoch [64] Batch [30/352] Loss: 1.0516 Acc: 71.52%
Epoch [64] Batch [40/352] Loss: 1.1298 Acc: 70.87%
Epoch [64] Batch [50/352] Loss: 1.1247 Acc: 71.00%
Epoch [64] Batch [60/352] Loss: 0.9930 Acc: 71.03%
Epoch [64] Batch [70/352] Loss: 1.0111 Acc: 70.85%
Epoch [64] Batch [80/352] Loss: 0.9177 Acc: 70.96%
Epoch [64] Batch [90/352] Loss: 1.0051 Acc: 70.86%
Epoch [64] Batch [100/352] Loss: 0.9932 Acc: 70.61%
Epoch [64] Batch [110/352] Loss: 1.0542 Acc: 70.52%
Epoch [64] Batch [120/352] Loss: 1.1227 Acc: 70.62%
Epoch [64] Batch [130/352] Loss: 0.9338 Acc: 70.63%
Epoch [64] Batch [140/352] Loss: 1.0703 Acc: 70.43%
Epoch [64] Batch [150/352] Loss: 1.0576 Acc: 70.41%
Epoch [64] Batch [160/352] Loss: 1.1322 Acc: 70.31%
Epoch [64] Batch [170/352] Loss: 1.0093 Acc: 70.27%
Epoch [64] Batch [180/352] Loss: 1.0226 Acc: 70.24%
Epoch [64] Batch [190/352] Loss: 1.0979 Acc: 70.23%
Epoch [64] Batch [200/352] Loss: 0.9423 Acc: 70.13%
Epoch [64] Batch [210/352] Loss: 0.8285 Acc: 70.15%
Epoch [64] Batch [220/352] Loss: 0.9928 Acc: 70.12%
Epoch [64] Batch [230/352] Loss: 1.1265 Acc: 70.05%
Epoch [64] Batch [240/352] Loss: 1.2543 Acc: 69.98%
Epoch [64] Batch [250/352] Loss: 1.0236 Acc: 69.87%
Epoch [64] Batch [260/352] Loss: 1.1389 Acc: 69.72%
Epoch [64] Batch [270/352] Loss: 1.0620 Acc: 69.73%
Epoch [64] Batch [280/352] Loss: 1.2504 Acc: 69.70%
Epoch [64] Batch [290/352] Loss: 1.1784 Acc: 69.65%
Epoch [64] Batch [300/352] Loss: 1.0872 Acc: 69.60%
Epoch [64] Batch [310/352] Loss: 1.1518 Acc: 69.46%
Epoch [64] Batch [320/352] Loss: 1.0093 Acc: 69.37%
Epoch [64] Batch [330/352] Loss: 1.1652 Acc: 69.30%
Epoch [64] Batch [340/352] Loss: 1.0405 Acc: 69.26%
Epoch [64] Batch [350/352] Loss: 0.9777 Acc: 69.19%

======================================================================
Epoch 64/100
Train Loss: 1.0500, Train Acc: 69.19%
Val Loss: 1.4739, Val Acc: 59.50%
Time: 92.71s
======================================================================

Epoch [65] Batch [0/352] Loss: 0.9672 Acc: 73.44%
Epoch [65] Batch [10/352] Loss: 0.9947 Acc: 71.09%
Epoch [65] Batch [20/352] Loss: 0.8094 Acc: 71.99%
Epoch [65] Batch [30/352] Loss: 0.8647 Acc: 71.45%
Epoch [65] Batch [40/352] Loss: 0.9010 Acc: 71.25%
Epoch [65] Batch [50/352] Loss: 1.1261 Acc: 70.63%
Epoch [65] Batch [60/352] Loss: 0.9241 Acc: 70.39%
Epoch [65] Batch [70/352] Loss: 0.8808 Acc: 70.16%
Epoch [65] Batch [80/352] Loss: 1.1057 Acc: 69.98%
Epoch [65] Batch [90/352] Loss: 1.0491 Acc: 70.12%
Epoch [65] Batch [100/352] Loss: 1.0418 Acc: 70.07%
Epoch [65] Batch [110/352] Loss: 1.0416 Acc: 70.10%
Epoch [65] Batch [120/352] Loss: 0.8681 Acc: 70.25%
Epoch [65] Batch [130/352] Loss: 1.1210 Acc: 70.13%
Epoch [65] Batch [140/352] Loss: 1.1308 Acc: 70.04%
Epoch [65] Batch [150/352] Loss: 1.1135 Acc: 69.96%
Epoch [65] Batch [160/352] Loss: 1.0549 Acc: 69.78%
Epoch [65] Batch [170/352] Loss: 1.1851 Acc: 69.67%
Epoch [65] Batch [180/352] Loss: 1.0466 Acc: 69.58%
Epoch [65] Batch [190/352] Loss: 1.1324 Acc: 69.68%
Epoch [65] Batch [200/352] Loss: 1.1845 Acc: 69.75%
Epoch [65] Batch [210/352] Loss: 1.1109 Acc: 69.78%
Epoch [65] Batch [220/352] Loss: 1.0248 Acc: 69.82%
Epoch [65] Batch [230/352] Loss: 1.0766 Acc: 69.84%
Epoch [65] Batch [240/352] Loss: 1.0219 Acc: 69.81%
Epoch [65] Batch [250/352] Loss: 1.0456 Acc: 69.71%
Epoch [65] Batch [260/352] Loss: 1.0059 Acc: 69.68%
Epoch [65] Batch [270/352] Loss: 1.3162 Acc: 69.57%
Epoch [65] Batch [280/352] Loss: 0.9383 Acc: 69.54%
Epoch [65] Batch [290/352] Loss: 0.9089 Acc: 69.60%
Epoch [65] Batch [300/352] Loss: 1.3792 Acc: 69.54%
Epoch [65] Batch [310/352] Loss: 1.0487 Acc: 69.59%
Epoch [65] Batch [320/352] Loss: 1.0881 Acc: 69.59%
Epoch [65] Batch [330/352] Loss: 0.9122 Acc: 69.58%
Epoch [65] Batch [340/352] Loss: 1.0350 Acc: 69.54%
Epoch [65] Batch [350/352] Loss: 1.1121 Acc: 69.54%

======================================================================
Epoch 65/100
Train Loss: 1.0374, Train Acc: 69.54%
Val Loss: 1.4551, Val Acc: 61.54%
Time: 92.61s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 61.54%)
Epoch [66] Batch [0/352] Loss: 0.9252 Acc: 75.00%
Epoch [66] Batch [10/352] Loss: 0.9752 Acc: 70.17%
Epoch [66] Batch [20/352] Loss: 1.1345 Acc: 70.61%
Epoch [66] Batch [30/352] Loss: 0.8594 Acc: 70.87%
Epoch [66] Batch [40/352] Loss: 1.0411 Acc: 71.07%
Epoch [66] Batch [50/352] Loss: 0.7433 Acc: 71.35%
Epoch [66] Batch [60/352] Loss: 0.9534 Acc: 71.22%
Epoch [66] Batch [70/352] Loss: 0.8953 Acc: 71.27%
Epoch [66] Batch [80/352] Loss: 0.9121 Acc: 71.51%
Epoch [66] Batch [90/352] Loss: 1.0464 Acc: 71.36%
Epoch [66] Batch [100/352] Loss: 1.0163 Acc: 71.12%
Epoch [66] Batch [110/352] Loss: 1.0711 Acc: 71.07%
Epoch [66] Batch [120/352] Loss: 0.9183 Acc: 70.90%
Epoch [66] Batch [130/352] Loss: 0.9008 Acc: 70.78%
Epoch [66] Batch [140/352] Loss: 0.9205 Acc: 70.84%
Epoch [66] Batch [150/352] Loss: 0.8208 Acc: 70.76%
Epoch [66] Batch [160/352] Loss: 1.0148 Acc: 70.72%
Epoch [66] Batch [170/352] Loss: 1.0093 Acc: 70.76%
Epoch [66] Batch [180/352] Loss: 0.8675 Acc: 70.81%
Epoch [66] Batch [190/352] Loss: 1.1645 Acc: 70.71%
Epoch [66] Batch [200/352] Loss: 1.1518 Acc: 70.71%
Epoch [66] Batch [210/352] Loss: 0.8700 Acc: 70.71%
Epoch [66] Batch [220/352] Loss: 0.9180 Acc: 70.75%
Epoch [66] Batch [230/352] Loss: 0.8285 Acc: 70.74%
Epoch [66] Batch [240/352] Loss: 1.1334 Acc: 70.61%
Epoch [66] Batch [250/352] Loss: 1.2158 Acc: 70.57%
Epoch [66] Batch [260/352] Loss: 0.9662 Acc: 70.57%
Epoch [66] Batch [270/352] Loss: 0.9265 Acc: 70.54%
Epoch [66] Batch [280/352] Loss: 0.9645 Acc: 70.47%
Epoch [66] Batch [290/352] Loss: 0.9557 Acc: 70.43%
Epoch [66] Batch [300/352] Loss: 0.8918 Acc: 70.41%
Epoch [66] Batch [310/352] Loss: 1.0978 Acc: 70.41%
Epoch [66] Batch [320/352] Loss: 1.1257 Acc: 70.39%
Epoch [66] Batch [330/352] Loss: 0.9778 Acc: 70.35%
Epoch [66] Batch [340/352] Loss: 0.9298 Acc: 70.25%
Epoch [66] Batch [350/352] Loss: 1.0248 Acc: 70.25%

======================================================================
Epoch 66/100
Train Loss: 1.0144, Train Acc: 70.24%
Val Loss: 1.5053, Val Acc: 60.92%
Time: 93.17s
======================================================================

Epoch [67] Batch [0/352] Loss: 1.0466 Acc: 67.19%
Epoch [67] Batch [10/352] Loss: 0.8647 Acc: 70.60%
Epoch [67] Batch [20/352] Loss: 0.7346 Acc: 72.17%
Epoch [67] Batch [30/352] Loss: 1.0394 Acc: 72.20%
Epoch [67] Batch [40/352] Loss: 1.0232 Acc: 72.14%
Epoch [67] Batch [50/352] Loss: 0.9650 Acc: 71.92%
Epoch [67] Batch [60/352] Loss: 1.1518 Acc: 71.86%
Epoch [67] Batch [70/352] Loss: 0.9176 Acc: 71.64%
Epoch [67] Batch [80/352] Loss: 1.1450 Acc: 71.55%
Epoch [67] Batch [90/352] Loss: 0.9029 Acc: 71.60%
Epoch [67] Batch [100/352] Loss: 0.7058 Acc: 71.86%
Epoch [67] Batch [110/352] Loss: 0.7062 Acc: 71.95%
Epoch [67] Batch [120/352] Loss: 0.6895 Acc: 71.90%
Epoch [67] Batch [130/352] Loss: 0.9428 Acc: 71.79%
Epoch [67] Batch [140/352] Loss: 0.9254 Acc: 71.81%
Epoch [67] Batch [150/352] Loss: 0.9938 Acc: 71.71%
Epoch [67] Batch [160/352] Loss: 0.9004 Acc: 71.63%
Epoch [67] Batch [170/352] Loss: 1.0668 Acc: 71.58%
Epoch [67] Batch [180/352] Loss: 1.0947 Acc: 71.59%
Epoch [67] Batch [190/352] Loss: 1.0548 Acc: 71.43%
Epoch [67] Batch [200/352] Loss: 1.1324 Acc: 71.40%
Epoch [67] Batch [210/352] Loss: 1.1438 Acc: 71.39%
Epoch [67] Batch [220/352] Loss: 0.9333 Acc: 71.34%
Epoch [67] Batch [230/352] Loss: 1.1906 Acc: 71.29%
Epoch [67] Batch [240/352] Loss: 0.9958 Acc: 71.21%
Epoch [67] Batch [250/352] Loss: 0.7666 Acc: 71.15%
Epoch [67] Batch [260/352] Loss: 0.9973 Acc: 71.10%
Epoch [67] Batch [270/352] Loss: 0.8893 Acc: 71.06%
Epoch [67] Batch [280/352] Loss: 1.2854 Acc: 70.97%
Epoch [67] Batch [290/352] Loss: 1.0586 Acc: 70.97%
Epoch [67] Batch [300/352] Loss: 1.0750 Acc: 70.95%
Epoch [67] Batch [310/352] Loss: 0.9551 Acc: 70.97%
Epoch [67] Batch [320/352] Loss: 0.9089 Acc: 70.98%
Epoch [67] Batch [330/352] Loss: 1.0204 Acc: 70.89%
Epoch [67] Batch [340/352] Loss: 1.2726 Acc: 70.88%
Epoch [67] Batch [350/352] Loss: 1.2209 Acc: 70.86%

======================================================================
Epoch 67/100
Train Loss: 0.9953, Train Acc: 70.85%
Val Loss: 1.4915, Val Acc: 60.24%
Time: 92.83s
======================================================================

Epoch [68] Batch [0/352] Loss: 0.9529 Acc: 69.53%
Epoch [68] Batch [10/352] Loss: 0.8096 Acc: 71.45%
Epoch [68] Batch [20/352] Loss: 0.8850 Acc: 72.84%
Epoch [68] Batch [30/352] Loss: 1.0796 Acc: 72.78%
Epoch [68] Batch [40/352] Loss: 1.0028 Acc: 72.66%
Epoch [68] Batch [50/352] Loss: 0.8378 Acc: 72.79%
Epoch [68] Batch [60/352] Loss: 0.9102 Acc: 72.80%
Epoch [68] Batch [70/352] Loss: 0.7818 Acc: 72.50%
Epoch [68] Batch [80/352] Loss: 0.8642 Acc: 72.86%
Epoch [68] Batch [90/352] Loss: 0.9191 Acc: 72.55%
Epoch [68] Batch [100/352] Loss: 1.0814 Acc: 72.32%
Epoch [68] Batch [110/352] Loss: 0.9452 Acc: 72.39%
Epoch [68] Batch [120/352] Loss: 0.8730 Acc: 72.20%
Epoch [68] Batch [130/352] Loss: 1.1399 Acc: 71.96%
Epoch [68] Batch [140/352] Loss: 0.9909 Acc: 72.09%
Epoch [68] Batch [150/352] Loss: 0.8943 Acc: 71.88%
Epoch [68] Batch [160/352] Loss: 1.0482 Acc: 71.82%
Epoch [68] Batch [170/352] Loss: 0.9296 Acc: 71.71%
Epoch [68] Batch [180/352] Loss: 0.9353 Acc: 71.75%
Epoch [68] Batch [190/352] Loss: 1.1160 Acc: 71.64%
Epoch [68] Batch [200/352] Loss: 0.8995 Acc: 71.60%
Epoch [68] Batch [210/352] Loss: 1.0195 Acc: 71.56%
Epoch [68] Batch [220/352] Loss: 0.8364 Acc: 71.50%
Epoch [68] Batch [230/352] Loss: 0.9770 Acc: 71.56%
Epoch [68] Batch [240/352] Loss: 0.9893 Acc: 71.53%
Epoch [68] Batch [250/352] Loss: 0.8863 Acc: 71.46%
Epoch [68] Batch [260/352] Loss: 0.9886 Acc: 71.49%
Epoch [68] Batch [270/352] Loss: 1.2243 Acc: 71.42%
Epoch [68] Batch [280/352] Loss: 1.1784 Acc: 71.40%
Epoch [68] Batch [290/352] Loss: 1.0421 Acc: 71.39%
Epoch [68] Batch [300/352] Loss: 0.9924 Acc: 71.41%
Epoch [68] Batch [310/352] Loss: 1.1603 Acc: 71.37%
Epoch [68] Batch [320/352] Loss: 0.7443 Acc: 71.31%
Epoch [68] Batch [330/352] Loss: 1.0743 Acc: 71.33%
Epoch [68] Batch [340/352] Loss: 1.0368 Acc: 71.27%
Epoch [68] Batch [350/352] Loss: 1.1489 Acc: 71.20%

======================================================================
Epoch 68/100
Train Loss: 0.9785, Train Acc: 71.20%
Val Loss: 1.4149, Val Acc: 61.94%
Time: 92.90s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 61.94%)
Epoch [69] Batch [0/352] Loss: 0.7294 Acc: 76.56%
Epoch [69] Batch [10/352] Loss: 1.0030 Acc: 72.66%
Epoch [69] Batch [20/352] Loss: 0.8603 Acc: 73.51%
Epoch [69] Batch [30/352] Loss: 0.7732 Acc: 73.66%
Epoch [69] Batch [40/352] Loss: 0.9608 Acc: 73.13%
Epoch [69] Batch [50/352] Loss: 0.8974 Acc: 72.93%
Epoch [69] Batch [60/352] Loss: 0.9902 Acc: 72.96%
Epoch [69] Batch [70/352] Loss: 0.9754 Acc: 72.82%
Epoch [69] Batch [80/352] Loss: 1.0264 Acc: 72.73%
Epoch [69] Batch [90/352] Loss: 0.9826 Acc: 72.60%
Epoch [69] Batch [100/352] Loss: 1.1371 Acc: 72.47%
Epoch [69] Batch [110/352] Loss: 1.0042 Acc: 72.33%
Epoch [69] Batch [120/352] Loss: 0.8572 Acc: 72.46%
Epoch [69] Batch [130/352] Loss: 0.7829 Acc: 72.45%
Epoch [69] Batch [140/352] Loss: 1.0464 Acc: 72.27%
Epoch [69] Batch [150/352] Loss: 0.9857 Acc: 72.23%
Epoch [69] Batch [160/352] Loss: 1.1927 Acc: 72.27%
Epoch [69] Batch [170/352] Loss: 1.0922 Acc: 72.20%
Epoch [69] Batch [180/352] Loss: 0.8578 Acc: 72.15%
Epoch [69] Batch [190/352] Loss: 0.9538 Acc: 72.06%
Epoch [69] Batch [200/352] Loss: 0.8612 Acc: 72.11%
Epoch [69] Batch [210/352] Loss: 0.9799 Acc: 72.03%
Epoch [69] Batch [220/352] Loss: 1.2773 Acc: 72.04%
Epoch [69] Batch [230/352] Loss: 0.9932 Acc: 71.97%
Epoch [69] Batch [240/352] Loss: 0.9680 Acc: 71.88%
Epoch [69] Batch [250/352] Loss: 0.9009 Acc: 71.73%
Epoch [69] Batch [260/352] Loss: 1.0734 Acc: 71.65%
Epoch [69] Batch [270/352] Loss: 1.0106 Acc: 71.72%
Epoch [69] Batch [280/352] Loss: 0.7159 Acc: 71.72%
Epoch [69] Batch [290/352] Loss: 0.8148 Acc: 71.72%
Epoch [69] Batch [300/352] Loss: 0.9364 Acc: 71.69%
Epoch [69] Batch [310/352] Loss: 1.0724 Acc: 71.66%
Epoch [69] Batch [320/352] Loss: 0.9252 Acc: 71.67%
Epoch [69] Batch [330/352] Loss: 1.0062 Acc: 71.64%
Epoch [69] Batch [340/352] Loss: 0.8479 Acc: 71.61%
Epoch [69] Batch [350/352] Loss: 1.0727 Acc: 71.57%

======================================================================
Epoch 69/100
Train Loss: 0.9597, Train Acc: 71.57%
Val Loss: 1.3231, Val Acc: 64.26%
Time: 92.38s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 64.26%)
Epoch [70] Batch [0/352] Loss: 0.9199 Acc: 72.66%
Epoch [70] Batch [10/352] Loss: 1.0085 Acc: 70.81%
Epoch [70] Batch [20/352] Loss: 0.9097 Acc: 72.88%
Epoch [70] Batch [30/352] Loss: 0.7405 Acc: 72.83%
Epoch [70] Batch [40/352] Loss: 0.8985 Acc: 73.08%
Epoch [70] Batch [50/352] Loss: 0.9171 Acc: 73.15%
Epoch [70] Batch [60/352] Loss: 0.7746 Acc: 73.41%
Epoch [70] Batch [70/352] Loss: 0.8629 Acc: 73.51%
Epoch [70] Batch [80/352] Loss: 0.8184 Acc: 73.42%
Epoch [70] Batch [90/352] Loss: 0.9479 Acc: 73.59%
Epoch [70] Batch [100/352] Loss: 1.1272 Acc: 73.66%
Epoch [70] Batch [110/352] Loss: 0.9019 Acc: 73.69%
Epoch [70] Batch [120/352] Loss: 0.9963 Acc: 73.43%
Epoch [70] Batch [130/352] Loss: 0.8514 Acc: 73.41%
Epoch [70] Batch [140/352] Loss: 0.8205 Acc: 73.31%
Epoch [70] Batch [150/352] Loss: 0.9016 Acc: 73.20%
Epoch [70] Batch [160/352] Loss: 0.9273 Acc: 73.14%
Epoch [70] Batch [170/352] Loss: 0.9030 Acc: 73.07%
Epoch [70] Batch [180/352] Loss: 0.9007 Acc: 73.02%
Epoch [70] Batch [190/352] Loss: 0.8599 Acc: 72.93%
Epoch [70] Batch [200/352] Loss: 0.9410 Acc: 72.89%
Epoch [70] Batch [210/352] Loss: 1.0014 Acc: 72.82%
Epoch [70] Batch [220/352] Loss: 1.0073 Acc: 72.73%
Epoch [70] Batch [230/352] Loss: 1.0938 Acc: 72.63%
Epoch [70] Batch [240/352] Loss: 1.1129 Acc: 72.55%
Epoch [70] Batch [250/352] Loss: 0.9564 Acc: 72.44%
Epoch [70] Batch [260/352] Loss: 0.9617 Acc: 72.37%
Epoch [70] Batch [270/352] Loss: 1.0397 Acc: 72.34%
Epoch [70] Batch [280/352] Loss: 0.9531 Acc: 72.33%
Epoch [70] Batch [290/352] Loss: 1.0265 Acc: 72.22%
Epoch [70] Batch [300/352] Loss: 1.0114 Acc: 72.18%
Epoch [70] Batch [310/352] Loss: 0.8863 Acc: 72.10%
Epoch [70] Batch [320/352] Loss: 1.0606 Acc: 72.12%
Epoch [70] Batch [330/352] Loss: 0.8848 Acc: 72.09%
Epoch [70] Batch [340/352] Loss: 0.9927 Acc: 72.08%
Epoch [70] Batch [350/352] Loss: 0.8119 Acc: 72.08%

======================================================================
Epoch 70/100
Train Loss: 0.9441, Train Acc: 72.09%
Val Loss: 1.4469, Val Acc: 61.40%
Time: 94.15s
======================================================================

Epoch [71] Batch [0/352] Loss: 0.9418 Acc: 70.31%
Epoch [71] Batch [10/352] Loss: 0.9116 Acc: 74.36%
Epoch [71] Batch [20/352] Loss: 0.7380 Acc: 75.19%
Epoch [71] Batch [30/352] Loss: 1.0790 Acc: 74.19%
Epoch [71] Batch [40/352] Loss: 0.8543 Acc: 74.20%
Epoch [71] Batch [50/352] Loss: 1.0680 Acc: 74.00%
Epoch [71] Batch [60/352] Loss: 0.9143 Acc: 74.03%
Epoch [71] Batch [70/352] Loss: 0.8056 Acc: 74.15%
Epoch [71] Batch [80/352] Loss: 0.9005 Acc: 74.06%
Epoch [71] Batch [90/352] Loss: 0.8737 Acc: 74.03%
Epoch [71] Batch [100/352] Loss: 1.0370 Acc: 73.98%
Epoch [71] Batch [110/352] Loss: 1.0190 Acc: 73.94%
Epoch [71] Batch [120/352] Loss: 0.9473 Acc: 73.72%
Epoch [71] Batch [130/352] Loss: 1.0740 Acc: 73.44%
Epoch [71] Batch [140/352] Loss: 0.9702 Acc: 73.26%
Epoch [71] Batch [150/352] Loss: 0.9856 Acc: 73.19%
Epoch [71] Batch [160/352] Loss: 0.9384 Acc: 73.17%
Epoch [71] Batch [170/352] Loss: 0.9899 Acc: 73.15%
Epoch [71] Batch [180/352] Loss: 1.1249 Acc: 73.11%
Epoch [71] Batch [190/352] Loss: 1.0256 Acc: 73.08%
Epoch [71] Batch [200/352] Loss: 1.0114 Acc: 72.98%
Epoch [71] Batch [210/352] Loss: 0.7682 Acc: 72.95%
Epoch [71] Batch [220/352] Loss: 1.0173 Acc: 72.86%
Epoch [71] Batch [230/352] Loss: 0.8879 Acc: 72.87%
Epoch [71] Batch [240/352] Loss: 0.7758 Acc: 72.87%
Epoch [71] Batch [250/352] Loss: 1.1988 Acc: 72.86%
Epoch [71] Batch [260/352] Loss: 0.9946 Acc: 72.86%
Epoch [71] Batch [270/352] Loss: 0.7370 Acc: 72.76%
Epoch [71] Batch [280/352] Loss: 0.8641 Acc: 72.67%
Epoch [71] Batch [290/352] Loss: 0.7506 Acc: 72.65%
Epoch [71] Batch [300/352] Loss: 0.7996 Acc: 72.63%
Epoch [71] Batch [310/352] Loss: 0.7168 Acc: 72.62%
Epoch [71] Batch [320/352] Loss: 1.0777 Acc: 72.52%
Epoch [71] Batch [330/352] Loss: 0.6987 Acc: 72.53%
Epoch [71] Batch [340/352] Loss: 1.0258 Acc: 72.55%
Epoch [71] Batch [350/352] Loss: 0.8909 Acc: 72.51%

======================================================================
Epoch 71/100
Train Loss: 0.9283, Train Acc: 72.51%
Val Loss: 1.4058, Val Acc: 61.76%
Time: 94.34s
======================================================================

Epoch [72] Batch [0/352] Loss: 0.7912 Acc: 76.56%
Epoch [72] Batch [10/352] Loss: 0.8241 Acc: 75.28%
Epoch [72] Batch [20/352] Loss: 0.9355 Acc: 74.18%
Epoch [72] Batch [30/352] Loss: 0.7655 Acc: 73.82%
Epoch [72] Batch [40/352] Loss: 0.9293 Acc: 73.21%
Epoch [72] Batch [50/352] Loss: 0.8910 Acc: 73.73%
Epoch [72] Batch [60/352] Loss: 0.7615 Acc: 74.14%
Epoch [72] Batch [70/352] Loss: 1.1336 Acc: 74.08%
Epoch [72] Batch [80/352] Loss: 0.8046 Acc: 74.10%
Epoch [72] Batch [90/352] Loss: 1.0015 Acc: 74.02%
Epoch [72] Batch [100/352] Loss: 0.8063 Acc: 74.14%
Epoch [72] Batch [110/352] Loss: 0.7461 Acc: 73.87%
Epoch [72] Batch [120/352] Loss: 1.0900 Acc: 73.84%
Epoch [72] Batch [130/352] Loss: 0.9584 Acc: 73.71%
Epoch [72] Batch [140/352] Loss: 0.8969 Acc: 73.67%
Epoch [72] Batch [150/352] Loss: 0.9235 Acc: 73.69%
Epoch [72] Batch [160/352] Loss: 0.9886 Acc: 73.74%
Epoch [72] Batch [170/352] Loss: 1.0843 Acc: 73.73%
Epoch [72] Batch [180/352] Loss: 0.9721 Acc: 73.71%
Epoch [72] Batch [190/352] Loss: 0.8218 Acc: 73.75%
Epoch [72] Batch [200/352] Loss: 0.8456 Acc: 73.76%
Epoch [72] Batch [210/352] Loss: 1.0655 Acc: 73.72%
Epoch [72] Batch [220/352] Loss: 0.7799 Acc: 73.70%
Epoch [72] Batch [230/352] Loss: 0.9324 Acc: 73.73%
Epoch [72] Batch [240/352] Loss: 0.9642 Acc: 73.61%
Epoch [72] Batch [250/352] Loss: 0.7671 Acc: 73.66%
Epoch [72] Batch [260/352] Loss: 0.9677 Acc: 73.58%
Epoch [72] Batch [270/352] Loss: 0.8995 Acc: 73.54%
Epoch [72] Batch [280/352] Loss: 1.0295 Acc: 73.46%
Epoch [72] Batch [290/352] Loss: 0.8882 Acc: 73.39%
Epoch [72] Batch [300/352] Loss: 1.0043 Acc: 73.31%
Epoch [72] Batch [310/352] Loss: 0.9495 Acc: 73.29%
Epoch [72] Batch [320/352] Loss: 1.0716 Acc: 73.27%
Epoch [72] Batch [330/352] Loss: 1.0362 Acc: 73.22%
Epoch [72] Batch [340/352] Loss: 0.9953 Acc: 73.20%
Epoch [72] Batch [350/352] Loss: 0.9152 Acc: 73.17%

======================================================================
Epoch 72/100
Train Loss: 0.9072, Train Acc: 73.16%
Val Loss: 1.3477, Val Acc: 63.90%
Time: 93.97s
======================================================================

Epoch [73] Batch [0/352] Loss: 0.7407 Acc: 77.34%
Epoch [73] Batch [10/352] Loss: 0.8433 Acc: 74.50%
Epoch [73] Batch [20/352] Loss: 0.9507 Acc: 73.70%
Epoch [73] Batch [30/352] Loss: 0.7478 Acc: 74.42%
Epoch [73] Batch [40/352] Loss: 1.0711 Acc: 74.26%
Epoch [73] Batch [50/352] Loss: 0.7640 Acc: 74.60%
Epoch [73] Batch [60/352] Loss: 0.6421 Acc: 75.04%
Epoch [73] Batch [70/352] Loss: 0.9588 Acc: 74.81%
Epoch [73] Batch [80/352] Loss: 1.0204 Acc: 74.84%
Epoch [73] Batch [90/352] Loss: 0.8939 Acc: 74.69%
Epoch [73] Batch [100/352] Loss: 0.8069 Acc: 74.84%
Epoch [73] Batch [110/352] Loss: 0.8988 Acc: 74.90%
Epoch [73] Batch [120/352] Loss: 0.8923 Acc: 74.83%
Epoch [73] Batch [130/352] Loss: 0.8550 Acc: 74.64%
Epoch [73] Batch [140/352] Loss: 0.8608 Acc: 74.54%
Epoch [73] Batch [150/352] Loss: 0.7057 Acc: 74.52%
Epoch [73] Batch [160/352] Loss: 1.1210 Acc: 74.44%
Epoch [73] Batch [170/352] Loss: 0.7213 Acc: 74.52%
Epoch [73] Batch [180/352] Loss: 0.8265 Acc: 74.43%
Epoch [73] Batch [190/352] Loss: 1.0168 Acc: 74.38%
Epoch [73] Batch [200/352] Loss: 0.9107 Acc: 74.41%
Epoch [73] Batch [210/352] Loss: 0.8565 Acc: 74.39%
Epoch [73] Batch [220/352] Loss: 0.6261 Acc: 74.41%
Epoch [73] Batch [230/352] Loss: 0.9251 Acc: 74.32%
Epoch [73] Batch [240/352] Loss: 0.9008 Acc: 74.26%
Epoch [73] Batch [250/352] Loss: 1.1179 Acc: 74.16%
Epoch [73] Batch [260/352] Loss: 1.0935 Acc: 74.11%
Epoch [73] Batch [270/352] Loss: 0.9706 Acc: 74.03%
Epoch [73] Batch [280/352] Loss: 1.0730 Acc: 73.95%
Epoch [73] Batch [290/352] Loss: 1.0929 Acc: 73.92%
Epoch [73] Batch [300/352] Loss: 0.8566 Acc: 73.90%
Epoch [73] Batch [310/352] Loss: 0.9176 Acc: 73.90%
Epoch [73] Batch [320/352] Loss: 0.9933 Acc: 73.80%
Epoch [73] Batch [330/352] Loss: 1.0440 Acc: 73.80%
Epoch [73] Batch [340/352] Loss: 0.8330 Acc: 73.76%
Epoch [73] Batch [350/352] Loss: 0.8579 Acc: 73.74%

======================================================================
Epoch 73/100
Train Loss: 0.8791, Train Acc: 73.73%
Val Loss: 1.3656, Val Acc: 63.12%
Time: 94.29s
======================================================================

Epoch [74] Batch [0/352] Loss: 0.7839 Acc: 75.00%
Epoch [74] Batch [10/352] Loss: 0.8747 Acc: 75.21%
Epoch [74] Batch [20/352] Loss: 0.8943 Acc: 75.19%
Epoch [74] Batch [30/352] Loss: 0.7326 Acc: 75.66%
Epoch [74] Batch [40/352] Loss: 0.6474 Acc: 76.03%
Epoch [74] Batch [50/352] Loss: 0.7739 Acc: 75.60%
Epoch [74] Batch [60/352] Loss: 0.8553 Acc: 75.87%
Epoch [74] Batch [70/352] Loss: 0.9261 Acc: 75.67%
Epoch [74] Batch [80/352] Loss: 0.7403 Acc: 75.63%
Epoch [74] Batch [90/352] Loss: 0.8527 Acc: 75.67%
Epoch [74] Batch [100/352] Loss: 0.8644 Acc: 75.55%
Epoch [74] Batch [110/352] Loss: 0.8508 Acc: 75.51%
Epoch [74] Batch [120/352] Loss: 0.8072 Acc: 75.56%
Epoch [74] Batch [130/352] Loss: 1.2052 Acc: 75.38%
Epoch [74] Batch [140/352] Loss: 0.9354 Acc: 75.34%
Epoch [74] Batch [150/352] Loss: 0.9519 Acc: 75.19%
Epoch [74] Batch [160/352] Loss: 0.8913 Acc: 75.08%
Epoch [74] Batch [170/352] Loss: 1.0960 Acc: 75.02%
Epoch [74] Batch [180/352] Loss: 1.0036 Acc: 75.03%
Epoch [74] Batch [190/352] Loss: 0.9364 Acc: 75.04%
Epoch [74] Batch [200/352] Loss: 0.8855 Acc: 74.99%
Epoch [74] Batch [210/352] Loss: 0.8025 Acc: 75.02%
Epoch [74] Batch [220/352] Loss: 0.6997 Acc: 75.04%
Epoch [74] Batch [230/352] Loss: 1.0576 Acc: 74.99%
Epoch [74] Batch [240/352] Loss: 0.9892 Acc: 74.96%
Epoch [74] Batch [250/352] Loss: 0.8236 Acc: 74.93%
Epoch [74] Batch [260/352] Loss: 0.7590 Acc: 74.82%
Epoch [74] Batch [270/352] Loss: 0.8271 Acc: 74.86%
Epoch [74] Batch [280/352] Loss: 1.0328 Acc: 74.84%
Epoch [74] Batch [290/352] Loss: 0.7636 Acc: 74.81%
Epoch [74] Batch [300/352] Loss: 0.8691 Acc: 74.82%
Epoch [74] Batch [310/352] Loss: 1.0382 Acc: 74.72%
Epoch [74] Batch [320/352] Loss: 0.8405 Acc: 74.73%
Epoch [74] Batch [330/352] Loss: 0.8093 Acc: 74.71%
Epoch [74] Batch [340/352] Loss: 0.7408 Acc: 74.62%
Epoch [74] Batch [350/352] Loss: 0.8231 Acc: 74.56%

======================================================================
Epoch 74/100
Train Loss: 0.8612, Train Acc: 74.56%
Val Loss: 1.2731, Val Acc: 65.66%
Time: 94.23s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 65.66%)
Epoch [75] Batch [0/352] Loss: 0.7755 Acc: 77.34%
Epoch [75] Batch [10/352] Loss: 0.7905 Acc: 75.36%
Epoch [75] Batch [20/352] Loss: 0.6677 Acc: 76.60%
Epoch [75] Batch [30/352] Loss: 0.8605 Acc: 76.59%
Epoch [75] Batch [40/352] Loss: 0.6887 Acc: 76.31%
Epoch [75] Batch [50/352] Loss: 0.6074 Acc: 76.26%
Epoch [75] Batch [60/352] Loss: 0.8092 Acc: 76.22%
Epoch [75] Batch [70/352] Loss: 0.9071 Acc: 75.95%
Epoch [75] Batch [80/352] Loss: 0.8830 Acc: 75.80%
Epoch [75] Batch [90/352] Loss: 0.8084 Acc: 75.94%
Epoch [75] Batch [100/352] Loss: 0.8967 Acc: 75.93%
Epoch [75] Batch [110/352] Loss: 0.9107 Acc: 75.93%
Epoch [75] Batch [120/352] Loss: 0.8399 Acc: 75.96%
Epoch [75] Batch [130/352] Loss: 0.7148 Acc: 75.78%
Epoch [75] Batch [140/352] Loss: 0.7194 Acc: 75.72%
Epoch [75] Batch [150/352] Loss: 0.9560 Acc: 75.61%
Epoch [75] Batch [160/352] Loss: 0.7893 Acc: 75.67%
Epoch [75] Batch [170/352] Loss: 0.8632 Acc: 75.67%
Epoch [75] Batch [180/352] Loss: 0.7925 Acc: 75.53%
Epoch [75] Batch [190/352] Loss: 0.8692 Acc: 75.51%
Epoch [75] Batch [200/352] Loss: 1.0575 Acc: 75.40%
Epoch [75] Batch [210/352] Loss: 0.9319 Acc: 75.32%
Epoch [75] Batch [220/352] Loss: 0.8882 Acc: 75.23%
Epoch [75] Batch [230/352] Loss: 0.8847 Acc: 75.23%
Epoch [75] Batch [240/352] Loss: 0.8717 Acc: 75.20%
Epoch [75] Batch [250/352] Loss: 0.8276 Acc: 75.15%
Epoch [75] Batch [260/352] Loss: 0.8223 Acc: 75.20%
Epoch [75] Batch [270/352] Loss: 0.8843 Acc: 75.09%
Epoch [75] Batch [280/352] Loss: 0.7783 Acc: 75.04%
Epoch [75] Batch [290/352] Loss: 0.9358 Acc: 74.96%
Epoch [75] Batch [300/352] Loss: 0.8310 Acc: 74.89%
Epoch [75] Batch [310/352] Loss: 0.7579 Acc: 74.88%
Epoch [75] Batch [320/352] Loss: 0.8142 Acc: 74.83%
Epoch [75] Batch [330/352] Loss: 0.7777 Acc: 74.82%
Epoch [75] Batch [340/352] Loss: 0.9746 Acc: 74.83%
Epoch [75] Batch [350/352] Loss: 0.8457 Acc: 74.77%

======================================================================
Epoch 75/100
Train Loss: 0.8511, Train Acc: 74.77%
Val Loss: 1.3481, Val Acc: 64.20%
Time: 93.73s
======================================================================

Epoch [76] Batch [0/352] Loss: 0.6869 Acc: 79.69%
Epoch [76] Batch [10/352] Loss: 0.7349 Acc: 78.55%
Epoch [76] Batch [20/352] Loss: 0.9076 Acc: 77.83%
Epoch [76] Batch [30/352] Loss: 0.7403 Acc: 76.97%
Epoch [76] Batch [40/352] Loss: 0.6354 Acc: 77.17%
Epoch [76] Batch [50/352] Loss: 0.7087 Acc: 77.07%
Epoch [76] Batch [60/352] Loss: 0.8354 Acc: 76.82%
Epoch [76] Batch [70/352] Loss: 0.8371 Acc: 76.82%
Epoch [76] Batch [80/352] Loss: 0.7013 Acc: 76.64%
Epoch [76] Batch [90/352] Loss: 0.6347 Acc: 76.56%
Epoch [76] Batch [100/352] Loss: 0.7277 Acc: 76.38%
Epoch [76] Batch [110/352] Loss: 0.9688 Acc: 76.30%
Epoch [76] Batch [120/352] Loss: 0.9940 Acc: 76.41%
Epoch [76] Batch [130/352] Loss: 0.6964 Acc: 76.42%
Epoch [76] Batch [140/352] Loss: 0.9040 Acc: 76.42%
Epoch [76] Batch [150/352] Loss: 0.6795 Acc: 76.46%
Epoch [76] Batch [160/352] Loss: 1.0911 Acc: 76.36%
Epoch [76] Batch [170/352] Loss: 0.7179 Acc: 76.42%
Epoch [76] Batch [180/352] Loss: 0.8474 Acc: 76.38%
Epoch [76] Batch [190/352] Loss: 0.6880 Acc: 76.30%
Epoch [76] Batch [200/352] Loss: 0.9932 Acc: 76.19%
Epoch [76] Batch [210/352] Loss: 0.7858 Acc: 76.21%
Epoch [76] Batch [220/352] Loss: 0.9698 Acc: 76.12%
Epoch [76] Batch [230/352] Loss: 0.8589 Acc: 76.25%
Epoch [76] Batch [240/352] Loss: 0.9814 Acc: 76.15%
Epoch [76] Batch [250/352] Loss: 0.7713 Acc: 76.08%
Epoch [76] Batch [260/352] Loss: 0.9318 Acc: 76.06%
Epoch [76] Batch [270/352] Loss: 0.7382 Acc: 75.94%
Epoch [76] Batch [280/352] Loss: 0.8287 Acc: 75.90%
Epoch [76] Batch [290/352] Loss: 0.7753 Acc: 75.87%
Epoch [76] Batch [300/352] Loss: 0.6269 Acc: 75.87%
Epoch [76] Batch [310/352] Loss: 0.8979 Acc: 75.79%
Epoch [76] Batch [320/352] Loss: 1.0565 Acc: 75.72%
Epoch [76] Batch [330/352] Loss: 0.9271 Acc: 75.64%
Epoch [76] Batch [340/352] Loss: 1.0034 Acc: 75.60%
Epoch [76] Batch [350/352] Loss: 0.7621 Acc: 75.58%

======================================================================
Epoch 76/100
Train Loss: 0.8280, Train Acc: 75.58%
Val Loss: 1.3041, Val Acc: 64.18%
Time: 92.86s
======================================================================

Epoch [77] Batch [0/352] Loss: 0.7982 Acc: 76.56%
Epoch [77] Batch [10/352] Loss: 0.8461 Acc: 76.49%
Epoch [77] Batch [20/352] Loss: 0.8604 Acc: 76.75%
Epoch [77] Batch [30/352] Loss: 0.7938 Acc: 76.89%
Epoch [77] Batch [40/352] Loss: 0.6942 Acc: 77.23%
Epoch [77] Batch [50/352] Loss: 0.7584 Acc: 77.01%
Epoch [77] Batch [60/352] Loss: 0.8566 Acc: 76.87%
Epoch [77] Batch [70/352] Loss: 0.6782 Acc: 77.13%
Epoch [77] Batch [80/352] Loss: 0.7146 Acc: 77.04%
Epoch [77] Batch [90/352] Loss: 0.7691 Acc: 77.16%
Epoch [77] Batch [100/352] Loss: 0.6374 Acc: 77.19%
Epoch [77] Batch [110/352] Loss: 0.6599 Acc: 77.29%
Epoch [77] Batch [120/352] Loss: 0.8455 Acc: 77.35%
Epoch [77] Batch [130/352] Loss: 0.9672 Acc: 77.20%
Epoch [77] Batch [140/352] Loss: 0.8036 Acc: 77.22%
Epoch [77] Batch [150/352] Loss: 0.8669 Acc: 77.22%
Epoch [77] Batch [160/352] Loss: 0.6567 Acc: 77.27%
Epoch [77] Batch [170/352] Loss: 0.8600 Acc: 77.25%
Epoch [77] Batch [180/352] Loss: 0.8257 Acc: 77.18%
Epoch [77] Batch [190/352] Loss: 0.8065 Acc: 77.13%
Epoch [77] Batch [200/352] Loss: 0.7427 Acc: 77.07%
Epoch [77] Batch [210/352] Loss: 0.8721 Acc: 76.97%
Epoch [77] Batch [220/352] Loss: 0.9383 Acc: 76.84%
Epoch [77] Batch [230/352] Loss: 0.8097 Acc: 76.83%
Epoch [77] Batch [240/352] Loss: 0.7467 Acc: 76.81%
Epoch [77] Batch [250/352] Loss: 0.6401 Acc: 76.69%
Epoch [77] Batch [260/352] Loss: 0.8836 Acc: 76.65%
Epoch [77] Batch [270/352] Loss: 0.7472 Acc: 76.65%
Epoch [77] Batch [280/352] Loss: 0.8167 Acc: 76.63%
Epoch [77] Batch [290/352] Loss: 0.7114 Acc: 76.59%
Epoch [77] Batch [300/352] Loss: 0.9708 Acc: 76.48%
Epoch [77] Batch [310/352] Loss: 0.7761 Acc: 76.38%
Epoch [77] Batch [320/352] Loss: 1.0488 Acc: 76.27%
Epoch [77] Batch [330/352] Loss: 0.6896 Acc: 76.24%
Epoch [77] Batch [340/352] Loss: 0.9795 Acc: 76.21%
Epoch [77] Batch [350/352] Loss: 0.7157 Acc: 76.23%

======================================================================
Epoch 77/100
Train Loss: 0.7987, Train Acc: 76.23%
Val Loss: 1.2786, Val Acc: 65.02%
Time: 93.12s
======================================================================

Epoch [78] Batch [0/352] Loss: 0.7226 Acc: 82.81%
Epoch [78] Batch [10/352] Loss: 0.7550 Acc: 79.12%
Epoch [78] Batch [20/352] Loss: 0.7225 Acc: 78.42%
Epoch [78] Batch [30/352] Loss: 0.7236 Acc: 77.67%
Epoch [78] Batch [40/352] Loss: 0.7686 Acc: 77.42%
Epoch [78] Batch [50/352] Loss: 0.7631 Acc: 77.57%
Epoch [78] Batch [60/352] Loss: 0.8238 Acc: 77.69%
Epoch [78] Batch [70/352] Loss: 0.6718 Acc: 77.45%
Epoch [78] Batch [80/352] Loss: 0.8003 Acc: 77.48%
Epoch [78] Batch [90/352] Loss: 0.6776 Acc: 77.45%
Epoch [78] Batch [100/352] Loss: 0.6668 Acc: 77.50%
Epoch [78] Batch [110/352] Loss: 0.4503 Acc: 77.66%
Epoch [78] Batch [120/352] Loss: 0.8453 Acc: 77.41%
Epoch [78] Batch [130/352] Loss: 0.8105 Acc: 77.26%
Epoch [78] Batch [140/352] Loss: 0.6907 Acc: 77.18%
Epoch [78] Batch [150/352] Loss: 0.8920 Acc: 77.29%
Epoch [78] Batch [160/352] Loss: 0.6733 Acc: 77.32%
Epoch [78] Batch [170/352] Loss: 0.6687 Acc: 77.53%
Epoch [78] Batch [180/352] Loss: 0.7889 Acc: 77.44%
Epoch [78] Batch [190/352] Loss: 0.7129 Acc: 77.49%
Epoch [78] Batch [200/352] Loss: 0.7124 Acc: 77.44%
Epoch [78] Batch [210/352] Loss: 0.8905 Acc: 77.38%
Epoch [78] Batch [220/352] Loss: 0.8052 Acc: 77.33%
Epoch [78] Batch [230/352] Loss: 0.9085 Acc: 77.28%
Epoch [78] Batch [240/352] Loss: 0.7906 Acc: 77.16%
Epoch [78] Batch [250/352] Loss: 0.8192 Acc: 77.15%
Epoch [78] Batch [260/352] Loss: 0.7537 Acc: 77.07%
Epoch [78] Batch [270/352] Loss: 1.0570 Acc: 77.01%
Epoch [78] Batch [280/352] Loss: 0.7692 Acc: 76.95%
Epoch [78] Batch [290/352] Loss: 0.7942 Acc: 76.99%
Epoch [78] Batch [300/352] Loss: 0.8058 Acc: 77.05%
Epoch [78] Batch [310/352] Loss: 0.7994 Acc: 77.07%
Epoch [78] Batch [320/352] Loss: 0.8300 Acc: 77.07%
Epoch [78] Batch [330/352] Loss: 0.8459 Acc: 77.09%
Epoch [78] Batch [340/352] Loss: 0.6125 Acc: 77.04%
Epoch [78] Batch [350/352] Loss: 0.7033 Acc: 76.97%

======================================================================
Epoch 78/100
Train Loss: 0.7801, Train Acc: 76.97%
Val Loss: 1.3319, Val Acc: 64.82%
Time: 92.44s
======================================================================

Epoch [79] Batch [0/352] Loss: 0.6621 Acc: 81.25%
Epoch [79] Batch [10/352] Loss: 0.5633 Acc: 78.41%
Epoch [79] Batch [20/352] Loss: 0.7230 Acc: 78.27%
Epoch [79] Batch [30/352] Loss: 0.6786 Acc: 78.73%
Epoch [79] Batch [40/352] Loss: 0.6912 Acc: 78.72%
Epoch [79] Batch [50/352] Loss: 0.6644 Acc: 78.86%
Epoch [79] Batch [60/352] Loss: 0.5855 Acc: 78.64%
Epoch [79] Batch [70/352] Loss: 0.7095 Acc: 78.82%
Epoch [79] Batch [80/352] Loss: 0.6642 Acc: 78.64%
Epoch [79] Batch [90/352] Loss: 0.6595 Acc: 78.70%
Epoch [79] Batch [100/352] Loss: 0.5231 Acc: 78.66%
Epoch [79] Batch [110/352] Loss: 0.8443 Acc: 78.46%
Epoch [79] Batch [120/352] Loss: 0.7774 Acc: 78.29%
Epoch [79] Batch [130/352] Loss: 0.6403 Acc: 78.33%
Epoch [79] Batch [140/352] Loss: 0.5882 Acc: 78.25%
Epoch [79] Batch [150/352] Loss: 0.7899 Acc: 78.05%
Epoch [79] Batch [160/352] Loss: 0.6428 Acc: 78.10%
Epoch [79] Batch [170/352] Loss: 0.7649 Acc: 78.08%
Epoch [79] Batch [180/352] Loss: 0.9191 Acc: 78.04%
Epoch [79] Batch [190/352] Loss: 0.7436 Acc: 77.95%
Epoch [79] Batch [200/352] Loss: 0.6756 Acc: 78.00%
Epoch [79] Batch [210/352] Loss: 0.8668 Acc: 77.94%
Epoch [79] Batch [220/352] Loss: 0.7522 Acc: 77.87%
Epoch [79] Batch [230/352] Loss: 0.7224 Acc: 77.87%
Epoch [79] Batch [240/352] Loss: 0.5877 Acc: 77.86%
Epoch [79] Batch [250/352] Loss: 0.8043 Acc: 77.81%
Epoch [79] Batch [260/352] Loss: 0.8585 Acc: 77.69%
Epoch [79] Batch [270/352] Loss: 0.8603 Acc: 77.60%
Epoch [79] Batch [280/352] Loss: 0.6469 Acc: 77.60%
Epoch [79] Batch [290/352] Loss: 0.7837 Acc: 77.56%
Epoch [79] Batch [300/352] Loss: 0.6568 Acc: 77.52%
Epoch [79] Batch [310/352] Loss: 0.7304 Acc: 77.44%
Epoch [79] Batch [320/352] Loss: 0.7198 Acc: 77.38%
Epoch [79] Batch [330/352] Loss: 0.8197 Acc: 77.34%
Epoch [79] Batch [340/352] Loss: 0.7098 Acc: 77.29%
Epoch [79] Batch [350/352] Loss: 0.6022 Acc: 77.28%

======================================================================
Epoch 79/100
Train Loss: 0.7632, Train Acc: 77.29%
Val Loss: 1.2684, Val Acc: 65.94%
Time: 92.46s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 65.94%)
Epoch [80] Batch [0/352] Loss: 0.6226 Acc: 82.81%
Epoch [80] Batch [10/352] Loss: 0.7366 Acc: 78.05%
Epoch [80] Batch [20/352] Loss: 0.7004 Acc: 78.94%
Epoch [80] Batch [30/352] Loss: 0.4634 Acc: 79.49%
Epoch [80] Batch [40/352] Loss: 0.7901 Acc: 79.04%
Epoch [80] Batch [50/352] Loss: 0.6731 Acc: 79.01%
Epoch [80] Batch [60/352] Loss: 0.6396 Acc: 79.00%
Epoch [80] Batch [70/352] Loss: 0.6805 Acc: 78.91%
Epoch [80] Batch [80/352] Loss: 0.5960 Acc: 78.74%
Epoch [80] Batch [90/352] Loss: 0.8161 Acc: 78.82%
Epoch [80] Batch [100/352] Loss: 0.6375 Acc: 78.86%
Epoch [80] Batch [110/352] Loss: 0.8062 Acc: 78.70%
Epoch [80] Batch [120/352] Loss: 0.9225 Acc: 78.64%
Epoch [80] Batch [130/352] Loss: 0.7095 Acc: 78.75%
Epoch [80] Batch [140/352] Loss: 0.5933 Acc: 78.80%
Epoch [80] Batch [150/352] Loss: 0.8236 Acc: 78.72%
Epoch [80] Batch [160/352] Loss: 0.6887 Acc: 78.77%
Epoch [80] Batch [170/352] Loss: 0.7892 Acc: 78.67%
Epoch [80] Batch [180/352] Loss: 0.8417 Acc: 78.63%
Epoch [80] Batch [190/352] Loss: 0.6554 Acc: 78.54%
Epoch [80] Batch [200/352] Loss: 0.6967 Acc: 78.47%
Epoch [80] Batch [210/352] Loss: 0.7964 Acc: 78.42%
Epoch [80] Batch [220/352] Loss: 0.9861 Acc: 78.28%
Epoch [80] Batch [230/352] Loss: 0.6406 Acc: 78.31%
Epoch [80] Batch [240/352] Loss: 0.7747 Acc: 78.27%
Epoch [80] Batch [250/352] Loss: 0.6146 Acc: 78.26%
Epoch [80] Batch [260/352] Loss: 0.7212 Acc: 78.26%
Epoch [80] Batch [270/352] Loss: 0.6397 Acc: 78.27%
Epoch [80] Batch [280/352] Loss: 0.6231 Acc: 78.26%
Epoch [80] Batch [290/352] Loss: 0.7480 Acc: 78.26%
Epoch [80] Batch [300/352] Loss: 0.8803 Acc: 78.17%
Epoch [80] Batch [310/352] Loss: 0.7010 Acc: 78.12%
Epoch [80] Batch [320/352] Loss: 0.7651 Acc: 78.10%
Epoch [80] Batch [330/352] Loss: 0.8569 Acc: 78.04%
Epoch [80] Batch [340/352] Loss: 0.8827 Acc: 78.01%
Epoch [80] Batch [350/352] Loss: 0.6805 Acc: 78.00%

======================================================================
Epoch 80/100
Train Loss: 0.7398, Train Acc: 78.00%
Val Loss: 1.2676, Val Acc: 66.26%
Time: 92.99s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 66.26%)
Epoch [81] Batch [0/352] Loss: 0.7856 Acc: 77.34%
Epoch [81] Batch [10/352] Loss: 0.7448 Acc: 78.84%
Epoch [81] Batch [20/352] Loss: 0.7558 Acc: 78.76%
Epoch [81] Batch [30/352] Loss: 0.7076 Acc: 79.59%
Epoch [81] Batch [40/352] Loss: 0.5989 Acc: 79.48%
Epoch [81] Batch [50/352] Loss: 0.7656 Acc: 79.78%
Epoch [81] Batch [60/352] Loss: 0.8031 Acc: 79.64%
Epoch [81] Batch [70/352] Loss: 0.7355 Acc: 79.43%
Epoch [81] Batch [80/352] Loss: 0.6383 Acc: 79.53%
Epoch [81] Batch [90/352] Loss: 0.6910 Acc: 79.61%
Epoch [81] Batch [100/352] Loss: 0.5862 Acc: 79.80%
Epoch [81] Batch [110/352] Loss: 0.8127 Acc: 79.85%
Epoch [81] Batch [120/352] Loss: 0.5991 Acc: 80.02%
Epoch [81] Batch [130/352] Loss: 0.7177 Acc: 79.91%
Epoch [81] Batch [140/352] Loss: 0.6940 Acc: 79.90%
Epoch [81] Batch [150/352] Loss: 0.5970 Acc: 79.86%
Epoch [81] Batch [160/352] Loss: 0.7117 Acc: 79.76%
Epoch [81] Batch [170/352] Loss: 0.8212 Acc: 79.65%
Epoch [81] Batch [180/352] Loss: 0.8148 Acc: 79.57%
Epoch [81] Batch [190/352] Loss: 0.6892 Acc: 79.57%
Epoch [81] Batch [200/352] Loss: 0.6753 Acc: 79.56%
Epoch [81] Batch [210/352] Loss: 0.8914 Acc: 79.47%
Epoch [81] Batch [220/352] Loss: 0.7923 Acc: 79.44%
Epoch [81] Batch [230/352] Loss: 0.7394 Acc: 79.41%
Epoch [81] Batch [240/352] Loss: 0.7172 Acc: 79.34%
Epoch [81] Batch [250/352] Loss: 0.8199 Acc: 79.30%
Epoch [81] Batch [260/352] Loss: 0.7817 Acc: 79.25%
Epoch [81] Batch [270/352] Loss: 0.7113 Acc: 79.18%
Epoch [81] Batch [280/352] Loss: 0.7332 Acc: 79.16%
Epoch [81] Batch [290/352] Loss: 0.6995 Acc: 79.09%
Epoch [81] Batch [300/352] Loss: 0.9578 Acc: 79.06%
Epoch [81] Batch [310/352] Loss: 0.8569 Acc: 79.08%
Epoch [81] Batch [320/352] Loss: 0.6448 Acc: 79.03%
Epoch [81] Batch [330/352] Loss: 0.7986 Acc: 78.98%
Epoch [81] Batch [340/352] Loss: 0.7413 Acc: 78.99%
Epoch [81] Batch [350/352] Loss: 0.6821 Acc: 79.00%

======================================================================
Epoch 81/100
Train Loss: 0.7101, Train Acc: 78.99%
Val Loss: 1.2416, Val Acc: 66.64%
Time: 92.38s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 66.64%)
Epoch [82] Batch [0/352] Loss: 0.6810 Acc: 80.47%
Epoch [82] Batch [10/352] Loss: 0.6809 Acc: 81.82%
Epoch [82] Batch [20/352] Loss: 0.5922 Acc: 80.43%
Epoch [82] Batch [30/352] Loss: 0.7393 Acc: 80.19%
Epoch [82] Batch [40/352] Loss: 0.5740 Acc: 80.47%
Epoch [82] Batch [50/352] Loss: 0.6741 Acc: 80.67%
Epoch [82] Batch [60/352] Loss: 0.6349 Acc: 80.94%
Epoch [82] Batch [70/352] Loss: 0.9029 Acc: 80.80%
Epoch [82] Batch [80/352] Loss: 0.6748 Acc: 80.72%
Epoch [82] Batch [90/352] Loss: 0.6528 Acc: 80.86%
Epoch [82] Batch [100/352] Loss: 0.5342 Acc: 80.75%
Epoch [82] Batch [110/352] Loss: 0.6300 Acc: 80.81%
Epoch [82] Batch [120/352] Loss: 0.5945 Acc: 80.86%
Epoch [82] Batch [130/352] Loss: 0.6299 Acc: 80.91%
Epoch [82] Batch [140/352] Loss: 0.8354 Acc: 80.86%
Epoch [82] Batch [150/352] Loss: 0.6426 Acc: 80.89%
Epoch [82] Batch [160/352] Loss: 0.6709 Acc: 80.82%
Epoch [82] Batch [170/352] Loss: 0.7396 Acc: 80.59%
Epoch [82] Batch [180/352] Loss: 0.6474 Acc: 80.48%
Epoch [82] Batch [190/352] Loss: 0.8161 Acc: 80.45%
Epoch [82] Batch [200/352] Loss: 0.7665 Acc: 80.49%
Epoch [82] Batch [210/352] Loss: 0.6540 Acc: 80.42%
Epoch [82] Batch [220/352] Loss: 0.7502 Acc: 80.37%
Epoch [82] Batch [230/352] Loss: 0.5723 Acc: 80.34%
Epoch [82] Batch [240/352] Loss: 0.6278 Acc: 80.29%
Epoch [82] Batch [250/352] Loss: 0.6489 Acc: 80.24%
Epoch [82] Batch [260/352] Loss: 0.9160 Acc: 80.16%
Epoch [82] Batch [270/352] Loss: 0.6749 Acc: 80.09%
Epoch [82] Batch [280/352] Loss: 0.8526 Acc: 79.98%
Epoch [82] Batch [290/352] Loss: 0.6592 Acc: 79.93%
Epoch [82] Batch [300/352] Loss: 0.6782 Acc: 79.93%
Epoch [82] Batch [310/352] Loss: 0.7215 Acc: 79.88%
Epoch [82] Batch [320/352] Loss: 0.5343 Acc: 79.86%
Epoch [82] Batch [330/352] Loss: 0.6233 Acc: 79.85%
Epoch [82] Batch [340/352] Loss: 0.7709 Acc: 79.76%
Epoch [82] Batch [350/352] Loss: 0.4897 Acc: 79.71%

======================================================================
Epoch 82/100
Train Loss: 0.6922, Train Acc: 79.70%
Val Loss: 1.2492, Val Acc: 66.00%
Time: 93.28s
======================================================================

Epoch [83] Batch [0/352] Loss: 0.8259 Acc: 75.78%
Epoch [83] Batch [10/352] Loss: 0.6142 Acc: 81.11%
Epoch [83] Batch [20/352] Loss: 0.8436 Acc: 80.17%
Epoch [83] Batch [30/352] Loss: 0.5792 Acc: 80.09%
Epoch [83] Batch [40/352] Loss: 0.7012 Acc: 80.26%
Epoch [83] Batch [50/352] Loss: 0.5951 Acc: 80.44%
Epoch [83] Batch [60/352] Loss: 0.6990 Acc: 80.38%
Epoch [83] Batch [70/352] Loss: 0.6390 Acc: 80.46%
Epoch [83] Batch [80/352] Loss: 0.6391 Acc: 80.43%
Epoch [83] Batch [90/352] Loss: 0.7036 Acc: 80.47%
Epoch [83] Batch [100/352] Loss: 0.7377 Acc: 80.36%
Epoch [83] Batch [110/352] Loss: 0.5584 Acc: 80.43%
Epoch [83] Batch [120/352] Loss: 0.5721 Acc: 80.51%
Epoch [83] Batch [130/352] Loss: 0.5845 Acc: 80.61%
Epoch [83] Batch [140/352] Loss: 0.7737 Acc: 80.54%
Epoch [83] Batch [150/352] Loss: 0.5216 Acc: 80.48%
Epoch [83] Batch [160/352] Loss: 0.6655 Acc: 80.43%
Epoch [83] Batch [170/352] Loss: 0.7615 Acc: 80.45%
Epoch [83] Batch [180/352] Loss: 0.7970 Acc: 80.38%
Epoch [83] Batch [190/352] Loss: 0.7727 Acc: 80.38%
Epoch [83] Batch [200/352] Loss: 0.5115 Acc: 80.26%
Epoch [83] Batch [210/352] Loss: 0.7776 Acc: 80.18%
Epoch [83] Batch [220/352] Loss: 0.6816 Acc: 80.18%
Epoch [83] Batch [230/352] Loss: 0.6429 Acc: 80.23%
Epoch [83] Batch [240/352] Loss: 0.6236 Acc: 80.22%
Epoch [83] Batch [250/352] Loss: 0.8562 Acc: 80.18%
Epoch [83] Batch [260/352] Loss: 0.7660 Acc: 80.20%
Epoch [83] Batch [270/352] Loss: 0.6461 Acc: 80.24%
Epoch [83] Batch [280/352] Loss: 0.6267 Acc: 80.25%
Epoch [83] Batch [290/352] Loss: 0.7498 Acc: 80.23%
Epoch [83] Batch [300/352] Loss: 0.6945 Acc: 80.25%
Epoch [83] Batch [310/352] Loss: 0.5628 Acc: 80.30%
Epoch [83] Batch [320/352] Loss: 0.6510 Acc: 80.25%
Epoch [83] Batch [330/352] Loss: 0.7866 Acc: 80.26%
Epoch [83] Batch [340/352] Loss: 0.7033 Acc: 80.18%
Epoch [83] Batch [350/352] Loss: 0.6114 Acc: 80.10%

======================================================================
Epoch 83/100
Train Loss: 0.6722, Train Acc: 80.10%
Val Loss: 1.2635, Val Acc: 66.72%
Time: 93.90s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 66.72%)
Epoch [84] Batch [0/352] Loss: 0.5839 Acc: 82.81%
Epoch [84] Batch [10/352] Loss: 0.7678 Acc: 80.11%
Epoch [84] Batch [20/352] Loss: 0.7638 Acc: 80.65%
Epoch [84] Batch [30/352] Loss: 0.5406 Acc: 81.02%
Epoch [84] Batch [40/352] Loss: 0.5421 Acc: 81.17%
Epoch [84] Batch [50/352] Loss: 0.6124 Acc: 81.45%
Epoch [84] Batch [60/352] Loss: 0.6559 Acc: 81.62%
Epoch [84] Batch [70/352] Loss: 0.6315 Acc: 81.77%
Epoch [84] Batch [80/352] Loss: 0.6238 Acc: 81.80%
Epoch [84] Batch [90/352] Loss: 0.6874 Acc: 81.71%
Epoch [84] Batch [100/352] Loss: 0.7299 Acc: 81.61%
Epoch [84] Batch [110/352] Loss: 0.6961 Acc: 81.45%
Epoch [84] Batch [120/352] Loss: 0.7958 Acc: 81.25%
Epoch [84] Batch [130/352] Loss: 0.5340 Acc: 81.32%
Epoch [84] Batch [140/352] Loss: 0.6813 Acc: 81.24%
Epoch [84] Batch [150/352] Loss: 0.8182 Acc: 81.15%
Epoch [84] Batch [160/352] Loss: 0.6623 Acc: 81.22%
Epoch [84] Batch [170/352] Loss: 0.7034 Acc: 81.28%
Epoch [84] Batch [180/352] Loss: 0.5477 Acc: 81.28%
Epoch [84] Batch [190/352] Loss: 0.6809 Acc: 81.16%
Epoch [84] Batch [200/352] Loss: 0.6280 Acc: 81.19%
Epoch [84] Batch [210/352] Loss: 0.7380 Acc: 81.18%
Epoch [84] Batch [220/352] Loss: 0.5783 Acc: 81.25%
Epoch [84] Batch [230/352] Loss: 0.6264 Acc: 81.26%
Epoch [84] Batch [240/352] Loss: 0.5973 Acc: 81.31%
Epoch [84] Batch [250/352] Loss: 0.8273 Acc: 81.27%
Epoch [84] Batch [260/352] Loss: 0.6789 Acc: 81.26%
Epoch [84] Batch [270/352] Loss: 0.5547 Acc: 81.22%
Epoch [84] Batch [280/352] Loss: 0.8966 Acc: 81.17%
Epoch [84] Batch [290/352] Loss: 0.7463 Acc: 81.12%
Epoch [84] Batch [300/352] Loss: 0.7149 Acc: 81.10%
Epoch [84] Batch [310/352] Loss: 0.5894 Acc: 81.02%
Epoch [84] Batch [320/352] Loss: 0.5407 Acc: 81.01%
Epoch [84] Batch [330/352] Loss: 0.7699 Acc: 80.98%
Epoch [84] Batch [340/352] Loss: 0.6723 Acc: 80.92%
Epoch [84] Batch [350/352] Loss: 0.6502 Acc: 80.87%

======================================================================
Epoch 84/100
Train Loss: 0.6546, Train Acc: 80.86%
Val Loss: 1.2502, Val Acc: 66.98%
Time: 93.86s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 66.98%)
Epoch [85] Batch [0/352] Loss: 0.4373 Acc: 88.28%
Epoch [85] Batch [10/352] Loss: 0.6273 Acc: 83.17%
Epoch [85] Batch [20/352] Loss: 0.7540 Acc: 81.40%
Epoch [85] Batch [30/352] Loss: 0.6519 Acc: 81.20%
Epoch [85] Batch [40/352] Loss: 0.5737 Acc: 81.31%
Epoch [85] Batch [50/352] Loss: 0.6402 Acc: 81.57%
Epoch [85] Batch [60/352] Loss: 0.6476 Acc: 81.44%
Epoch [85] Batch [70/352] Loss: 0.7142 Acc: 81.78%
Epoch [85] Batch [80/352] Loss: 0.6519 Acc: 81.97%
Epoch [85] Batch [90/352] Loss: 0.6532 Acc: 81.97%
Epoch [85] Batch [100/352] Loss: 0.8370 Acc: 81.75%
Epoch [85] Batch [110/352] Loss: 0.5325 Acc: 81.73%
Epoch [85] Batch [120/352] Loss: 0.6913 Acc: 81.70%
Epoch [85] Batch [130/352] Loss: 0.7132 Acc: 81.72%
Epoch [85] Batch [140/352] Loss: 0.5916 Acc: 81.73%
Epoch [85] Batch [150/352] Loss: 0.6536 Acc: 81.66%
Epoch [85] Batch [160/352] Loss: 0.6342 Acc: 81.64%
Epoch [85] Batch [170/352] Loss: 0.6341 Acc: 81.49%
Epoch [85] Batch [180/352] Loss: 0.5110 Acc: 81.49%
Epoch [85] Batch [190/352] Loss: 0.6623 Acc: 81.48%
Epoch [85] Batch [200/352] Loss: 0.5599 Acc: 81.52%
Epoch [85] Batch [210/352] Loss: 0.5596 Acc: 81.48%
Epoch [85] Batch [220/352] Loss: 0.5736 Acc: 81.53%
Epoch [85] Batch [230/352] Loss: 0.6485 Acc: 81.52%
Epoch [85] Batch [240/352] Loss: 0.6702 Acc: 81.49%
Epoch [85] Batch [250/352] Loss: 0.5996 Acc: 81.47%
Epoch [85] Batch [260/352] Loss: 0.6902 Acc: 81.49%
Epoch [85] Batch [270/352] Loss: 0.7344 Acc: 81.51%
Epoch [85] Batch [280/352] Loss: 0.5148 Acc: 81.55%
Epoch [85] Batch [290/352] Loss: 0.6980 Acc: 81.46%
Epoch [85] Batch [300/352] Loss: 0.5399 Acc: 81.44%
Epoch [85] Batch [310/352] Loss: 0.5524 Acc: 81.47%
Epoch [85] Batch [320/352] Loss: 0.6553 Acc: 81.51%
Epoch [85] Batch [330/352] Loss: 0.5459 Acc: 81.52%
Epoch [85] Batch [340/352] Loss: 0.9131 Acc: 81.49%
Epoch [85] Batch [350/352] Loss: 0.5434 Acc: 81.46%

======================================================================
Epoch 85/100
Train Loss: 0.6298, Train Acc: 81.45%
Val Loss: 1.2379, Val Acc: 67.52%
Time: 94.23s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 67.52%)
Epoch [86] Batch [0/352] Loss: 0.7017 Acc: 78.12%
Epoch [86] Batch [10/352] Loss: 0.6722 Acc: 82.95%
Epoch [86] Batch [20/352] Loss: 0.6867 Acc: 83.18%
Epoch [86] Batch [30/352] Loss: 0.4927 Acc: 83.54%
Epoch [86] Batch [40/352] Loss: 0.6139 Acc: 83.59%
Epoch [86] Batch [50/352] Loss: 0.6013 Acc: 83.32%
Epoch [86] Batch [60/352] Loss: 0.5527 Acc: 83.15%
Epoch [86] Batch [70/352] Loss: 0.6836 Acc: 83.37%
Epoch [86] Batch [80/352] Loss: 0.6008 Acc: 83.65%
Epoch [86] Batch [90/352] Loss: 0.6282 Acc: 83.38%
Epoch [86] Batch [100/352] Loss: 0.7702 Acc: 83.12%
Epoch [86] Batch [110/352] Loss: 0.5517 Acc: 83.16%
Epoch [86] Batch [120/352] Loss: 0.5491 Acc: 83.01%
Epoch [86] Batch [130/352] Loss: 0.6396 Acc: 82.87%
Epoch [86] Batch [140/352] Loss: 0.5707 Acc: 82.81%
Epoch [86] Batch [150/352] Loss: 0.5096 Acc: 82.78%
Epoch [86] Batch [160/352] Loss: 0.5900 Acc: 82.71%
Epoch [86] Batch [170/352] Loss: 0.5811 Acc: 82.71%
Epoch [86] Batch [180/352] Loss: 0.5442 Acc: 82.56%
Epoch [86] Batch [190/352] Loss: 0.6567 Acc: 82.50%
Epoch [86] Batch [200/352] Loss: 0.6306 Acc: 82.39%
Epoch [86] Batch [210/352] Loss: 0.5861 Acc: 82.35%
Epoch [86] Batch [220/352] Loss: 0.5704 Acc: 82.31%
Epoch [86] Batch [230/352] Loss: 0.6584 Acc: 82.37%
Epoch [86] Batch [240/352] Loss: 0.5573 Acc: 82.40%
Epoch [86] Batch [250/352] Loss: 0.4862 Acc: 82.40%
Epoch [86] Batch [260/352] Loss: 0.6423 Acc: 82.36%
Epoch [86] Batch [270/352] Loss: 0.6230 Acc: 82.35%
Epoch [86] Batch [280/352] Loss: 0.5898 Acc: 82.33%
Epoch [86] Batch [290/352] Loss: 0.7254 Acc: 82.27%
Epoch [86] Batch [300/352] Loss: 0.6003 Acc: 82.24%
Epoch [86] Batch [310/352] Loss: 0.5332 Acc: 82.21%
Epoch [86] Batch [320/352] Loss: 0.4800 Acc: 82.17%
Epoch [86] Batch [330/352] Loss: 0.6511 Acc: 82.12%
Epoch [86] Batch [340/352] Loss: 0.6872 Acc: 82.12%
Epoch [86] Batch [350/352] Loss: 0.5946 Acc: 82.10%

======================================================================
Epoch 86/100
Train Loss: 0.6101, Train Acc: 82.09%
Val Loss: 1.2077, Val Acc: 67.70%
Time: 94.35s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 67.70%)
Epoch [87] Batch [0/352] Loss: 0.6577 Acc: 81.25%
Epoch [87] Batch [10/352] Loss: 0.6353 Acc: 81.89%
Epoch [87] Batch [20/352] Loss: 0.5422 Acc: 82.92%
Epoch [87] Batch [30/352] Loss: 0.5260 Acc: 83.64%
Epoch [87] Batch [40/352] Loss: 0.6253 Acc: 83.17%
Epoch [87] Batch [50/352] Loss: 0.5126 Acc: 83.43%
Epoch [87] Batch [60/352] Loss: 0.5644 Acc: 83.41%
Epoch [87] Batch [70/352] Loss: 0.6636 Acc: 83.43%
Epoch [87] Batch [80/352] Loss: 0.5366 Acc: 83.29%
Epoch [87] Batch [90/352] Loss: 0.5839 Acc: 83.15%
Epoch [87] Batch [100/352] Loss: 0.5480 Acc: 83.28%
Epoch [87] Batch [110/352] Loss: 0.4575 Acc: 83.28%
Epoch [87] Batch [120/352] Loss: 0.7199 Acc: 83.25%
Epoch [87] Batch [130/352] Loss: 0.7240 Acc: 83.24%
Epoch [87] Batch [140/352] Loss: 0.4826 Acc: 83.20%
Epoch [87] Batch [150/352] Loss: 0.4870 Acc: 83.13%
Epoch [87] Batch [160/352] Loss: 0.6794 Acc: 83.09%
Epoch [87] Batch [170/352] Loss: 0.5511 Acc: 83.06%
Epoch [87] Batch [180/352] Loss: 0.5982 Acc: 83.00%
Epoch [87] Batch [190/352] Loss: 0.7131 Acc: 82.97%
Epoch [87] Batch [200/352] Loss: 0.6012 Acc: 82.86%
Epoch [87] Batch [210/352] Loss: 0.5024 Acc: 82.91%
Epoch [87] Batch [220/352] Loss: 0.6127 Acc: 82.91%
Epoch [87] Batch [230/352] Loss: 0.6171 Acc: 82.81%
Epoch [87] Batch [240/352] Loss: 0.5601 Acc: 82.82%
Epoch [87] Batch [250/352] Loss: 0.7018 Acc: 82.78%
Epoch [87] Batch [260/352] Loss: 0.6535 Acc: 82.77%
Epoch [87] Batch [270/352] Loss: 0.6370 Acc: 82.71%
Epoch [87] Batch [280/352] Loss: 0.5692 Acc: 82.67%
Epoch [87] Batch [290/352] Loss: 0.6442 Acc: 82.69%
Epoch [87] Batch [300/352] Loss: 0.6226 Acc: 82.65%
Epoch [87] Batch [310/352] Loss: 0.7845 Acc: 82.63%
Epoch [87] Batch [320/352] Loss: 0.6398 Acc: 82.61%
Epoch [87] Batch [330/352] Loss: 0.6127 Acc: 82.54%
Epoch [87] Batch [340/352] Loss: 0.6673 Acc: 82.54%
Epoch [87] Batch [350/352] Loss: 0.6582 Acc: 82.51%

======================================================================
Epoch 87/100
Train Loss: 0.5966, Train Acc: 82.52%
Val Loss: 1.1875, Val Acc: 68.14%
Time: 94.21s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 68.14%)
Epoch [88] Batch [0/352] Loss: 0.5751 Acc: 82.81%
Epoch [88] Batch [10/352] Loss: 0.4947 Acc: 84.30%
Epoch [88] Batch [20/352] Loss: 0.5120 Acc: 84.67%
Epoch [88] Batch [30/352] Loss: 0.6785 Acc: 84.58%
Epoch [88] Batch [40/352] Loss: 0.7187 Acc: 84.47%
Epoch [88] Batch [50/352] Loss: 0.6089 Acc: 84.18%
Epoch [88] Batch [60/352] Loss: 0.4755 Acc: 84.14%
Epoch [88] Batch [70/352] Loss: 0.4228 Acc: 84.06%
Epoch [88] Batch [80/352] Loss: 0.6223 Acc: 83.92%
Epoch [88] Batch [90/352] Loss: 0.5853 Acc: 83.95%
Epoch [88] Batch [100/352] Loss: 0.6400 Acc: 83.93%
Epoch [88] Batch [110/352] Loss: 0.7074 Acc: 83.95%
Epoch [88] Batch [120/352] Loss: 0.5716 Acc: 83.85%
Epoch [88] Batch [130/352] Loss: 0.5714 Acc: 83.82%
Epoch [88] Batch [140/352] Loss: 0.7161 Acc: 83.72%
Epoch [88] Batch [150/352] Loss: 0.6411 Acc: 83.63%
Epoch [88] Batch [160/352] Loss: 0.4758 Acc: 83.71%
Epoch [88] Batch [170/352] Loss: 0.4493 Acc: 83.79%
Epoch [88] Batch [180/352] Loss: 0.6336 Acc: 83.71%
Epoch [88] Batch [190/352] Loss: 0.5178 Acc: 83.68%
Epoch [88] Batch [200/352] Loss: 0.5890 Acc: 83.69%
Epoch [88] Batch [210/352] Loss: 0.7038 Acc: 83.70%
Epoch [88] Batch [220/352] Loss: 0.5848 Acc: 83.74%
Epoch [88] Batch [230/352] Loss: 0.4488 Acc: 83.75%
Epoch [88] Batch [240/352] Loss: 0.5957 Acc: 83.79%
Epoch [88] Batch [250/352] Loss: 0.5983 Acc: 83.70%
Epoch [88] Batch [260/352] Loss: 0.5237 Acc: 83.68%
Epoch [88] Batch [270/352] Loss: 0.6159 Acc: 83.63%
Epoch [88] Batch [280/352] Loss: 0.6810 Acc: 83.57%
Epoch [88] Batch [290/352] Loss: 0.6337 Acc: 83.52%
Epoch [88] Batch [300/352] Loss: 0.6031 Acc: 83.53%
Epoch [88] Batch [310/352] Loss: 0.5017 Acc: 83.52%
Epoch [88] Batch [320/352] Loss: 0.6711 Acc: 83.53%
Epoch [88] Batch [330/352] Loss: 0.5657 Acc: 83.45%
Epoch [88] Batch [340/352] Loss: 0.6204 Acc: 83.40%
Epoch [88] Batch [350/352] Loss: 0.4868 Acc: 83.37%

======================================================================
Epoch 88/100
Train Loss: 0.5738, Train Acc: 83.38%
Val Loss: 1.1931, Val Acc: 67.84%
Time: 92.54s
======================================================================

Epoch [89] Batch [0/352] Loss: 0.5719 Acc: 84.38%
Epoch [89] Batch [10/352] Loss: 0.6097 Acc: 85.80%
Epoch [89] Batch [20/352] Loss: 0.4088 Acc: 84.82%
Epoch [89] Batch [30/352] Loss: 0.3949 Acc: 84.93%
Epoch [89] Batch [40/352] Loss: 0.4663 Acc: 84.91%
Epoch [89] Batch [50/352] Loss: 0.6745 Acc: 84.73%
Epoch [89] Batch [60/352] Loss: 0.4725 Acc: 84.27%
Epoch [89] Batch [70/352] Loss: 0.5524 Acc: 84.42%
Epoch [89] Batch [80/352] Loss: 0.5575 Acc: 84.45%
Epoch [89] Batch [90/352] Loss: 0.4808 Acc: 84.44%
Epoch [89] Batch [100/352] Loss: 0.5639 Acc: 84.54%
Epoch [89] Batch [110/352] Loss: 0.4502 Acc: 84.53%
Epoch [89] Batch [120/352] Loss: 0.5153 Acc: 84.45%
Epoch [89] Batch [130/352] Loss: 0.4611 Acc: 84.43%
Epoch [89] Batch [140/352] Loss: 0.5712 Acc: 84.45%
Epoch [89] Batch [150/352] Loss: 0.5354 Acc: 84.44%
Epoch [89] Batch [160/352] Loss: 0.6351 Acc: 84.44%
Epoch [89] Batch [170/352] Loss: 0.7251 Acc: 84.42%
Epoch [89] Batch [180/352] Loss: 0.5327 Acc: 84.46%
Epoch [89] Batch [190/352] Loss: 0.5680 Acc: 84.46%
Epoch [89] Batch [200/352] Loss: 0.5310 Acc: 84.48%
Epoch [89] Batch [210/352] Loss: 0.4217 Acc: 84.41%
Epoch [89] Batch [220/352] Loss: 0.5611 Acc: 84.39%
Epoch [89] Batch [230/352] Loss: 0.4210 Acc: 84.38%
Epoch [89] Batch [240/352] Loss: 0.5272 Acc: 84.32%
Epoch [89] Batch [250/352] Loss: 0.4452 Acc: 84.27%
Epoch [89] Batch [260/352] Loss: 0.5840 Acc: 84.31%
Epoch [89] Batch [270/352] Loss: 0.4786 Acc: 84.31%
Epoch [89] Batch [280/352] Loss: 0.5084 Acc: 84.34%
Epoch [89] Batch [290/352] Loss: 0.5054 Acc: 84.30%
Epoch [89] Batch [300/352] Loss: 0.5837 Acc: 84.27%
Epoch [89] Batch [310/352] Loss: 0.4853 Acc: 84.23%
Epoch [89] Batch [320/352] Loss: 0.5666 Acc: 84.24%
Epoch [89] Batch [330/352] Loss: 0.5088 Acc: 84.22%
Epoch [89] Batch [340/352] Loss: 0.5643 Acc: 84.19%
Epoch [89] Batch [350/352] Loss: 0.6434 Acc: 84.16%

======================================================================
Epoch 89/100
Train Loss: 0.5514, Train Acc: 84.15%
Val Loss: 1.1812, Val Acc: 68.64%
Time: 92.32s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 68.64%)
Epoch [90] Batch [0/352] Loss: 0.5640 Acc: 83.59%
Epoch [90] Batch [10/352] Loss: 0.4205 Acc: 86.86%
Epoch [90] Batch [20/352] Loss: 0.5329 Acc: 85.45%
Epoch [90] Batch [30/352] Loss: 0.5543 Acc: 85.66%
Epoch [90] Batch [40/352] Loss: 0.5125 Acc: 85.38%
Epoch [90] Batch [50/352] Loss: 0.5157 Acc: 85.62%
Epoch [90] Batch [60/352] Loss: 0.6564 Acc: 85.46%
Epoch [90] Batch [70/352] Loss: 0.5698 Acc: 85.27%
Epoch [90] Batch [80/352] Loss: 0.5783 Acc: 85.21%
Epoch [90] Batch [90/352] Loss: 0.6712 Acc: 85.28%
Epoch [90] Batch [100/352] Loss: 0.5152 Acc: 85.33%
Epoch [90] Batch [110/352] Loss: 0.4893 Acc: 85.29%
Epoch [90] Batch [120/352] Loss: 0.6216 Acc: 85.21%
Epoch [90] Batch [130/352] Loss: 0.5736 Acc: 85.10%
Epoch [90] Batch [140/352] Loss: 0.6337 Acc: 85.08%
Epoch [90] Batch [150/352] Loss: 0.5767 Acc: 85.09%
Epoch [90] Batch [160/352] Loss: 0.4465 Acc: 85.16%
Epoch [90] Batch [170/352] Loss: 0.5283 Acc: 85.12%
Epoch [90] Batch [180/352] Loss: 0.5026 Acc: 85.13%
Epoch [90] Batch [190/352] Loss: 0.4581 Acc: 85.09%
Epoch [90] Batch [200/352] Loss: 0.5963 Acc: 85.10%
Epoch [90] Batch [210/352] Loss: 0.5509 Acc: 85.03%
Epoch [90] Batch [220/352] Loss: 0.5270 Acc: 85.11%
Epoch [90] Batch [230/352] Loss: 0.6384 Acc: 85.09%
Epoch [90] Batch [240/352] Loss: 0.7912 Acc: 85.07%
Epoch [90] Batch [250/352] Loss: 0.6588 Acc: 85.00%
Epoch [90] Batch [260/352] Loss: 0.5398 Acc: 84.97%
Epoch [90] Batch [270/352] Loss: 0.5502 Acc: 84.99%
Epoch [90] Batch [280/352] Loss: 0.4040 Acc: 85.01%
Epoch [90] Batch [290/352] Loss: 0.5777 Acc: 84.97%
Epoch [90] Batch [300/352] Loss: 0.5357 Acc: 84.92%
Epoch [90] Batch [310/352] Loss: 0.5553 Acc: 84.89%
Epoch [90] Batch [320/352] Loss: 0.5371 Acc: 84.91%
Epoch [90] Batch [330/352] Loss: 0.5575 Acc: 84.87%
Epoch [90] Batch [340/352] Loss: 0.5798 Acc: 84.85%
Epoch [90] Batch [350/352] Loss: 0.5945 Acc: 84.85%

======================================================================
Epoch 90/100
Train Loss: 0.5352, Train Acc: 84.84%
Val Loss: 1.1680, Val Acc: 68.96%
Time: 92.62s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 68.96%)
Epoch [91] Batch [0/352] Loss: 0.5177 Acc: 84.38%
Epoch [91] Batch [10/352] Loss: 0.5005 Acc: 84.38%
Epoch [91] Batch [20/352] Loss: 0.4596 Acc: 84.82%
Epoch [91] Batch [30/352] Loss: 0.3441 Acc: 85.03%
Epoch [91] Batch [40/352] Loss: 0.4323 Acc: 85.29%
Epoch [91] Batch [50/352] Loss: 0.6101 Acc: 85.48%
Epoch [91] Batch [60/352] Loss: 0.5975 Acc: 85.54%
Epoch [91] Batch [70/352] Loss: 0.6193 Acc: 85.72%
Epoch [91] Batch [80/352] Loss: 0.5627 Acc: 86.02%
Epoch [91] Batch [90/352] Loss: 0.6049 Acc: 85.91%
Epoch [91] Batch [100/352] Loss: 0.5640 Acc: 85.73%
Epoch [91] Batch [110/352] Loss: 0.5574 Acc: 85.73%
Epoch [91] Batch [120/352] Loss: 0.5038 Acc: 85.67%
Epoch [91] Batch [130/352] Loss: 0.6334 Acc: 85.56%
Epoch [91] Batch [140/352] Loss: 0.5500 Acc: 85.57%
Epoch [91] Batch [150/352] Loss: 0.5065 Acc: 85.57%
Epoch [91] Batch [160/352] Loss: 0.4516 Acc: 85.51%
Epoch [91] Batch [170/352] Loss: 0.5613 Acc: 85.54%
Epoch [91] Batch [180/352] Loss: 0.5925 Acc: 85.47%
Epoch [91] Batch [190/352] Loss: 0.6599 Acc: 85.51%
Epoch [91] Batch [200/352] Loss: 0.4587 Acc: 85.46%
Epoch [91] Batch [210/352] Loss: 0.6979 Acc: 85.47%
Epoch [91] Batch [220/352] Loss: 0.2902 Acc: 85.49%
Epoch [91] Batch [230/352] Loss: 0.5657 Acc: 85.48%
Epoch [91] Batch [240/352] Loss: 0.3879 Acc: 85.49%
Epoch [91] Batch [250/352] Loss: 0.5023 Acc: 85.45%
Epoch [91] Batch [260/352] Loss: 0.5153 Acc: 85.42%
Epoch [91] Batch [270/352] Loss: 0.5176 Acc: 85.36%
Epoch [91] Batch [280/352] Loss: 0.4993 Acc: 85.28%
Epoch [91] Batch [290/352] Loss: 0.5611 Acc: 85.21%
Epoch [91] Batch [300/352] Loss: 0.5251 Acc: 85.21%
Epoch [91] Batch [310/352] Loss: 0.4309 Acc: 85.19%
Epoch [91] Batch [320/352] Loss: 0.5936 Acc: 85.14%
Epoch [91] Batch [330/352] Loss: 0.6214 Acc: 85.12%
Epoch [91] Batch [340/352] Loss: 0.5817 Acc: 85.09%
Epoch [91] Batch [350/352] Loss: 0.5337 Acc: 85.11%

======================================================================
Epoch 91/100
Train Loss: 0.5214, Train Acc: 85.10%
Val Loss: 1.1590, Val Acc: 69.22%
Time: 92.70s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 69.22%)
Epoch [92] Batch [0/352] Loss: 0.5403 Acc: 83.59%
Epoch [92] Batch [10/352] Loss: 0.7039 Acc: 84.94%
Epoch [92] Batch [20/352] Loss: 0.4265 Acc: 85.49%
Epoch [92] Batch [30/352] Loss: 0.4987 Acc: 85.46%
Epoch [92] Batch [40/352] Loss: 0.5084 Acc: 85.79%
Epoch [92] Batch [50/352] Loss: 0.4702 Acc: 86.06%
Epoch [92] Batch [60/352] Loss: 0.3976 Acc: 86.12%
Epoch [92] Batch [70/352] Loss: 0.6228 Acc: 85.72%
Epoch [92] Batch [80/352] Loss: 0.5696 Acc: 85.73%
Epoch [92] Batch [90/352] Loss: 0.3982 Acc: 85.66%
Epoch [92] Batch [100/352] Loss: 0.5430 Acc: 85.60%
Epoch [92] Batch [110/352] Loss: 0.5377 Acc: 85.78%
Epoch [92] Batch [120/352] Loss: 0.5402 Acc: 85.65%
Epoch [92] Batch [130/352] Loss: 0.6323 Acc: 85.57%
Epoch [92] Batch [140/352] Loss: 0.5336 Acc: 85.65%
Epoch [92] Batch [150/352] Loss: 0.6797 Acc: 85.59%
Epoch [92] Batch [160/352] Loss: 0.5214 Acc: 85.55%
Epoch [92] Batch [170/352] Loss: 0.4258 Acc: 85.61%
Epoch [92] Batch [180/352] Loss: 0.6047 Acc: 85.55%
Epoch [92] Batch [190/352] Loss: 0.4471 Acc: 85.47%
Epoch [92] Batch [200/352] Loss: 0.6471 Acc: 85.47%
Epoch [92] Batch [210/352] Loss: 0.5033 Acc: 85.47%
Epoch [92] Batch [220/352] Loss: 0.4128 Acc: 85.37%
Epoch [92] Batch [230/352] Loss: 0.5991 Acc: 85.32%
Epoch [92] Batch [240/352] Loss: 0.6977 Acc: 85.40%
Epoch [92] Batch [250/352] Loss: 0.4041 Acc: 85.41%
Epoch [92] Batch [260/352] Loss: 0.5603 Acc: 85.35%
Epoch [92] Batch [270/352] Loss: 0.5896 Acc: 85.34%
Epoch [92] Batch [280/352] Loss: 0.4191 Acc: 85.34%
Epoch [92] Batch [290/352] Loss: 0.5670 Acc: 85.36%
Epoch [92] Batch [300/352] Loss: 0.5453 Acc: 85.37%
Epoch [92] Batch [310/352] Loss: 0.5187 Acc: 85.37%
Epoch [92] Batch [320/352] Loss: 0.4721 Acc: 85.41%
Epoch [92] Batch [330/352] Loss: 0.5465 Acc: 85.40%
Epoch [92] Batch [340/352] Loss: 0.5978 Acc: 85.39%
Epoch [92] Batch [350/352] Loss: 0.4919 Acc: 85.35%

======================================================================
Epoch 92/100
Train Loss: 0.5115, Train Acc: 85.35%
Val Loss: 1.1597, Val Acc: 69.54%
Time: 92.41s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 69.54%)
Epoch [93] Batch [0/352] Loss: 0.4851 Acc: 88.28%
Epoch [93] Batch [10/352] Loss: 0.4587 Acc: 87.43%
Epoch [93] Batch [20/352] Loss: 0.3607 Acc: 87.54%
Epoch [93] Batch [30/352] Loss: 0.4093 Acc: 87.12%
Epoch [93] Batch [40/352] Loss: 0.5328 Acc: 86.93%
Epoch [93] Batch [50/352] Loss: 0.5725 Acc: 86.80%
Epoch [93] Batch [60/352] Loss: 0.5186 Acc: 86.45%
Epoch [93] Batch [70/352] Loss: 0.5355 Acc: 86.09%
Epoch [93] Batch [80/352] Loss: 0.4410 Acc: 85.85%
Epoch [93] Batch [90/352] Loss: 0.5337 Acc: 85.83%
Epoch [93] Batch [100/352] Loss: 0.6003 Acc: 85.98%
Epoch [93] Batch [110/352] Loss: 0.3736 Acc: 86.11%
Epoch [93] Batch [120/352] Loss: 0.3674 Acc: 86.05%
Epoch [93] Batch [130/352] Loss: 0.4586 Acc: 86.06%
Epoch [93] Batch [140/352] Loss: 0.3970 Acc: 86.05%
Epoch [93] Batch [150/352] Loss: 0.6590 Acc: 86.09%
Epoch [93] Batch [160/352] Loss: 0.5479 Acc: 86.08%
Epoch [93] Batch [170/352] Loss: 0.6576 Acc: 85.96%
Epoch [93] Batch [180/352] Loss: 0.5522 Acc: 86.03%
Epoch [93] Batch [190/352] Loss: 0.4214 Acc: 86.05%
Epoch [93] Batch [200/352] Loss: 0.4586 Acc: 86.07%
Epoch [93] Batch [210/352] Loss: 0.4966 Acc: 86.00%
Epoch [93] Batch [220/352] Loss: 0.4866 Acc: 85.98%
Epoch [93] Batch [230/352] Loss: 0.2768 Acc: 86.06%
Epoch [93] Batch [240/352] Loss: 0.5577 Acc: 86.15%
Epoch [93] Batch [250/352] Loss: 0.4902 Acc: 86.12%
Epoch [93] Batch [260/352] Loss: 0.4684 Acc: 86.15%
Epoch [93] Batch [270/352] Loss: 0.6034 Acc: 86.15%
Epoch [93] Batch [280/352] Loss: 0.7267 Acc: 86.07%
Epoch [93] Batch [290/352] Loss: 0.4803 Acc: 86.07%
Epoch [93] Batch [300/352] Loss: 0.5089 Acc: 86.11%
Epoch [93] Batch [310/352] Loss: 0.5065 Acc: 86.13%
Epoch [93] Batch [320/352] Loss: 0.5234 Acc: 86.11%
Epoch [93] Batch [330/352] Loss: 0.5563 Acc: 86.08%
Epoch [93] Batch [340/352] Loss: 0.5497 Acc: 86.08%
Epoch [93] Batch [350/352] Loss: 0.5015 Acc: 86.04%

======================================================================
Epoch 93/100
Train Loss: 0.4924, Train Acc: 86.04%
Val Loss: 1.1550, Val Acc: 69.70%
Time: 92.95s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 69.70%)
Epoch [94] Batch [0/352] Loss: 0.4893 Acc: 85.94%
Epoch [94] Batch [10/352] Loss: 0.4301 Acc: 86.43%
Epoch [94] Batch [20/352] Loss: 0.5165 Acc: 85.79%
Epoch [94] Batch [30/352] Loss: 0.3611 Acc: 85.94%
Epoch [94] Batch [40/352] Loss: 0.5228 Acc: 85.88%
Epoch [94] Batch [50/352] Loss: 0.4714 Acc: 85.86%
Epoch [94] Batch [60/352] Loss: 0.6224 Acc: 85.91%
Epoch [94] Batch [70/352] Loss: 0.4703 Acc: 86.01%
Epoch [94] Batch [80/352] Loss: 0.6050 Acc: 85.92%
Epoch [94] Batch [90/352] Loss: 0.5448 Acc: 85.96%
Epoch [94] Batch [100/352] Loss: 0.5534 Acc: 86.05%
Epoch [94] Batch [110/352] Loss: 0.6169 Acc: 86.01%
Epoch [94] Batch [120/352] Loss: 0.4896 Acc: 86.01%
Epoch [94] Batch [130/352] Loss: 0.4117 Acc: 86.03%
Epoch [94] Batch [140/352] Loss: 0.4589 Acc: 86.08%
Epoch [94] Batch [150/352] Loss: 0.5103 Acc: 86.20%
Epoch [94] Batch [160/352] Loss: 0.4936 Acc: 86.18%
Epoch [94] Batch [170/352] Loss: 0.5159 Acc: 86.25%
Epoch [94] Batch [180/352] Loss: 0.4234 Acc: 86.30%
Epoch [94] Batch [190/352] Loss: 0.3317 Acc: 86.33%
Epoch [94] Batch [200/352] Loss: 0.4765 Acc: 86.38%
Epoch [94] Batch [210/352] Loss: 0.5758 Acc: 86.40%
Epoch [94] Batch [220/352] Loss: 0.5290 Acc: 86.44%
Epoch [94] Batch [230/352] Loss: 0.4956 Acc: 86.42%
Epoch [94] Batch [240/352] Loss: 0.3921 Acc: 86.45%
Epoch [94] Batch [250/352] Loss: 0.4909 Acc: 86.49%
Epoch [94] Batch [260/352] Loss: 0.6130 Acc: 86.43%
Epoch [94] Batch [270/352] Loss: 0.3973 Acc: 86.40%
Epoch [94] Batch [280/352] Loss: 0.5082 Acc: 86.36%
Epoch [94] Batch [290/352] Loss: 0.4095 Acc: 86.34%
Epoch [94] Batch [300/352] Loss: 0.5315 Acc: 86.37%
Epoch [94] Batch [310/352] Loss: 0.4698 Acc: 86.31%
Epoch [94] Batch [320/352] Loss: 0.4619 Acc: 86.30%
Epoch [94] Batch [330/352] Loss: 0.5176 Acc: 86.28%
Epoch [94] Batch [340/352] Loss: 0.3540 Acc: 86.25%
Epoch [94] Batch [350/352] Loss: 0.5315 Acc: 86.20%

======================================================================
Epoch 94/100
Train Loss: 0.4898, Train Acc: 86.18%
Val Loss: 1.1598, Val Acc: 69.64%
Time: 92.44s
======================================================================

Epoch [95] Batch [0/352] Loss: 0.4763 Acc: 86.72%
Epoch [95] Batch [10/352] Loss: 0.4588 Acc: 86.51%
Epoch [95] Batch [20/352] Loss: 0.3394 Acc: 86.64%
Epoch [95] Batch [30/352] Loss: 0.5193 Acc: 86.92%
Epoch [95] Batch [40/352] Loss: 0.4447 Acc: 86.66%
Epoch [95] Batch [50/352] Loss: 0.4392 Acc: 86.78%
Epoch [95] Batch [60/352] Loss: 0.4408 Acc: 86.86%
Epoch [95] Batch [70/352] Loss: 0.5517 Acc: 86.94%
Epoch [95] Batch [80/352] Loss: 0.4896 Acc: 87.12%
Epoch [95] Batch [90/352] Loss: 0.4265 Acc: 87.11%
Epoch [95] Batch [100/352] Loss: 0.4630 Acc: 87.00%
Epoch [95] Batch [110/352] Loss: 0.5726 Acc: 86.90%
Epoch [95] Batch [120/352] Loss: 0.5376 Acc: 86.79%
Epoch [95] Batch [130/352] Loss: 0.5212 Acc: 86.78%
Epoch [95] Batch [140/352] Loss: 0.4617 Acc: 86.70%
Epoch [95] Batch [150/352] Loss: 0.5735 Acc: 86.63%
Epoch [95] Batch [160/352] Loss: 0.5045 Acc: 86.73%
Epoch [95] Batch [170/352] Loss: 0.4960 Acc: 86.70%
Epoch [95] Batch [180/352] Loss: 0.5512 Acc: 86.68%
Epoch [95] Batch [190/352] Loss: 0.4567 Acc: 86.71%
Epoch [95] Batch [200/352] Loss: 0.4833 Acc: 86.68%
Epoch [95] Batch [210/352] Loss: 0.3906 Acc: 86.70%
Epoch [95] Batch [220/352] Loss: 0.4403 Acc: 86.66%
Epoch [95] Batch [230/352] Loss: 0.4960 Acc: 86.65%
Epoch [95] Batch [240/352] Loss: 0.2875 Acc: 86.71%
Epoch [95] Batch [250/352] Loss: 0.4769 Acc: 86.71%
Epoch [95] Batch [260/352] Loss: 0.4463 Acc: 86.76%
Epoch [95] Batch [270/352] Loss: 0.5924 Acc: 86.74%
Epoch [95] Batch [280/352] Loss: 0.7004 Acc: 86.72%
Epoch [95] Batch [290/352] Loss: 0.5731 Acc: 86.68%
Epoch [95] Batch [300/352] Loss: 0.3773 Acc: 86.72%
Epoch [95] Batch [310/352] Loss: 0.4701 Acc: 86.71%
Epoch [95] Batch [320/352] Loss: 0.4906 Acc: 86.69%
Epoch [95] Batch [330/352] Loss: 0.4383 Acc: 86.70%
Epoch [95] Batch [340/352] Loss: 0.3931 Acc: 86.71%
Epoch [95] Batch [350/352] Loss: 0.5159 Acc: 86.63%

======================================================================
Epoch 95/100
Train Loss: 0.4790, Train Acc: 86.63%
Val Loss: 1.1551, Val Acc: 69.54%
Time: 93.84s
======================================================================

Epoch [96] Batch [0/352] Loss: 0.4856 Acc: 87.50%
Epoch [96] Batch [10/352] Loss: 0.4109 Acc: 87.64%
Epoch [96] Batch [20/352] Loss: 0.4612 Acc: 87.39%
Epoch [96] Batch [30/352] Loss: 0.3976 Acc: 87.47%
Epoch [96] Batch [40/352] Loss: 0.4333 Acc: 87.67%
Epoch [96] Batch [50/352] Loss: 0.4294 Acc: 87.30%
Epoch [96] Batch [60/352] Loss: 0.4387 Acc: 87.36%
Epoch [96] Batch [70/352] Loss: 0.5195 Acc: 87.46%
Epoch [96] Batch [80/352] Loss: 0.4736 Acc: 87.49%
Epoch [96] Batch [90/352] Loss: 0.3477 Acc: 87.42%
Epoch [96] Batch [100/352] Loss: 0.4728 Acc: 87.38%
Epoch [96] Batch [110/352] Loss: 0.5292 Acc: 87.34%
Epoch [96] Batch [120/352] Loss: 0.3424 Acc: 87.40%
Epoch [96] Batch [130/352] Loss: 0.4693 Acc: 87.45%
Epoch [96] Batch [140/352] Loss: 0.4690 Acc: 87.25%
Epoch [96] Batch [150/352] Loss: 0.4411 Acc: 87.17%
Epoch [96] Batch [160/352] Loss: 0.4143 Acc: 87.24%
Epoch [96] Batch [170/352] Loss: 0.5987 Acc: 87.19%
Epoch [96] Batch [180/352] Loss: 0.5002 Acc: 87.17%
Epoch [96] Batch [190/352] Loss: 0.6006 Acc: 87.10%
Epoch [96] Batch [200/352] Loss: 0.3230 Acc: 87.13%
Epoch [96] Batch [210/352] Loss: 0.5392 Acc: 87.17%
Epoch [96] Batch [220/352] Loss: 0.4244 Acc: 87.16%
Epoch [96] Batch [230/352] Loss: 0.5369 Acc: 87.18%
Epoch [96] Batch [240/352] Loss: 0.5479 Acc: 87.17%
Epoch [96] Batch [250/352] Loss: 0.4905 Acc: 87.16%
Epoch [96] Batch [260/352] Loss: 0.3949 Acc: 87.13%
Epoch [96] Batch [270/352] Loss: 0.5900 Acc: 87.11%
Epoch [96] Batch [280/352] Loss: 0.5450 Acc: 87.06%
Epoch [96] Batch [290/352] Loss: 0.4289 Acc: 87.00%
Epoch [96] Batch [300/352] Loss: 0.4250 Acc: 87.02%
Epoch [96] Batch [310/352] Loss: 0.5240 Acc: 86.99%
Epoch [96] Batch [320/352] Loss: 0.4724 Acc: 87.02%
Epoch [96] Batch [330/352] Loss: 0.4138 Acc: 87.01%
Epoch [96] Batch [340/352] Loss: 0.4202 Acc: 86.96%
Epoch [96] Batch [350/352] Loss: 0.5054 Acc: 86.93%

======================================================================
Epoch 96/100
Train Loss: 0.4701, Train Acc: 86.93%
Val Loss: 1.1559, Val Acc: 69.60%
Time: 93.91s
======================================================================

Epoch [97] Batch [0/352] Loss: 0.4074 Acc: 88.28%
Epoch [97] Batch [10/352] Loss: 0.3651 Acc: 87.57%
Epoch [97] Batch [20/352] Loss: 0.3182 Acc: 87.83%
Epoch [97] Batch [30/352] Loss: 0.5845 Acc: 87.32%
Epoch [97] Batch [40/352] Loss: 0.5195 Acc: 87.46%
Epoch [97] Batch [50/352] Loss: 0.3854 Acc: 87.52%
Epoch [97] Batch [60/352] Loss: 0.3654 Acc: 87.53%
Epoch [97] Batch [70/352] Loss: 0.4916 Acc: 87.41%
Epoch [97] Batch [80/352] Loss: 0.4855 Acc: 87.18%
Epoch [97] Batch [90/352] Loss: 0.5104 Acc: 87.16%
Epoch [97] Batch [100/352] Loss: 0.4879 Acc: 87.11%
Epoch [97] Batch [110/352] Loss: 0.4635 Acc: 87.14%
Epoch [97] Batch [120/352] Loss: 0.3525 Acc: 87.17%
Epoch [97] Batch [130/352] Loss: 0.5613 Acc: 87.11%
Epoch [97] Batch [140/352] Loss: 0.3414 Acc: 87.11%
Epoch [97] Batch [150/352] Loss: 0.3964 Acc: 87.14%
Epoch [97] Batch [160/352] Loss: 0.4521 Acc: 87.11%
Epoch [97] Batch [170/352] Loss: 0.5231 Acc: 87.13%
Epoch [97] Batch [180/352] Loss: 0.4840 Acc: 87.14%
Epoch [97] Batch [190/352] Loss: 0.4113 Acc: 87.10%
Epoch [97] Batch [200/352] Loss: 0.4414 Acc: 87.14%
Epoch [97] Batch [210/352] Loss: 0.5309 Acc: 87.23%
Epoch [97] Batch [220/352] Loss: 0.5690 Acc: 87.15%
Epoch [97] Batch [230/352] Loss: 0.4845 Acc: 87.15%
Epoch [97] Batch [240/352] Loss: 0.4200 Acc: 87.16%
Epoch [97] Batch [250/352] Loss: 0.4650 Acc: 87.18%
Epoch [97] Batch [260/352] Loss: 0.4262 Acc: 87.17%
Epoch [97] Batch [270/352] Loss: 0.5309 Acc: 87.13%
Epoch [97] Batch [280/352] Loss: 0.3588 Acc: 87.15%
Epoch [97] Batch [290/352] Loss: 0.4788 Acc: 87.16%
Epoch [97] Batch [300/352] Loss: 0.3659 Acc: 87.14%
Epoch [97] Batch [310/352] Loss: 0.4730 Acc: 87.12%
Epoch [97] Batch [320/352] Loss: 0.4991 Acc: 87.11%
Epoch [97] Batch [330/352] Loss: 0.4815 Acc: 87.08%
Epoch [97] Batch [340/352] Loss: 0.4251 Acc: 87.08%
Epoch [97] Batch [350/352] Loss: 0.5663 Acc: 87.06%

======================================================================
Epoch 97/100
Train Loss: 0.4675, Train Acc: 87.06%
Val Loss: 1.1516, Val Acc: 69.46%
Time: 94.04s
======================================================================

Epoch [98] Batch [0/352] Loss: 0.3956 Acc: 87.50%
Epoch [98] Batch [10/352] Loss: 0.4933 Acc: 87.57%
Epoch [98] Batch [20/352] Loss: 0.4313 Acc: 87.46%
Epoch [98] Batch [30/352] Loss: 0.4480 Acc: 86.69%
Epoch [98] Batch [40/352] Loss: 0.4015 Acc: 86.93%
Epoch [98] Batch [50/352] Loss: 0.3778 Acc: 87.22%
Epoch [98] Batch [60/352] Loss: 0.4607 Acc: 87.26%
Epoch [98] Batch [70/352] Loss: 0.4696 Acc: 87.28%
Epoch [98] Batch [80/352] Loss: 0.3602 Acc: 87.31%
Epoch [98] Batch [90/352] Loss: 0.3851 Acc: 87.39%
Epoch [98] Batch [100/352] Loss: 0.3951 Acc: 87.33%
Epoch [98] Batch [110/352] Loss: 0.4359 Acc: 87.25%
Epoch [98] Batch [120/352] Loss: 0.5711 Acc: 87.14%
Epoch [98] Batch [130/352] Loss: 0.4921 Acc: 87.26%
Epoch [98] Batch [140/352] Loss: 0.4310 Acc: 87.28%
Epoch [98] Batch [150/352] Loss: 0.4590 Acc: 87.28%
Epoch [98] Batch [160/352] Loss: 0.3912 Acc: 87.29%
Epoch [98] Batch [170/352] Loss: 0.5352 Acc: 87.23%
Epoch [98] Batch [180/352] Loss: 0.4186 Acc: 87.27%
Epoch [98] Batch [190/352] Loss: 0.3775 Acc: 87.33%
Epoch [98] Batch [200/352] Loss: 0.5288 Acc: 87.33%
Epoch [98] Batch [210/352] Loss: 0.4012 Acc: 87.34%
Epoch [98] Batch [220/352] Loss: 0.5887 Acc: 87.32%
Epoch [98] Batch [230/352] Loss: 0.3804 Acc: 87.35%
Epoch [98] Batch [240/352] Loss: 0.5341 Acc: 87.40%
Epoch [98] Batch [250/352] Loss: 0.5116 Acc: 87.41%
Epoch [98] Batch [260/352] Loss: 0.5009 Acc: 87.40%
Epoch [98] Batch [270/352] Loss: 0.3487 Acc: 87.28%
Epoch [98] Batch [280/352] Loss: 0.3746 Acc: 87.26%
Epoch [98] Batch [290/352] Loss: 0.3688 Acc: 87.25%
Epoch [98] Batch [300/352] Loss: 0.5067 Acc: 87.17%
Epoch [98] Batch [310/352] Loss: 0.3618 Acc: 87.17%
Epoch [98] Batch [320/352] Loss: 0.4199 Acc: 87.15%
Epoch [98] Batch [330/352] Loss: 0.3965 Acc: 87.19%
Epoch [98] Batch [340/352] Loss: 0.6078 Acc: 87.10%
Epoch [98] Batch [350/352] Loss: 0.4574 Acc: 87.04%

======================================================================
Epoch 98/100
Train Loss: 0.4622, Train Acc: 87.04%
Val Loss: 1.1485, Val Acc: 69.74%
Time: 94.03s
======================================================================

[BEST] Saved best model -> checkpoints/best_data_2.pth (Val Acc: 69.74%)
Epoch [99] Batch [0/352] Loss: 0.5299 Acc: 83.59%
Epoch [99] Batch [10/352] Loss: 0.4430 Acc: 87.50%
Epoch [99] Batch [20/352] Loss: 0.2905 Acc: 87.72%
Epoch [99] Batch [30/352] Loss: 0.4080 Acc: 87.68%
Epoch [99] Batch [40/352] Loss: 0.3021 Acc: 87.65%
Epoch [99] Batch [50/352] Loss: 0.3569 Acc: 87.78%
Epoch [99] Batch [60/352] Loss: 0.5314 Acc: 87.56%
Epoch [99] Batch [70/352] Loss: 0.5446 Acc: 87.61%
Epoch [99] Batch [80/352] Loss: 0.5260 Acc: 87.40%
Epoch [99] Batch [90/352] Loss: 0.4773 Acc: 87.47%
Epoch [99] Batch [100/352] Loss: 0.4465 Acc: 87.38%
Epoch [99] Batch [110/352] Loss: 0.3951 Acc: 87.27%
Epoch [99] Batch [120/352] Loss: 0.4179 Acc: 87.28%
Epoch [99] Batch [130/352] Loss: 0.3626 Acc: 87.25%
Epoch [99] Batch [140/352] Loss: 0.4246 Acc: 87.21%
Epoch [99] Batch [150/352] Loss: 0.5073 Acc: 87.24%
Epoch [99] Batch [160/352] Loss: 0.4092 Acc: 87.33%
Epoch [99] Batch [170/352] Loss: 0.3310 Acc: 87.33%
Epoch [99] Batch [180/352] Loss: 0.3611 Acc: 87.40%
Epoch [99] Batch [190/352] Loss: 0.5062 Acc: 87.45%
Epoch [99] Batch [200/352] Loss: 0.4912 Acc: 87.53%
Epoch [99] Batch [210/352] Loss: 0.3956 Acc: 87.48%
Epoch [99] Batch [220/352] Loss: 0.5398 Acc: 87.49%
Epoch [99] Batch [230/352] Loss: 0.4203 Acc: 87.47%
Epoch [99] Batch [240/352] Loss: 0.3769 Acc: 87.50%
Epoch [99] Batch [250/352] Loss: 0.4317 Acc: 87.52%
Epoch [99] Batch [260/352] Loss: 0.3874 Acc: 87.52%
Epoch [99] Batch [270/352] Loss: 0.4662 Acc: 87.53%
Epoch [99] Batch [280/352] Loss: 0.3107 Acc: 87.50%
Epoch [99] Batch [290/352] Loss: 0.4832 Acc: 87.47%
Epoch [99] Batch [300/352] Loss: 0.5952 Acc: 87.47%
Epoch [99] Batch [310/352] Loss: 0.4971 Acc: 87.46%
Epoch [99] Batch [320/352] Loss: 0.4714 Acc: 87.43%
Epoch [99] Batch [330/352] Loss: 0.4603 Acc: 87.41%
Epoch [99] Batch [340/352] Loss: 0.4697 Acc: 87.42%
Epoch [99] Batch [350/352] Loss: 0.4189 Acc: 87.44%

======================================================================
Epoch 99/100
Train Loss: 0.4576, Train Acc: 87.44%
Val Loss: 1.1443, Val Acc: 69.62%
Time: 93.94s
======================================================================

Epoch [100] Batch [0/352] Loss: 0.4462 Acc: 90.62%
Epoch [100] Batch [10/352] Loss: 0.4186 Acc: 87.57%
Epoch [100] Batch [20/352] Loss: 0.4856 Acc: 87.05%
Epoch [100] Batch [30/352] Loss: 0.3298 Acc: 86.72%
Epoch [100] Batch [40/352] Loss: 0.4484 Acc: 86.76%
Epoch [100] Batch [50/352] Loss: 0.3017 Acc: 86.93%
Epoch [100] Batch [60/352] Loss: 0.5385 Acc: 86.97%
Epoch [100] Batch [70/352] Loss: 0.3889 Acc: 87.02%
Epoch [100] Batch [80/352] Loss: 0.3881 Acc: 87.12%
Epoch [100] Batch [90/352] Loss: 0.3559 Acc: 87.26%
Epoch [100] Batch [100/352] Loss: 0.3002 Acc: 87.39%
Epoch [100] Batch [110/352] Loss: 0.4127 Acc: 87.44%
Epoch [100] Batch [120/352] Loss: 0.4877 Acc: 87.53%
Epoch [100] Batch [130/352] Loss: 0.4657 Acc: 87.54%
Epoch [100] Batch [140/352] Loss: 0.3890 Acc: 87.51%
Epoch [100] Batch [150/352] Loss: 0.4071 Acc: 87.55%
Epoch [100] Batch [160/352] Loss: 0.3737 Acc: 87.59%
Epoch [100] Batch [170/352] Loss: 0.4499 Acc: 87.61%
Epoch [100] Batch [180/352] Loss: 0.5338 Acc: 87.63%
Epoch [100] Batch [190/352] Loss: 0.5587 Acc: 87.60%
Epoch [100] Batch [200/352] Loss: 0.3562 Acc: 87.60%
Epoch [100] Batch [210/352] Loss: 0.6154 Acc: 87.50%
Epoch [100] Batch [220/352] Loss: 0.6425 Acc: 87.52%
Epoch [100] Batch [230/352] Loss: 0.3281 Acc: 87.45%
Epoch [100] Batch [240/352] Loss: 0.4095 Acc: 87.45%
Epoch [100] Batch [250/352] Loss: 0.5546 Acc: 87.37%
Epoch [100] Batch [260/352] Loss: 0.4527 Acc: 87.40%
Epoch [100] Batch [270/352] Loss: 0.5434 Acc: 87.35%
Epoch [100] Batch [280/352] Loss: 0.5084 Acc: 87.36%
Epoch [100] Batch [290/352] Loss: 0.3105 Acc: 87.38%
Epoch [100] Batch [300/352] Loss: 0.4314 Acc: 87.35%
Epoch [100] Batch [310/352] Loss: 0.3999 Acc: 87.37%
Epoch [100] Batch [320/352] Loss: 0.3653 Acc: 87.40%
Epoch [100] Batch [330/352] Loss: 0.3387 Acc: 87.43%
Epoch [100] Batch [340/352] Loss: 0.3362 Acc: 87.44%
Epoch [100] Batch [350/352] Loss: 0.3899 Acc: 87.46%

======================================================================
Epoch 100/100
Train Loss: 0.4540, Train Acc: 87.46%
Val Loss: 1.1464, Val Acc: 69.68%
Time: 93.72s
======================================================================

======================================================================
Training Complete!
Best Validation Accuracy: 69.74%
======================================================================
